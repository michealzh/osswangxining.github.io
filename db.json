{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/hdfs1.jpg","path":"images/hdfs1.jpg","modified":0,"renderable":0},{"_id":"source/images/hdfs-read.jpg","path":"images/hdfs-read.jpg","modified":0,"renderable":0},{"_id":"source/images/docker-network-perf.jpg","path":"images/docker-network-perf.jpg","modified":0,"renderable":0},{"_id":"source/images/hdfs-write.jpg","path":"images/hdfs-write.jpg","modified":0,"renderable":0},{"_id":"source/images/slider.png","path":"images/slider.png","modified":0,"renderable":0},{"_id":"source/images/yarn-dev2.png","path":"images/yarn-dev2.png","modified":0,"renderable":0},{"_id":"source/images/kubearch01.png","path":"images/kubearch01.png","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/build.gradle","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/build.gradle","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew.bat","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew.bat","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/pom.xml","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/pom.xml","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw.cmd","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw.cmd","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/build.gradle","path":"samplecodes/ServiceRegistryConsulDistributedTrace/build.gradle","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradlew.bat","path":"samplecodes/ServiceRegistryConsulDistributedTrace/gradlew.bat","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/pom.xml","path":"samplecodes/ServiceRegistryConsulDistributedTrace/pom.xml","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/mvnw.cmd","path":"samplecodes/ServiceRegistryConsulDistributedTrace/mvnw.cmd","modified":0,"renderable":0},{"_id":"source/images/yarn-dev3.png","path":"images/yarn-dev3.png","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradlew","path":"samplecodes/ServiceRegistryConsulDistributedTrace/gradlew","modified":0,"renderable":0},{"_id":"source/samplecodes/hbaseclient/HBaseSample.java","path":"samplecodes/hbaseclient/HBaseSample.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/mvnw","path":"samplecodes/ServiceRegistryConsulDistributedTrace/mvnw","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"source/images/yarn-dev1.png","path":"images/yarn-dev1.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.jar","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.jar","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.properties","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.properties","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.properties","path":"samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.properties","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.jar","path":"samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.jar","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"source/images/ingress-nginx.png","path":"images/ingress-nginx.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.properties","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.properties","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/application.properties","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/application.properties","modified":0,"renderable":0},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/HelloController.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/HelloController.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/Application.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/Application.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/SayHelloConfiguration.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/SayHelloConfiguration.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerTest.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerTest.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/Application.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/Application.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerIT.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerIT.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HelloController.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HelloController.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HomeController.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HomeController.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleController.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleController.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HystrixController.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HystrixController.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hystrix/CommandHelloWorld.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hystrix/CommandHelloWorld.java","modified":0,"renderable":0},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleBackground.java","path":"samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleBackground.java","modified":0,"renderable":0},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/com/netflix/client/http/HttpResponse.java","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/com/netflix/client/http/HttpResponse.java","modified":0,"renderable":0},{"_id":"source/images/bio.png","path":"images/bio.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"4209802d976709b281f24a3d170016e59763b247","modified":1494398447000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1494293744000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1494293744000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1494293744000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1494293744000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1494293744000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1494293744000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1494293744000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1494293744000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1494293744000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1494293744000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1494293744000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1494293744000},{"_id":"themes/next/_config.yml","hash":"be42dc20d6c5e3583015c232d429a337e043db1a","modified":1494401830000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1494293744000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1494293744000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1494293744000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1494293744000},{"_id":"source/_posts/README.md","hash":"455d126273179eb3b830a5138fff091bd6052444","modified":1494400133000},{"_id":"source/_posts/Hystrix.md","hash":"6119ee7d125051cd67e924451e61be5f06dfa43f","modified":1494343548000},{"_id":"source/_posts/consul.md","hash":"fc36de5b545f12ef3a6a6bde10ab80884e6fc758","modified":1494343548000},{"_id":"source/_posts/gateway.md","hash":"3231b6648decf484c23df1388785e8b79e1e1260","modified":1494343548000},{"_id":"source/_posts/hdfs.md","hash":"490ecac2979589aaefc6a0b8755bd32500ad7514","modified":1494343548000},{"_id":"source/_posts/kafka.md","hash":"bf1a1bd4a30e9e6f039b4ee1c6c0c5405f04feeb","modified":1494343548000},{"_id":"source/_posts/hbase.md","hash":"9955bb99e77d208514b321540b24609769dd72d1","modified":1494343548000},{"_id":"source/_posts/kubernetes-deployment.md","hash":"99182f6dee5db9cde240a135502a62c01bb56593","modified":1494400030000},{"_id":"source/_posts/kubernetes-ingress.md","hash":"28f0d047945ab7ec5ce20c50e6a9265ab9e9d797","modified":1494400023000},{"_id":"source/_posts/kubernetes.md","hash":"29ae4441c1dd7887fbf2e9121dcb0be0a0666fc7","modified":1494343548000},{"_id":"source/_posts/kubernetes-configmap.md","hash":"3d0474891ec48f4fbd9a57df36048a833228e155","modified":1494343548000},{"_id":"source/_posts/kubernetes-secrets.md","hash":"5bcec968e55cc454747f903ab9d136281715cf44","modified":1494343548000},{"_id":"source/_posts/mongo.md","hash":"0c22a612c266a81f6f11e2f42a86fac009759684","modified":1494343548000},{"_id":"source/_posts/redis.md","hash":"4f895642a9555c8004063f072a24b0b131c96fb4","modified":1494343548000},{"_id":"source/_posts/running-spark-on-yarn.md","hash":"2bdd5ddfe39e44db0aeb303585976ebc4c5cf10b","modified":1494343548000},{"_id":"source/_posts/sgg.md","hash":"0bdd23c35826208816c8108e397622fa7720d920","modified":1494343548000},{"_id":"source/_posts/spark.md","hash":"f6ba36704b962c4ba0d2aa80dec85246125825fb","modified":1494343548000},{"_id":"source/_posts/yarn-appdev.md","hash":"fde518ebe3f219f5c6db3f842dd1ed0f9617b282","modified":1494343548000},{"_id":"source/_posts/zipkin.md","hash":"066576bb5702004f78de28e774c218e97ac5345c","modified":1494343548000},{"_id":"source/_posts/yarn.md","hash":"bf8a6d21a1fd7f3c9862b12d56f45d96dae075cd","modified":1494343548000},{"_id":"source/about/index.md","hash":"735eeeffd5ae3eff4f6240842336a3b2bd71e7b6","modified":1494293744000},{"_id":"source/categories/index.md","hash":"e80c28979398a0be7415c6607a424b7417e734c3","modified":1494293744000},{"_id":"source/images/hdfs1.jpg","hash":"d381fbecb914a7785b6ef74d7d8ee23e41ace452","modified":1494293744000},{"_id":"source/samplecodes/.DS_Store","hash":"f289bd0c554f09dd7ea7e1d804f2997973f512ee","modified":1494293744000},{"_id":"source/tags/index.md","hash":"ec13ad4f972e0e9bc811b0a15d0384911c264644","modified":1494293744000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1494293744000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1494293744000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1494293744000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1494293744000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1494293744000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1494293744000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1494293744000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1494293744000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1494293744000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1494293744000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1494293744000},{"_id":"themes/next/languages/ru.yml","hash":"7462c3017dae88e5f80ff308db0b95baf960c83f","modified":1494293744000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1494293744000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1494293744000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1494293744000},{"_id":"themes/next/layout/_layout.swig","hash":"909d68b164227fe7601d82e2303bf574eb754172","modified":1494293744000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1494293744000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1494293744000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1494293744000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1494293744000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1494293744000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1494293744000},{"_id":"themes/next/layout/schedule.swig","hash":"234dc8c3b9e276e7811c69011efd5d560519ef19","modified":1494293744000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1494293744000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1494293744000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1494293744000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1494293744000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1494293744000},{"_id":"source/images/hdfs-read.jpg","hash":"d0b0511f74f19adf21de3bbd0532e649069b7b1c","modified":1494293744000},{"_id":"source/images/docker-network-perf.jpg","hash":"dcc351005d9bbeea021b010681f1247895e43c5e","modified":1494293744000},{"_id":"source/images/hdfs-write.jpg","hash":"0482dc43f508920abb89fb9113ac6d11bfd9c717","modified":1494293744000},{"_id":"source/images/slider.png","hash":"9ba5c6fa28b8cd21cdb22fd45641009922b3e13d","modified":1494293744000},{"_id":"source/images/yarn-dev2.png","hash":"38bd74730eebb3ec9461c1e87c86368d569ad701","modified":1494293744000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"source/images/kubearch01.png","hash":"88843df017b9fe8afc5533419b87aec3a2f58bb3","modified":1494337554000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.classpath","hash":"82ef1892b55f32506a74f8266cc69f5c0c3f275a","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.gitignore","hash":"deca8b979c4147a1d8af858e15b32fb47ae0ef8d","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.project","hash":"f3d185523bb9e5669cdf640223781fa0d892817d","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/build.gradle","hash":"03952c793a85387ba0fa1042694fad98d1a98d72","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew","hash":"6409d6256df6b2f9e2142183b4c6408823a10f6a","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradlew.bat","hash":"8751d7831ca6cd1cad48e1475a79596b54b48994","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw","hash":"dc153403a872c675192ab8e5eb7d2fa472cdf54f","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/pom.xml","hash":"31e8378ff4c448f1102508a2db5a7afeb7bd8e19","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.gitignore","hash":"deca8b979c4147a1d8af858e15b32fb47ae0ef8d","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/mvnw.cmd","hash":"385dfa8b0d6db3e26e0fec1ffd5078051d1ebbbd","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.classpath","hash":"82ef1892b55f32506a74f8266cc69f5c0c3f275a","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.project","hash":"48b8531d7ce3d57d49515dada462d33095e84c90","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/build.gradle","hash":"03952c793a85387ba0fa1042694fad98d1a98d72","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradlew.bat","hash":"8751d7831ca6cd1cad48e1475a79596b54b48994","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/pom.xml","hash":"3783f66dacd688adef5a07c8637994d77f8a2fb9","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/mvnw.cmd","hash":"385dfa8b0d6db3e26e0fec1ffd5078051d1ebbbd","modified":1494293744000},{"_id":"source/images/yarn-dev3.png","hash":"8005b57f7661cf4b5b745e7dbb82f460557fa6a8","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradlew","hash":"6409d6256df6b2f9e2142183b4c6408823a10f6a","modified":1494293744000},{"_id":"source/samplecodes/hbaseclient/HBaseSample.java","hash":"a82bd074443770aac9f7a3e7424cf5c40b9751e0","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/mvnw","hash":"dc153403a872c675192ab8e5eb7d2fa472cdf54f","modified":1494293744000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494293744000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1494293744000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494293744000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1494293744000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1494293744000},{"_id":"themes/next/layout/_macro/post.swig","hash":"640b431eccbbd27f10c6781f33db5ea9a6e064de","modified":1494293744000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1494293744000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1494293744000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1494293744000},{"_id":"themes/next/layout/_partials/head.swig","hash":"a0eafe24d1dae30c790ae35612154b3ffbbd5cce","modified":1494293744000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1494293744000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1494293744000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1494293744000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1494293744000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1494293744000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1494293744000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9de352a32865869e7ed6863db271c46db5853e5a","modified":1494293744000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1494293744000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1494293744000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1494293744000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1494293744000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1494293744000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1494293744000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1494293744000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1494293744000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1494293744000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1494293744000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1494293744000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1494293744000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1494293744000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1494293744000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1494293744000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1494293744000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1494293744000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1494293744000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1494293744000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1494293744000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494293744000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1494293744000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494293744000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1494293744000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1494293744000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1494293744000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.settings/org.eclipse.core.resources.prefs","hash":"200185d1276e5d2c34857dc1d3e5b014ac0389be","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.settings/org.eclipse.jdt.core.prefs","hash":"e1d8a5e5b0495ab00929407c30e150aa3b5949e5","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.settings/org.eclipse.wst.common.project.facet.core.xml","hash":"632c8b8d948358902f748deceee787111310afde","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.settings/org.eclipse.m2e.core.prefs","hash":"fdc827aee8edc9b9a55077f16f0811b7a13d3c8e","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.settings/org.eclipse.core.resources.prefs","hash":"200185d1276e5d2c34857dc1d3e5b014ac0389be","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.settings/org.eclipse.jdt.core.prefs","hash":"e1d8a5e5b0495ab00929407c30e150aa3b5949e5","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.settings/org.eclipse.wst.common.project.facet.core.xml","hash":"632c8b8d948358902f748deceee787111310afde","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.settings/org.eclipse.m2e.core.prefs","hash":"fdc827aee8edc9b9a55077f16f0811b7a13d3c8e","modified":1494293744000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1494293744000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1494293744000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2d1075f4cabcb3956b7b84a8e210f5a66f0a5562","modified":1494293744000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1494293744000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1494293744000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1494293744000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1494293744000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1494293744000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1494293744000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1494293744000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a652f202bd5b30c648c228ab8f0e997eb4928e44","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1494293744000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"7240f2e5ec7115f8abbbc4c9ef73d4bed180fdc7","modified":1494293744000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1494293744000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1494293744000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"f4dbd4c896e6510ded8ebe05394c28f8a86e71bf","modified":1494293744000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1494293744000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1494293744000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1494293744000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1494293744000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1494293744000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1494293744000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1494293744000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"28a7f84242ca816a6452a0a79669ca963d824607","modified":1494293744000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1494293744000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1494293744000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1494293744000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1494293744000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1494293744000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1494293744000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1494293744000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1494293744000},{"_id":"themes/next/source/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1494293744000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1494293744000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1494293744000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1494293744000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1494293744000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1494293744000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1494293744000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1494293744000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1494293744000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1494293744000},{"_id":"source/images/yarn-dev1.png","hash":"fdbeac79b3d99c307f8f53886e156449799ae0af","modified":1494293744000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1494293744000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.mvn/wrapper/maven-wrapper.properties","hash":"5a2bee75d4d2d934a799fdfb0e3ee87dc842ef4f","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/.mvn/wrapper/maven-wrapper.jar","hash":"7bad9d340cf3f615bf4c6125ddc3adf7ddfa8c62","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.jar","hash":"d5f2cff8bfce6bd848ee3dceb06393502f78ca7c","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/gradle/wrapper/gradle-wrapper.properties","hash":"da7a40ec47afa7b379847a640f34498a5963c021","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.mvn/wrapper/maven-wrapper.properties","hash":"5a2bee75d4d2d934a799fdfb0e3ee87dc842ef4f","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/.mvn/wrapper/maven-wrapper.jar","hash":"7bad9d340cf3f615bf4c6125ddc3adf7ddfa8c62","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.properties","hash":"da7a40ec47afa7b379847a640f34498a5963c021","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/gradle/wrapper/gradle-wrapper.jar","hash":"d5f2cff8bfce6bd848ee3dceb06393502f78ca7c","modified":1494293744000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1494293744000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1494293744000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1494293744000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1494293744000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1494293744000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1494293744000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1494293744000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"1eb34b9c1f6d541605ff23333eeb133e1c4daf17","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1494293744000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1494293744000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1494293744000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1494293744000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1494293744000},{"_id":"source/images/ingress-nginx.png","hash":"2d9c16dae21bb25f1bdfea8b9d4bdd749676544e","modified":1494398541000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.properties","hash":"d39c4c518fc727c3204f5b4af0c2052c84cc372d","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.yml","hash":"0e5b90c0d0d5af42280774d5fe2134fd5ce72d91","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/application.properties","hash":"58ab3507069966ad6e1dfb16e0e90bad38ca93a8","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"755b04edbbfbdd981a783edb09c9cc34cb79cea7","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"b9a2e76f019a5941191f1263b54aef7b69c48789","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1494293744000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"173490e21bece35a34858e8e534cf86e34561350","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1494293744000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1494293744000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/HelloController.java","hash":"2359b4ff00a161b7ea7e7962f8de667123cfc29f","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/META-INF/additional-spring-configuration-metadata.json","hash":"93f91030ff286a6d0ee05797fe7537d697b5b626","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/Application.java","hash":"1d90950b3b75cdb9d7de623d414c55c5355d3243","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/hello/SayHelloConfiguration.java","hash":"112803c240bcaf74efdcbdb84559bd7c5ca2cecc","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerTest.java","hash":"b41dfe898d427ea7023da52c94d3ef3631f23e59","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/Application.java","hash":"0d64abfb605cc3dd6588a2d6556f65af5098da8b","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/test/java/hello/HelloControllerIT.java","hash":"2dd5e0100a223911441ef68be6d7f7a9d311a141","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HelloController.java","hash":"6aff8151be6e43c0cbe449fbd5e19e8533ad9e00","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HomeController.java","hash":"03d68150f5240ea0e49e3455d747c1dfbb640655","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleController.java","hash":"c57e518ba99b9dc4d87f903ec01853d65dc3923a","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/HystrixController.java","hash":"ccbd7212d455ffbe0f65e61d7aab0d75414d28b9","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hystrix/CommandHelloWorld.java","hash":"aa2cd43624588007222dc286e2185ca06eb996b5","modified":1494293744000},{"_id":"source/samplecodes/ServiceRegistryConsulDistributedTrace/src/main/java/hello/SampleBackground.java","hash":"6dbef45799032627487d6a3978d956e387577f7f","modified":1494293744000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1494293744000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1494293744000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1494293744000},{"_id":"source/samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/com/netflix/client/http/HttpResponse.java","hash":"3d06e30f1526df8993885f32464e76c7f3f470e8","modified":1494293744000},{"_id":"themes/next/.DS_Store","hash":"8a751c2499409934faaddca052dde5a033cad567","modified":1494399815000},{"_id":"source/images/bio.png","hash":"f23c62aa84c414efa80bd6d906b81b683fb7b7d1","modified":1494401655000}],"Category":[{"name":"分布式&云计算","_id":"cj2in1bfo0004i272m5481pju"},{"name":"Kubernetes","parent":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bg2000ii272x2bpn3fh"}],"Data":[],"Page":[{"title":"about","date":"2017-05-08T13:59:13.000Z","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-08 21:59:13\n---\n","updated":"2017-05-09T01:35:44.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cj2in1bfk0001i272pwgk13xu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-05-08T14:11:35.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-05-08 22:11:35\ntype: \"categories\"\ncomments: false\n---\n","updated":"2017-05-09T01:35:44.000Z","path":"categories/index.html","layout":"page","_id":"cj2in1bfn0003i272ih5v1r0s","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2017-05-08T14:11:46.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-05-08 22:11:46\ntype: \"tags\"\ncomments: false\n---\n","updated":"2017-05-09T01:35:44.000Z","path":"tags/index.html","layout":"page","_id":"cj2in1bft0007i272wodu4itk","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"say-hello:\n ribbon:\n  okhttp:\n    enabled: true","source":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.yml","raw":"say-hello:\n ribbon:\n  okhttp:\n    enabled: true","date":"2017-05-09T01:35:44.000Z","updated":"2017-05-09T01:35:44.000Z","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/application.json","layout":"false","title":"","comments":1,"_id":"cj2in1bp5002bi272mmo70242","content":"{\"say-hello\":{\"ribbon\":{\"okhttp\":{\"enabled\":true}}}}","site":{"data":{}},"excerpt":"","more":"{\"say-hello\":{\"ribbon\":{\"okhttp\":{\"enabled\":true}}}}"},{"_content":"{\"properties\": [{\n  \"name\": \"say-hello.ribbon.okhttp.enabled\",\n  \"type\": \"java.lang.String\",\n  \"description\": \"A description for 'say-hello.ribbon.okhttp.enabled'\"\n}]}","source":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/META-INF/additional-spring-configuration-metadata.json","raw":"{\"properties\": [{\n  \"name\": \"say-hello.ribbon.okhttp.enabled\",\n  \"type\": \"java.lang.String\",\n  \"description\": \"A description for 'say-hello.ribbon.okhttp.enabled'\"\n}]}","date":"2017-05-09T01:35:44.000Z","updated":"2017-05-09T01:35:44.000Z","path":"samplecodes/ServiceDiscoveryLoadBalancerSample/src/main/java/META-INF/additional-spring-configuration-metadata.json","layout":"false","title":"","comments":1,"_id":"cj2in1bpm002ci2721qet8mn1","content":"{\"properties\":[{\"name\":\"say-hello.ribbon.okhttp.enabled\",\"type\":\"java.lang.String\",\"description\":\"A description for 'say-hello.ribbon.okhttp.enabled'\"}]}","site":{"data":{}},"excerpt":"","more":"{\"properties\":[{\"name\":\"say-hello.ribbon.okhttp.enabled\",\"type\":\"java.lang.String\",\"description\":\"A description for 'say-hello.ribbon.okhttp.enabled'\"}]}"}],"Post":[{"title":"分布式架构","date":"2017-06-09T12:46:25.000Z","_content":"\n## 分布式配置管理\n  - [Consul](/consul)\n  - Archaius\n  - [Kubernetes ConfigMap](/kubernetes-configmap/)\n  - [Kubernetes Secrets](/kubernetes-secrets/)\n\n配置的集中管理：采用consul的KV，将所有微服务的application.properties中的配置内容存入consul。\n\n配置的动态管理：采用archaius，将consul上的配置信息读到spring的PropertySource和archaius的PollResult中，当修改了配置信息后，经常改变的值通过DynamicFactory来获取，不经常改变的值可以通过其他方式获取. 大部分情况下，修改了consul上的配置信息后，相应的项目不需要重启，也会读到最新的值。\n\n## 服务注册与发现\n  - [Consul](/consul/)\n  - Eureka\n  - [Kubernetes Service](/kubernetes/#service)\n\n## 负载平衡\n  - Netflix Ribbon（Spring Cloud）\n  - [Kubernetes Service](/kubernetes/#service)\n\n  [Client Side Load Balancing](/consul/)\n\n## API网关与智能路由\n  - Netflix Zuul（SpringCloud）\n  - [Kubernetes Service](/kubernetes/#service)\n  - [Kubernetes Ingress](/kubernetes-ingress/)\n\n  [Gateway](/gateway/)\n\n## 分布式服务弹性与容错\n - 弹性服务\n - 服务降级\n - 线程池/信号隔离\n - 快速解决依赖隔离\n\n [Hystrix架构设计](/Hystrix/)\n\n## 日志管理\n  - ELK Stack（LogStash -> ES -> Kibana）\n\n## 分布式跟踪\n  - Zipkin\n  - SpringCloud Sleuth\n\n  [Zipkin](/zipkin/) is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. It manages both the collection and lookup of this data. Zipkin’s design is based on the Google Dapper paper.\n\n  Applications are instrumented to report timing data to Zipkin. The Zipkin UI also presents a Dependency diagram showing how many traced requests went through each application. If you are troubleshooting latency problems or errors, you can filter or sort all traces based on the application, length of trace, annotation, or timestamp. Once you select a trace, you can see the percentage of the total trace time each span takes which allows you to identify the problem application.\n\n## 监控与度量\n  - Application/Infrastructure monitoring using StatsD + Graphite + Grafana\n\n  [StatsD + Graphite + Grafana](/sgg/)\n\n## 服务安全\n  - SpringCloud Security\n\n## Auto Scaling\n  - [Kubernetes Autoscaling](/kubernetes/#Autoscaling)\n\n## 打包部署和调度部署\n  - Spring Boot；\n  - [Docker／Rkt、Kubernetes Scheduler&Deployment](/kubernetes/#Deployment)\n\n## 任务工作管理\n  - Spring Batch\n  - [Kubernetes Jobs](/kubernetes/#Job)\n\n\n## 分布式存储\n### 持久化 - 分布式文件系统\n- [HDFS分布式文件系统](/hdfs/)\n\n### 持久化 - 分布式数据库\n- [传统关系型数据库集群,如MySQL Cluster]\n- [Mongo](/mongo/)\n- [Cassandra,HBase](/hbase/)\n\n### 非持久化 - 分布式缓存/消息系统\n- [Kafka](/kafka/)\n- [Redis]\n\n## 分布式计算框架\n  - [YARN分布式计算框架](/yarn/)\n  - [YARN应用开发的几种方式](/yarn-appdev/)\n  - [Running Spark on YARN](/running-spark-on-yarn/)\n  - [Spark Big Data Analytics](spark.md)\n","source":"_posts/README.md","raw":"---\ntitle: 分布式架构\ndate: 2017-6-9 20:46:25\ncategories: 分布式&云计算\ntags:\n  - 分布式\n  - 架构设计\n---\n\n## 分布式配置管理\n  - [Consul](/consul)\n  - Archaius\n  - [Kubernetes ConfigMap](/kubernetes-configmap/)\n  - [Kubernetes Secrets](/kubernetes-secrets/)\n\n配置的集中管理：采用consul的KV，将所有微服务的application.properties中的配置内容存入consul。\n\n配置的动态管理：采用archaius，将consul上的配置信息读到spring的PropertySource和archaius的PollResult中，当修改了配置信息后，经常改变的值通过DynamicFactory来获取，不经常改变的值可以通过其他方式获取. 大部分情况下，修改了consul上的配置信息后，相应的项目不需要重启，也会读到最新的值。\n\n## 服务注册与发现\n  - [Consul](/consul/)\n  - Eureka\n  - [Kubernetes Service](/kubernetes/#service)\n\n## 负载平衡\n  - Netflix Ribbon（Spring Cloud）\n  - [Kubernetes Service](/kubernetes/#service)\n\n  [Client Side Load Balancing](/consul/)\n\n## API网关与智能路由\n  - Netflix Zuul（SpringCloud）\n  - [Kubernetes Service](/kubernetes/#service)\n  - [Kubernetes Ingress](/kubernetes-ingress/)\n\n  [Gateway](/gateway/)\n\n## 分布式服务弹性与容错\n - 弹性服务\n - 服务降级\n - 线程池/信号隔离\n - 快速解决依赖隔离\n\n [Hystrix架构设计](/Hystrix/)\n\n## 日志管理\n  - ELK Stack（LogStash -> ES -> Kibana）\n\n## 分布式跟踪\n  - Zipkin\n  - SpringCloud Sleuth\n\n  [Zipkin](/zipkin/) is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. It manages both the collection and lookup of this data. Zipkin’s design is based on the Google Dapper paper.\n\n  Applications are instrumented to report timing data to Zipkin. The Zipkin UI also presents a Dependency diagram showing how many traced requests went through each application. If you are troubleshooting latency problems or errors, you can filter or sort all traces based on the application, length of trace, annotation, or timestamp. Once you select a trace, you can see the percentage of the total trace time each span takes which allows you to identify the problem application.\n\n## 监控与度量\n  - Application/Infrastructure monitoring using StatsD + Graphite + Grafana\n\n  [StatsD + Graphite + Grafana](/sgg/)\n\n## 服务安全\n  - SpringCloud Security\n\n## Auto Scaling\n  - [Kubernetes Autoscaling](/kubernetes/#Autoscaling)\n\n## 打包部署和调度部署\n  - Spring Boot；\n  - [Docker／Rkt、Kubernetes Scheduler&Deployment](/kubernetes/#Deployment)\n\n## 任务工作管理\n  - Spring Batch\n  - [Kubernetes Jobs](/kubernetes/#Job)\n\n\n## 分布式存储\n### 持久化 - 分布式文件系统\n- [HDFS分布式文件系统](/hdfs/)\n\n### 持久化 - 分布式数据库\n- [传统关系型数据库集群,如MySQL Cluster]\n- [Mongo](/mongo/)\n- [Cassandra,HBase](/hbase/)\n\n### 非持久化 - 分布式缓存/消息系统\n- [Kafka](/kafka/)\n- [Redis]\n\n## 分布式计算框架\n  - [YARN分布式计算框架](/yarn/)\n  - [YARN应用开发的几种方式](/yarn-appdev/)\n  - [Running Spark on YARN](/running-spark-on-yarn/)\n  - [Spark Big Data Analytics](spark.md)\n","slug":"README","published":1,"updated":"2017-05-10T07:08:53.000Z","_id":"cj2in1bff0000i2728ktted8p","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"分布式配置管理\"><a href=\"#分布式配置管理\" class=\"headerlink\" title=\"分布式配置管理\"></a>分布式配置管理</h2><ul>\n<li><a href=\"/consul\">Consul</a></li>\n<li>Archaius</li>\n<li><a href=\"/kubernetes-configmap/\">Kubernetes ConfigMap</a></li>\n<li><a href=\"/kubernetes-secrets/\">Kubernetes Secrets</a></li>\n</ul>\n<p>配置的集中管理：采用consul的KV，将所有微服务的application.properties中的配置内容存入consul。</p>\n<p>配置的动态管理：采用archaius，将consul上的配置信息读到spring的PropertySource和archaius的PollResult中，当修改了配置信息后，经常改变的值通过DynamicFactory来获取，不经常改变的值可以通过其他方式获取. 大部分情况下，修改了consul上的配置信息后，相应的项目不需要重启，也会读到最新的值。</p>\n<h2 id=\"服务注册与发现\"><a href=\"#服务注册与发现\" class=\"headerlink\" title=\"服务注册与发现\"></a>服务注册与发现</h2><ul>\n<li><a href=\"/consul/\">Consul</a></li>\n<li>Eureka</li>\n<li><a href=\"/kubernetes/#service\">Kubernetes Service</a></li>\n</ul>\n<h2 id=\"负载平衡\"><a href=\"#负载平衡\" class=\"headerlink\" title=\"负载平衡\"></a>负载平衡</h2><ul>\n<li>Netflix Ribbon（Spring Cloud）</li>\n<li><p><a href=\"/kubernetes/#service\">Kubernetes Service</a></p>\n<p><a href=\"/consul/\">Client Side Load Balancing</a></p>\n</li>\n</ul>\n<h2 id=\"API网关与智能路由\"><a href=\"#API网关与智能路由\" class=\"headerlink\" title=\"API网关与智能路由\"></a>API网关与智能路由</h2><ul>\n<li>Netflix Zuul（SpringCloud）</li>\n<li><a href=\"/kubernetes/#service\">Kubernetes Service</a></li>\n<li><p><a href=\"/kubernetes-ingress/\">Kubernetes Ingress</a></p>\n<p><a href=\"/gateway/\">Gateway</a></p>\n</li>\n</ul>\n<h2 id=\"分布式服务弹性与容错\"><a href=\"#分布式服务弹性与容错\" class=\"headerlink\" title=\"分布式服务弹性与容错\"></a>分布式服务弹性与容错</h2><ul>\n<li>弹性服务</li>\n<li>服务降级</li>\n<li>线程池/信号隔离</li>\n<li><p>快速解决依赖隔离</p>\n<p><a href=\"/Hystrix/\">Hystrix架构设计</a></p>\n</li>\n</ul>\n<h2 id=\"日志管理\"><a href=\"#日志管理\" class=\"headerlink\" title=\"日志管理\"></a>日志管理</h2><ul>\n<li>ELK Stack（LogStash -&gt; ES -&gt; Kibana）</li>\n</ul>\n<h2 id=\"分布式跟踪\"><a href=\"#分布式跟踪\" class=\"headerlink\" title=\"分布式跟踪\"></a>分布式跟踪</h2><ul>\n<li>Zipkin</li>\n<li><p>SpringCloud Sleuth</p>\n<p><a href=\"/zipkin/\">Zipkin</a> is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. It manages both the collection and lookup of this data. Zipkin’s design is based on the Google Dapper paper.</p>\n<p>Applications are instrumented to report timing data to Zipkin. The Zipkin UI also presents a Dependency diagram showing how many traced requests went through each application. If you are troubleshooting latency problems or errors, you can filter or sort all traces based on the application, length of trace, annotation, or timestamp. Once you select a trace, you can see the percentage of the total trace time each span takes which allows you to identify the problem application.</p>\n</li>\n</ul>\n<h2 id=\"监控与度量\"><a href=\"#监控与度量\" class=\"headerlink\" title=\"监控与度量\"></a>监控与度量</h2><ul>\n<li><p>Application/Infrastructure monitoring using StatsD + Graphite + Grafana</p>\n<p><a href=\"/sgg/\">StatsD + Graphite + Grafana</a></p>\n</li>\n</ul>\n<h2 id=\"服务安全\"><a href=\"#服务安全\" class=\"headerlink\" title=\"服务安全\"></a>服务安全</h2><ul>\n<li>SpringCloud Security</li>\n</ul>\n<h2 id=\"Auto-Scaling\"><a href=\"#Auto-Scaling\" class=\"headerlink\" title=\"Auto Scaling\"></a>Auto Scaling</h2><ul>\n<li><a href=\"/kubernetes/#Autoscaling\">Kubernetes Autoscaling</a></li>\n</ul>\n<h2 id=\"打包部署和调度部署\"><a href=\"#打包部署和调度部署\" class=\"headerlink\" title=\"打包部署和调度部署\"></a>打包部署和调度部署</h2><ul>\n<li>Spring Boot；</li>\n<li><a href=\"/kubernetes/#Deployment\">Docker／Rkt、Kubernetes Scheduler&amp;Deployment</a></li>\n</ul>\n<h2 id=\"任务工作管理\"><a href=\"#任务工作管理\" class=\"headerlink\" title=\"任务工作管理\"></a>任务工作管理</h2><ul>\n<li>Spring Batch</li>\n<li><a href=\"/kubernetes/#Job\">Kubernetes Jobs</a></li>\n</ul>\n<h2 id=\"分布式存储\"><a href=\"#分布式存储\" class=\"headerlink\" title=\"分布式存储\"></a>分布式存储</h2><h3 id=\"持久化-分布式文件系统\"><a href=\"#持久化-分布式文件系统\" class=\"headerlink\" title=\"持久化 - 分布式文件系统\"></a>持久化 - 分布式文件系统</h3><ul>\n<li><a href=\"/hdfs/\">HDFS分布式文件系统</a></li>\n</ul>\n<h3 id=\"持久化-分布式数据库\"><a href=\"#持久化-分布式数据库\" class=\"headerlink\" title=\"持久化 - 分布式数据库\"></a>持久化 - 分布式数据库</h3><ul>\n<li>[传统关系型数据库集群,如MySQL Cluster]</li>\n<li><a href=\"/mongo/\">Mongo</a></li>\n<li><a href=\"/hbase/\">Cassandra,HBase</a></li>\n</ul>\n<h3 id=\"非持久化-分布式缓存-消息系统\"><a href=\"#非持久化-分布式缓存-消息系统\" class=\"headerlink\" title=\"非持久化 - 分布式缓存/消息系统\"></a>非持久化 - 分布式缓存/消息系统</h3><ul>\n<li><a href=\"/kafka/\">Kafka</a></li>\n<li>[Redis]</li>\n</ul>\n<h2 id=\"分布式计算框架\"><a href=\"#分布式计算框架\" class=\"headerlink\" title=\"分布式计算框架\"></a>分布式计算框架</h2><ul>\n<li><a href=\"/yarn/\">YARN分布式计算框架</a></li>\n<li><a href=\"/yarn-appdev/\">YARN应用开发的几种方式</a></li>\n<li><a href=\"/running-spark-on-yarn/\">Running Spark on YARN</a></li>\n<li><a href=\"spark.md\">Spark Big Data Analytics</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"分布式配置管理\"><a href=\"#分布式配置管理\" class=\"headerlink\" title=\"分布式配置管理\"></a>分布式配置管理</h2><ul>\n<li><a href=\"/consul\">Consul</a></li>\n<li>Archaius</li>\n<li><a href=\"/kubernetes-configmap/\">Kubernetes ConfigMap</a></li>\n<li><a href=\"/kubernetes-secrets/\">Kubernetes Secrets</a></li>\n</ul>\n<p>配置的集中管理：采用consul的KV，将所有微服务的application.properties中的配置内容存入consul。</p>\n<p>配置的动态管理：采用archaius，将consul上的配置信息读到spring的PropertySource和archaius的PollResult中，当修改了配置信息后，经常改变的值通过DynamicFactory来获取，不经常改变的值可以通过其他方式获取. 大部分情况下，修改了consul上的配置信息后，相应的项目不需要重启，也会读到最新的值。</p>\n<h2 id=\"服务注册与发现\"><a href=\"#服务注册与发现\" class=\"headerlink\" title=\"服务注册与发现\"></a>服务注册与发现</h2><ul>\n<li><a href=\"/consul/\">Consul</a></li>\n<li>Eureka</li>\n<li><a href=\"/kubernetes/#service\">Kubernetes Service</a></li>\n</ul>\n<h2 id=\"负载平衡\"><a href=\"#负载平衡\" class=\"headerlink\" title=\"负载平衡\"></a>负载平衡</h2><ul>\n<li>Netflix Ribbon（Spring Cloud）</li>\n<li><p><a href=\"/kubernetes/#service\">Kubernetes Service</a></p>\n<p><a href=\"/consul/\">Client Side Load Balancing</a></p>\n</li>\n</ul>\n<h2 id=\"API网关与智能路由\"><a href=\"#API网关与智能路由\" class=\"headerlink\" title=\"API网关与智能路由\"></a>API网关与智能路由</h2><ul>\n<li>Netflix Zuul（SpringCloud）</li>\n<li><a href=\"/kubernetes/#service\">Kubernetes Service</a></li>\n<li><p><a href=\"/kubernetes-ingress/\">Kubernetes Ingress</a></p>\n<p><a href=\"/gateway/\">Gateway</a></p>\n</li>\n</ul>\n<h2 id=\"分布式服务弹性与容错\"><a href=\"#分布式服务弹性与容错\" class=\"headerlink\" title=\"分布式服务弹性与容错\"></a>分布式服务弹性与容错</h2><ul>\n<li>弹性服务</li>\n<li>服务降级</li>\n<li>线程池/信号隔离</li>\n<li><p>快速解决依赖隔离</p>\n<p><a href=\"/Hystrix/\">Hystrix架构设计</a></p>\n</li>\n</ul>\n<h2 id=\"日志管理\"><a href=\"#日志管理\" class=\"headerlink\" title=\"日志管理\"></a>日志管理</h2><ul>\n<li>ELK Stack（LogStash -&gt; ES -&gt; Kibana）</li>\n</ul>\n<h2 id=\"分布式跟踪\"><a href=\"#分布式跟踪\" class=\"headerlink\" title=\"分布式跟踪\"></a>分布式跟踪</h2><ul>\n<li>Zipkin</li>\n<li><p>SpringCloud Sleuth</p>\n<p><a href=\"/zipkin/\">Zipkin</a> is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. It manages both the collection and lookup of this data. Zipkin’s design is based on the Google Dapper paper.</p>\n<p>Applications are instrumented to report timing data to Zipkin. The Zipkin UI also presents a Dependency diagram showing how many traced requests went through each application. If you are troubleshooting latency problems or errors, you can filter or sort all traces based on the application, length of trace, annotation, or timestamp. Once you select a trace, you can see the percentage of the total trace time each span takes which allows you to identify the problem application.</p>\n</li>\n</ul>\n<h2 id=\"监控与度量\"><a href=\"#监控与度量\" class=\"headerlink\" title=\"监控与度量\"></a>监控与度量</h2><ul>\n<li><p>Application/Infrastructure monitoring using StatsD + Graphite + Grafana</p>\n<p><a href=\"/sgg/\">StatsD + Graphite + Grafana</a></p>\n</li>\n</ul>\n<h2 id=\"服务安全\"><a href=\"#服务安全\" class=\"headerlink\" title=\"服务安全\"></a>服务安全</h2><ul>\n<li>SpringCloud Security</li>\n</ul>\n<h2 id=\"Auto-Scaling\"><a href=\"#Auto-Scaling\" class=\"headerlink\" title=\"Auto Scaling\"></a>Auto Scaling</h2><ul>\n<li><a href=\"/kubernetes/#Autoscaling\">Kubernetes Autoscaling</a></li>\n</ul>\n<h2 id=\"打包部署和调度部署\"><a href=\"#打包部署和调度部署\" class=\"headerlink\" title=\"打包部署和调度部署\"></a>打包部署和调度部署</h2><ul>\n<li>Spring Boot；</li>\n<li><a href=\"/kubernetes/#Deployment\">Docker／Rkt、Kubernetes Scheduler&amp;Deployment</a></li>\n</ul>\n<h2 id=\"任务工作管理\"><a href=\"#任务工作管理\" class=\"headerlink\" title=\"任务工作管理\"></a>任务工作管理</h2><ul>\n<li>Spring Batch</li>\n<li><a href=\"/kubernetes/#Job\">Kubernetes Jobs</a></li>\n</ul>\n<h2 id=\"分布式存储\"><a href=\"#分布式存储\" class=\"headerlink\" title=\"分布式存储\"></a>分布式存储</h2><h3 id=\"持久化-分布式文件系统\"><a href=\"#持久化-分布式文件系统\" class=\"headerlink\" title=\"持久化 - 分布式文件系统\"></a>持久化 - 分布式文件系统</h3><ul>\n<li><a href=\"/hdfs/\">HDFS分布式文件系统</a></li>\n</ul>\n<h3 id=\"持久化-分布式数据库\"><a href=\"#持久化-分布式数据库\" class=\"headerlink\" title=\"持久化 - 分布式数据库\"></a>持久化 - 分布式数据库</h3><ul>\n<li>[传统关系型数据库集群,如MySQL Cluster]</li>\n<li><a href=\"/mongo/\">Mongo</a></li>\n<li><a href=\"/hbase/\">Cassandra,HBase</a></li>\n</ul>\n<h3 id=\"非持久化-分布式缓存-消息系统\"><a href=\"#非持久化-分布式缓存-消息系统\" class=\"headerlink\" title=\"非持久化 - 分布式缓存/消息系统\"></a>非持久化 - 分布式缓存/消息系统</h3><ul>\n<li><a href=\"/kafka/\">Kafka</a></li>\n<li>[Redis]</li>\n</ul>\n<h2 id=\"分布式计算框架\"><a href=\"#分布式计算框架\" class=\"headerlink\" title=\"分布式计算框架\"></a>分布式计算框架</h2><ul>\n<li><a href=\"/yarn/\">YARN分布式计算框架</a></li>\n<li><a href=\"/yarn-appdev/\">YARN应用开发的几种方式</a></li>\n<li><a href=\"/running-spark-on-yarn/\">Running Spark on YARN</a></li>\n<li><a href=\"spark.md\">Spark Big Data Analytics</a></li>\n</ul>\n"},{"title":"分布式服务弹性框架","date":"2016-05-09T12:46:25.000Z","_content":"\n### Overview\n在复杂的分布式 架构 的应用程序有很多的依赖，都会不可避免地在某些时候失败。高并发的依赖失败时如果没有隔离措施，当前应用服务就有被拖垮的风险。\nHystrix 是Netflix开源的一个针对分布式系统的延迟和容错库，由Java写成。\n```Example\n例如:一个依赖30个SOA服务的系统,每个服务99.99%可用。\n99.99%的30次方 ≈ 99.7%\n0.3% 意味着一亿次请求 会有 3,000,00次失败\n换算成时间大约每月有2个小时服务不稳定.\n随着服务依赖数量的变多，服务不稳定的概率会成指数性提高.\n解决问题方案:对依赖做隔离,Hystrix就是处理依赖隔离的框架,同时也是可以帮我们做依赖服务的治理和监控.\n\n```\n\n1）Hystrix使用命令模式HystrixCommand(Command)包装依赖调用逻辑，每个命令在单独线程中/信号 授权 下执行\n\n2）提供熔断器组件,可以自动运行或手动调用,停止当前依赖一段时间(10秒)，熔断器默认 错误 率阈值为50%,超过将自动运行。\n\n3）可配置依赖调用 超时 时间,超时时间一般设为比99.5%平均时间略高即可.当调用超时时，直接返回或执行fallback逻辑。\n\n4）为每个依赖提供一个小的线程池（或信号），如果线程池已满调用将被立即拒绝，默认不采用排队.加速失败判定时间。\n\n5）依赖调用结果分:成功，失败（抛出 异常 ），超时，线程拒绝，短路。 请求失败(异常，拒绝，超时，短路)时执行fallback(降级)逻辑。\n\n![依赖架构](https://github.com/Netflix/Hystrix/wiki/images/soa-4-isolation-640.png)\n### 设计思想\nHystrixCommand.execute方法实际上是调用了HystrixCommand.queue().get()，而queue方法除了最终调用run之外，还需要为run方法提供超时和异常等保护功能，外部也不能直接调用非安全的run方法.\n\n1.Hystrix可以为分布式服务提供弹性保护\n\n2.Hystrix通过命令模式封装调用，来实现弹性保护，继承HystrixCommand并且实现run方法，就完成了最简单的封装。\n\n3.实现getFallBack方法可以为熔断或者异常提供后备处理方法。\n\n![Command设计模式](http://img2.tuicool.com/JrQNFzN.png!web)\n```Sample\npublic class CommandHelloWorld extends HystrixCommand<String> {\n\n  private final String name;\n\n  public CommandHelloWorld(String name) {\n    super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"HelloServiceGroup\"))\n        .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(500)));\n    this.name = name;\n  }\n\n  @Override\n  protected String run() throws InterruptedException {\n    Thread.sleep(600);\n\n    return \"Hello \" + name + \"!\";\n  }\n\n  @Override\n  protected String getFallback() {\n    return String.format(\"[FallBack]Hello %s!\", name);\n  }\n}\n```\n\nEnable Hystrix in Spring Boot Application\n```\n@EnableHystrix\n@EnableHystrixDashboard\npublic class Application {\n```\n### How It works\n\n![9个步骤](https://github.com/Netflix/Hystrix/wiki/images/hystrix-command-flow-chart.png)\n\n流程说明:\n- 1:每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中.\n- 2:执行execute()/queue做同步或异步调用.\n- 3:判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤.\n- 4:判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤.\n- 5:调用HystrixCommand的run方法.运行依赖逻辑\n - 5a:依赖逻辑调用超时,进入步骤8.\n- 6:判断逻辑是否调用成功\n - 6a:返回成功调用结果\n - 6b:调用出错，进入步骤8.\n- 7:计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态.\n- 8:getFallback()降级逻辑.\n  以下四种情况将触发getFallback调用：\n (1):run()方法抛出非HystrixBadRequestException异常。\n (2):run()方法调用超时\n (3):熔断器开启拦截调用\n (4):线程池/队列/信号量是否跑满\n - 8a:没有实现getFallback的Command将直接抛出异常\n - 8b:fallback降级逻辑调用成功直接返回\n - 8c:降级逻辑调用失败抛出异常\n- 9:返回执行成功结果\n\n### Circuit Breaker 流程架构和统计\n每个熔断器默认维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态，\n默认错误超过50%且10秒内超过20个请求进行中断拦截.\n\n![](https://github.com/Netflix/Hystrix/wiki/images/circuit-breaker-640.png)\n\n### 隔离(Isolation)分析\n#### 线程隔离\n\n把执行依赖代码的线程与请求线程(如:jetty线程)分离，请求线程可以自由控制离开的时间(异步过程)。\n通过线程池大小可以控制并发量，当线程池饱和时可以提前拒绝服务,防止依赖问题扩散。\n线上建议线程池不要设置过大，否则大量堵塞线程有可能会拖慢服务器。\n- 线程隔离的优点:\n\n  - 使用线程可以完全隔离第三方代码,请求线程可以快速放回。当一个失败的依赖再次变成可用时，线程池将清理，并立即恢复可用，而不是一个长时间的恢复。\n - 可以完全模拟异步调用，方便异步编程。\n\n- 线程隔离的缺点:\n\n  - 线程池的主要缺点是它增加了cpu，因为每个命令的执行涉及到排队(默认使用SynchronousQueue避免排队)，调度和上下文切换。\n  - 对使用ThreadLocal等依赖线程状态的代码增加复杂性，需要手动传递和清理线程状态。\n\n  - NOTE: Netflix公司内部认为线程隔离开销足够小，不会造成重大的成本或性能的影响。\n  - Netflix 内部API 每天100亿的HystrixCommand依赖请求使用线程隔，每个应用大约40多个线程池，每个线程池大约5-20个线程。      \n\n#### 信号隔离\n信号隔离也可以用于限制并发访问，防止阻塞扩散, 与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请）.\n\n如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离,降低开销.\n信号量的大小可以动态调整, 线程池大小不可以.\n\n线程隔离与信号隔离区别如下图:\n\n![](https://github.com/Netflix/Hystrix/wiki/images/isolation-options-640.png)\n\n### Monitor Dashboard\n\nDocker Image for this dashboard:\n\n$ docker run -d -p 7979:7979 kennedyoliveira/hystrix-dashboard hystrix-dashboard\n\n在Hystrix Dashboard中输入Hystrix Stream: http://localhost:8080/hystrix.stream\n\n#### Hystrix 指标流(Hystrix Metrics Stream)\nTo enable the Hystrix metrics stream include a dependency on spring-boot-starter-actuator. This will expose the /hystrix.stream as a management endpoint.\n\n使Hystrix指标流包括依赖于spring-boot-starter-actuator。这将使/hystrix.stream流作为一个管理端点。\n```\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-actuator</artifactId>\n    </dependency>\n```\n#### 断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)\n\nHystrix的主要作用是会采集每一个HystrixCommand的信息指标,把每一个断路器的信息指标显示的Hystrix仪表盘上。\n运行Hystrix仪表板需要在spring boot主类上标注@EnableHystrixDashboard。然后访问/hystrix查看仪表盘，在hystrix客户端应用使用/hystrix.stream监控。\n\n![dashboard](https://github.com/Netflix/Hystrix/wiki/images/dashboard-annoted-circuit-640.png)\n","source":"_posts/Hystrix.md","raw":"---\ntitle: 分布式服务弹性框架\ndate: 2016-5-9 20:46:25\n---\n\n### Overview\n在复杂的分布式 架构 的应用程序有很多的依赖，都会不可避免地在某些时候失败。高并发的依赖失败时如果没有隔离措施，当前应用服务就有被拖垮的风险。\nHystrix 是Netflix开源的一个针对分布式系统的延迟和容错库，由Java写成。\n```Example\n例如:一个依赖30个SOA服务的系统,每个服务99.99%可用。\n99.99%的30次方 ≈ 99.7%\n0.3% 意味着一亿次请求 会有 3,000,00次失败\n换算成时间大约每月有2个小时服务不稳定.\n随着服务依赖数量的变多，服务不稳定的概率会成指数性提高.\n解决问题方案:对依赖做隔离,Hystrix就是处理依赖隔离的框架,同时也是可以帮我们做依赖服务的治理和监控.\n\n```\n\n1）Hystrix使用命令模式HystrixCommand(Command)包装依赖调用逻辑，每个命令在单独线程中/信号 授权 下执行\n\n2）提供熔断器组件,可以自动运行或手动调用,停止当前依赖一段时间(10秒)，熔断器默认 错误 率阈值为50%,超过将自动运行。\n\n3）可配置依赖调用 超时 时间,超时时间一般设为比99.5%平均时间略高即可.当调用超时时，直接返回或执行fallback逻辑。\n\n4）为每个依赖提供一个小的线程池（或信号），如果线程池已满调用将被立即拒绝，默认不采用排队.加速失败判定时间。\n\n5）依赖调用结果分:成功，失败（抛出 异常 ），超时，线程拒绝，短路。 请求失败(异常，拒绝，超时，短路)时执行fallback(降级)逻辑。\n\n![依赖架构](https://github.com/Netflix/Hystrix/wiki/images/soa-4-isolation-640.png)\n### 设计思想\nHystrixCommand.execute方法实际上是调用了HystrixCommand.queue().get()，而queue方法除了最终调用run之外，还需要为run方法提供超时和异常等保护功能，外部也不能直接调用非安全的run方法.\n\n1.Hystrix可以为分布式服务提供弹性保护\n\n2.Hystrix通过命令模式封装调用，来实现弹性保护，继承HystrixCommand并且实现run方法，就完成了最简单的封装。\n\n3.实现getFallBack方法可以为熔断或者异常提供后备处理方法。\n\n![Command设计模式](http://img2.tuicool.com/JrQNFzN.png!web)\n```Sample\npublic class CommandHelloWorld extends HystrixCommand<String> {\n\n  private final String name;\n\n  public CommandHelloWorld(String name) {\n    super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"HelloServiceGroup\"))\n        .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(500)));\n    this.name = name;\n  }\n\n  @Override\n  protected String run() throws InterruptedException {\n    Thread.sleep(600);\n\n    return \"Hello \" + name + \"!\";\n  }\n\n  @Override\n  protected String getFallback() {\n    return String.format(\"[FallBack]Hello %s!\", name);\n  }\n}\n```\n\nEnable Hystrix in Spring Boot Application\n```\n@EnableHystrix\n@EnableHystrixDashboard\npublic class Application {\n```\n### How It works\n\n![9个步骤](https://github.com/Netflix/Hystrix/wiki/images/hystrix-command-flow-chart.png)\n\n流程说明:\n- 1:每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中.\n- 2:执行execute()/queue做同步或异步调用.\n- 3:判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤.\n- 4:判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤.\n- 5:调用HystrixCommand的run方法.运行依赖逻辑\n - 5a:依赖逻辑调用超时,进入步骤8.\n- 6:判断逻辑是否调用成功\n - 6a:返回成功调用结果\n - 6b:调用出错，进入步骤8.\n- 7:计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态.\n- 8:getFallback()降级逻辑.\n  以下四种情况将触发getFallback调用：\n (1):run()方法抛出非HystrixBadRequestException异常。\n (2):run()方法调用超时\n (3):熔断器开启拦截调用\n (4):线程池/队列/信号量是否跑满\n - 8a:没有实现getFallback的Command将直接抛出异常\n - 8b:fallback降级逻辑调用成功直接返回\n - 8c:降级逻辑调用失败抛出异常\n- 9:返回执行成功结果\n\n### Circuit Breaker 流程架构和统计\n每个熔断器默认维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态，\n默认错误超过50%且10秒内超过20个请求进行中断拦截.\n\n![](https://github.com/Netflix/Hystrix/wiki/images/circuit-breaker-640.png)\n\n### 隔离(Isolation)分析\n#### 线程隔离\n\n把执行依赖代码的线程与请求线程(如:jetty线程)分离，请求线程可以自由控制离开的时间(异步过程)。\n通过线程池大小可以控制并发量，当线程池饱和时可以提前拒绝服务,防止依赖问题扩散。\n线上建议线程池不要设置过大，否则大量堵塞线程有可能会拖慢服务器。\n- 线程隔离的优点:\n\n  - 使用线程可以完全隔离第三方代码,请求线程可以快速放回。当一个失败的依赖再次变成可用时，线程池将清理，并立即恢复可用，而不是一个长时间的恢复。\n - 可以完全模拟异步调用，方便异步编程。\n\n- 线程隔离的缺点:\n\n  - 线程池的主要缺点是它增加了cpu，因为每个命令的执行涉及到排队(默认使用SynchronousQueue避免排队)，调度和上下文切换。\n  - 对使用ThreadLocal等依赖线程状态的代码增加复杂性，需要手动传递和清理线程状态。\n\n  - NOTE: Netflix公司内部认为线程隔离开销足够小，不会造成重大的成本或性能的影响。\n  - Netflix 内部API 每天100亿的HystrixCommand依赖请求使用线程隔，每个应用大约40多个线程池，每个线程池大约5-20个线程。      \n\n#### 信号隔离\n信号隔离也可以用于限制并发访问，防止阻塞扩散, 与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请）.\n\n如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离,降低开销.\n信号量的大小可以动态调整, 线程池大小不可以.\n\n线程隔离与信号隔离区别如下图:\n\n![](https://github.com/Netflix/Hystrix/wiki/images/isolation-options-640.png)\n\n### Monitor Dashboard\n\nDocker Image for this dashboard:\n\n$ docker run -d -p 7979:7979 kennedyoliveira/hystrix-dashboard hystrix-dashboard\n\n在Hystrix Dashboard中输入Hystrix Stream: http://localhost:8080/hystrix.stream\n\n#### Hystrix 指标流(Hystrix Metrics Stream)\nTo enable the Hystrix metrics stream include a dependency on spring-boot-starter-actuator. This will expose the /hystrix.stream as a management endpoint.\n\n使Hystrix指标流包括依赖于spring-boot-starter-actuator。这将使/hystrix.stream流作为一个管理端点。\n```\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-actuator</artifactId>\n    </dependency>\n```\n#### 断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)\n\nHystrix的主要作用是会采集每一个HystrixCommand的信息指标,把每一个断路器的信息指标显示的Hystrix仪表盘上。\n运行Hystrix仪表板需要在spring boot主类上标注@EnableHystrixDashboard。然后访问/hystrix查看仪表盘，在hystrix客户端应用使用/hystrix.stream监控。\n\n![dashboard](https://github.com/Netflix/Hystrix/wiki/images/dashboard-annoted-circuit-640.png)\n","slug":"Hystrix","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bfl0002i272k638skt7","content":"<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p>在复杂的分布式 架构 的应用程序有很多的依赖，都会不可避免地在某些时候失败。高并发的依赖失败时如果没有隔离措施，当前应用服务就有被拖垮的风险。<br>Hystrix 是Netflix开源的一个针对分布式系统的延迟和容错库，由Java写成。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">例如:一个依赖30个SOA服务的系统,每个服务99.99%可用。</div><div class=\"line\">99.99%的30次方 ≈ 99.7%</div><div class=\"line\">0.3% 意味着一亿次请求 会有 3,000,00次失败</div><div class=\"line\">换算成时间大约每月有2个小时服务不稳定.</div><div class=\"line\">随着服务依赖数量的变多，服务不稳定的概率会成指数性提高.</div><div class=\"line\">解决问题方案:对依赖做隔离,Hystrix就是处理依赖隔离的框架,同时也是可以帮我们做依赖服务的治理和监控.</div></pre></td></tr></table></figure></p>\n<p>1）Hystrix使用命令模式HystrixCommand(Command)包装依赖调用逻辑，每个命令在单独线程中/信号 授权 下执行</p>\n<p>2）提供熔断器组件,可以自动运行或手动调用,停止当前依赖一段时间(10秒)，熔断器默认 错误 率阈值为50%,超过将自动运行。</p>\n<p>3）可配置依赖调用 超时 时间,超时时间一般设为比99.5%平均时间略高即可.当调用超时时，直接返回或执行fallback逻辑。</p>\n<p>4）为每个依赖提供一个小的线程池（或信号），如果线程池已满调用将被立即拒绝，默认不采用排队.加速失败判定时间。</p>\n<p>5）依赖调用结果分:成功，失败（抛出 异常 ），超时，线程拒绝，短路。 请求失败(异常，拒绝，超时，短路)时执行fallback(降级)逻辑。</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/soa-4-isolation-640.png\" alt=\"依赖架构\"></p>\n<h3 id=\"设计思想\"><a href=\"#设计思想\" class=\"headerlink\" title=\"设计思想\"></a>设计思想</h3><p>HystrixCommand.execute方法实际上是调用了HystrixCommand.queue().get()，而queue方法除了最终调用run之外，还需要为run方法提供超时和异常等保护功能，外部也不能直接调用非安全的run方法.</p>\n<p>1.Hystrix可以为分布式服务提供弹性保护</p>\n<p>2.Hystrix通过命令模式封装调用，来实现弹性保护，继承HystrixCommand并且实现run方法，就完成了最简单的封装。</p>\n<p>3.实现getFallBack方法可以为熔断或者异常提供后备处理方法。</p>\n<p><img src=\"http://img2.tuicool.com/JrQNFzN.png!web\" alt=\"Command设计模式\"><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class CommandHelloWorld extends HystrixCommand&lt;String&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">  private final String name;</div><div class=\"line\"></div><div class=\"line\">  public CommandHelloWorld(String name) &#123;</div><div class=\"line\">    super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;HelloServiceGroup&quot;))</div><div class=\"line\">        .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(500)));</div><div class=\"line\">    this.name = name;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  @Override</div><div class=\"line\">  protected String run() throws InterruptedException &#123;</div><div class=\"line\">    Thread.sleep(600);</div><div class=\"line\"></div><div class=\"line\">    return &quot;Hello &quot; + name + &quot;!&quot;;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  @Override</div><div class=\"line\">  protected String getFallback() &#123;</div><div class=\"line\">    return String.format(&quot;[FallBack]Hello %s!&quot;, name);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Enable Hystrix in Spring Boot Application<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">@EnableHystrix</div><div class=\"line\">@EnableHystrixDashboard</div><div class=\"line\">public class Application &#123;</div></pre></td></tr></table></figure></p>\n<h3 id=\"How-It-works\"><a href=\"#How-It-works\" class=\"headerlink\" title=\"How It works\"></a>How It works</h3><p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/hystrix-command-flow-chart.png\" alt=\"9个步骤\"></p>\n<p>流程说明:</p>\n<ul>\n<li>1:每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中.</li>\n<li>2:执行execute()/queue做同步或异步调用.</li>\n<li>3:判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤.</li>\n<li>4:判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤.</li>\n<li>5:调用HystrixCommand的run方法.运行依赖逻辑<ul>\n<li>5a:依赖逻辑调用超时,进入步骤8.</li>\n</ul>\n</li>\n<li>6:判断逻辑是否调用成功<ul>\n<li>6a:返回成功调用结果</li>\n<li>6b:调用出错，进入步骤8.</li>\n</ul>\n</li>\n<li>7:计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态.</li>\n<li>8:getFallback()降级逻辑.<br>以下四种情况将触发getFallback调用：<br>(1):run()方法抛出非HystrixBadRequestException异常。<br>(2):run()方法调用超时<br>(3):熔断器开启拦截调用<br>(4):线程池/队列/信号量是否跑满<ul>\n<li>8a:没有实现getFallback的Command将直接抛出异常</li>\n<li>8b:fallback降级逻辑调用成功直接返回</li>\n<li>8c:降级逻辑调用失败抛出异常</li>\n</ul>\n</li>\n<li>9:返回执行成功结果</li>\n</ul>\n<h3 id=\"Circuit-Breaker-流程架构和统计\"><a href=\"#Circuit-Breaker-流程架构和统计\" class=\"headerlink\" title=\"Circuit Breaker 流程架构和统计\"></a>Circuit Breaker 流程架构和统计</h3><p>每个熔断器默认维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态，<br>默认错误超过50%且10秒内超过20个请求进行中断拦截.</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/circuit-breaker-640.png\" alt=\"\"></p>\n<h3 id=\"隔离-Isolation-分析\"><a href=\"#隔离-Isolation-分析\" class=\"headerlink\" title=\"隔离(Isolation)分析\"></a>隔离(Isolation)分析</h3><h4 id=\"线程隔离\"><a href=\"#线程隔离\" class=\"headerlink\" title=\"线程隔离\"></a>线程隔离</h4><p>把执行依赖代码的线程与请求线程(如:jetty线程)分离，请求线程可以自由控制离开的时间(异步过程)。<br>通过线程池大小可以控制并发量，当线程池饱和时可以提前拒绝服务,防止依赖问题扩散。<br>线上建议线程池不要设置过大，否则大量堵塞线程有可能会拖慢服务器。</p>\n<ul>\n<li><p>线程隔离的优点:</p>\n<ul>\n<li>使用线程可以完全隔离第三方代码,请求线程可以快速放回。当一个失败的依赖再次变成可用时，线程池将清理，并立即恢复可用，而不是一个长时间的恢复。</li>\n<li>可以完全模拟异步调用，方便异步编程。</li>\n</ul>\n</li>\n<li><p>线程隔离的缺点:</p>\n<ul>\n<li>线程池的主要缺点是它增加了cpu，因为每个命令的执行涉及到排队(默认使用SynchronousQueue避免排队)，调度和上下文切换。</li>\n<li><p>对使用ThreadLocal等依赖线程状态的代码增加复杂性，需要手动传递和清理线程状态。</p>\n</li>\n<li><p>NOTE: Netflix公司内部认为线程隔离开销足够小，不会造成重大的成本或性能的影响。</p>\n</li>\n<li>Netflix 内部API 每天100亿的HystrixCommand依赖请求使用线程隔，每个应用大约40多个线程池，每个线程池大约5-20个线程。      </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"信号隔离\"><a href=\"#信号隔离\" class=\"headerlink\" title=\"信号隔离\"></a>信号隔离</h4><p>信号隔离也可以用于限制并发访问，防止阻塞扩散, 与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请）.</p>\n<p>如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离,降低开销.<br>信号量的大小可以动态调整, 线程池大小不可以.</p>\n<p>线程隔离与信号隔离区别如下图:</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/isolation-options-640.png\" alt=\"\"></p>\n<h3 id=\"Monitor-Dashboard\"><a href=\"#Monitor-Dashboard\" class=\"headerlink\" title=\"Monitor Dashboard\"></a>Monitor Dashboard</h3><p>Docker Image for this dashboard:</p>\n<p>$ docker run -d -p 7979:7979 kennedyoliveira/hystrix-dashboard hystrix-dashboard</p>\n<p>在Hystrix Dashboard中输入Hystrix Stream: <a href=\"http://localhost:8080/hystrix.stream\" target=\"_blank\" rel=\"external\">http://localhost:8080/hystrix.stream</a></p>\n<h4 id=\"Hystrix-指标流-Hystrix-Metrics-Stream\"><a href=\"#Hystrix-指标流-Hystrix-Metrics-Stream\" class=\"headerlink\" title=\"Hystrix 指标流(Hystrix Metrics Stream)\"></a>Hystrix 指标流(Hystrix Metrics Stream)</h4><p>To enable the Hystrix metrics stream include a dependency on spring-boot-starter-actuator. This will expose the /hystrix.stream as a management endpoint.</p>\n<p>使Hystrix指标流包括依赖于spring-boot-starter-actuator。这将使/hystrix.stream流作为一个管理端点。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<h4 id=\"断路器-Hystrix-仪表盘-Circuit-Breaker-Hystrix-Dashboard\"><a href=\"#断路器-Hystrix-仪表盘-Circuit-Breaker-Hystrix-Dashboard\" class=\"headerlink\" title=\"断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)\"></a>断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)</h4><p>Hystrix的主要作用是会采集每一个HystrixCommand的信息指标,把每一个断路器的信息指标显示的Hystrix仪表盘上。<br>运行Hystrix仪表板需要在spring boot主类上标注@EnableHystrixDashboard。然后访问/hystrix查看仪表盘，在hystrix客户端应用使用/hystrix.stream监控。</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/dashboard-annoted-circuit-640.png\" alt=\"dashboard\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p>在复杂的分布式 架构 的应用程序有很多的依赖，都会不可避免地在某些时候失败。高并发的依赖失败时如果没有隔离措施，当前应用服务就有被拖垮的风险。<br>Hystrix 是Netflix开源的一个针对分布式系统的延迟和容错库，由Java写成。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">例如:一个依赖30个SOA服务的系统,每个服务99.99%可用。</div><div class=\"line\">99.99%的30次方 ≈ 99.7%</div><div class=\"line\">0.3% 意味着一亿次请求 会有 3,000,00次失败</div><div class=\"line\">换算成时间大约每月有2个小时服务不稳定.</div><div class=\"line\">随着服务依赖数量的变多，服务不稳定的概率会成指数性提高.</div><div class=\"line\">解决问题方案:对依赖做隔离,Hystrix就是处理依赖隔离的框架,同时也是可以帮我们做依赖服务的治理和监控.</div></pre></td></tr></table></figure></p>\n<p>1）Hystrix使用命令模式HystrixCommand(Command)包装依赖调用逻辑，每个命令在单独线程中/信号 授权 下执行</p>\n<p>2）提供熔断器组件,可以自动运行或手动调用,停止当前依赖一段时间(10秒)，熔断器默认 错误 率阈值为50%,超过将自动运行。</p>\n<p>3）可配置依赖调用 超时 时间,超时时间一般设为比99.5%平均时间略高即可.当调用超时时，直接返回或执行fallback逻辑。</p>\n<p>4）为每个依赖提供一个小的线程池（或信号），如果线程池已满调用将被立即拒绝，默认不采用排队.加速失败判定时间。</p>\n<p>5）依赖调用结果分:成功，失败（抛出 异常 ），超时，线程拒绝，短路。 请求失败(异常，拒绝，超时，短路)时执行fallback(降级)逻辑。</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/soa-4-isolation-640.png\" alt=\"依赖架构\"></p>\n<h3 id=\"设计思想\"><a href=\"#设计思想\" class=\"headerlink\" title=\"设计思想\"></a>设计思想</h3><p>HystrixCommand.execute方法实际上是调用了HystrixCommand.queue().get()，而queue方法除了最终调用run之外，还需要为run方法提供超时和异常等保护功能，外部也不能直接调用非安全的run方法.</p>\n<p>1.Hystrix可以为分布式服务提供弹性保护</p>\n<p>2.Hystrix通过命令模式封装调用，来实现弹性保护，继承HystrixCommand并且实现run方法，就完成了最简单的封装。</p>\n<p>3.实现getFallBack方法可以为熔断或者异常提供后备处理方法。</p>\n<p><img src=\"http://img2.tuicool.com/JrQNFzN.png!web\" alt=\"Command设计模式\"><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class CommandHelloWorld extends HystrixCommand&lt;String&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">  private final String name;</div><div class=\"line\"></div><div class=\"line\">  public CommandHelloWorld(String name) &#123;</div><div class=\"line\">    super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;HelloServiceGroup&quot;))</div><div class=\"line\">        .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(500)));</div><div class=\"line\">    this.name = name;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  @Override</div><div class=\"line\">  protected String run() throws InterruptedException &#123;</div><div class=\"line\">    Thread.sleep(600);</div><div class=\"line\"></div><div class=\"line\">    return &quot;Hello &quot; + name + &quot;!&quot;;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  @Override</div><div class=\"line\">  protected String getFallback() &#123;</div><div class=\"line\">    return String.format(&quot;[FallBack]Hello %s!&quot;, name);</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Enable Hystrix in Spring Boot Application<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">@EnableHystrix</div><div class=\"line\">@EnableHystrixDashboard</div><div class=\"line\">public class Application &#123;</div></pre></td></tr></table></figure></p>\n<h3 id=\"How-It-works\"><a href=\"#How-It-works\" class=\"headerlink\" title=\"How It works\"></a>How It works</h3><p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/hystrix-command-flow-chart.png\" alt=\"9个步骤\"></p>\n<p>流程说明:</p>\n<ul>\n<li>1:每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中.</li>\n<li>2:执行execute()/queue做同步或异步调用.</li>\n<li>3:判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤.</li>\n<li>4:判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤.</li>\n<li>5:调用HystrixCommand的run方法.运行依赖逻辑<ul>\n<li>5a:依赖逻辑调用超时,进入步骤8.</li>\n</ul>\n</li>\n<li>6:判断逻辑是否调用成功<ul>\n<li>6a:返回成功调用结果</li>\n<li>6b:调用出错，进入步骤8.</li>\n</ul>\n</li>\n<li>7:计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态.</li>\n<li>8:getFallback()降级逻辑.<br>以下四种情况将触发getFallback调用：<br>(1):run()方法抛出非HystrixBadRequestException异常。<br>(2):run()方法调用超时<br>(3):熔断器开启拦截调用<br>(4):线程池/队列/信号量是否跑满<ul>\n<li>8a:没有实现getFallback的Command将直接抛出异常</li>\n<li>8b:fallback降级逻辑调用成功直接返回</li>\n<li>8c:降级逻辑调用失败抛出异常</li>\n</ul>\n</li>\n<li>9:返回执行成功结果</li>\n</ul>\n<h3 id=\"Circuit-Breaker-流程架构和统计\"><a href=\"#Circuit-Breaker-流程架构和统计\" class=\"headerlink\" title=\"Circuit Breaker 流程架构和统计\"></a>Circuit Breaker 流程架构和统计</h3><p>每个熔断器默认维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态，<br>默认错误超过50%且10秒内超过20个请求进行中断拦截.</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/circuit-breaker-640.png\" alt=\"\"></p>\n<h3 id=\"隔离-Isolation-分析\"><a href=\"#隔离-Isolation-分析\" class=\"headerlink\" title=\"隔离(Isolation)分析\"></a>隔离(Isolation)分析</h3><h4 id=\"线程隔离\"><a href=\"#线程隔离\" class=\"headerlink\" title=\"线程隔离\"></a>线程隔离</h4><p>把执行依赖代码的线程与请求线程(如:jetty线程)分离，请求线程可以自由控制离开的时间(异步过程)。<br>通过线程池大小可以控制并发量，当线程池饱和时可以提前拒绝服务,防止依赖问题扩散。<br>线上建议线程池不要设置过大，否则大量堵塞线程有可能会拖慢服务器。</p>\n<ul>\n<li><p>线程隔离的优点:</p>\n<ul>\n<li>使用线程可以完全隔离第三方代码,请求线程可以快速放回。当一个失败的依赖再次变成可用时，线程池将清理，并立即恢复可用，而不是一个长时间的恢复。</li>\n<li>可以完全模拟异步调用，方便异步编程。</li>\n</ul>\n</li>\n<li><p>线程隔离的缺点:</p>\n<ul>\n<li>线程池的主要缺点是它增加了cpu，因为每个命令的执行涉及到排队(默认使用SynchronousQueue避免排队)，调度和上下文切换。</li>\n<li><p>对使用ThreadLocal等依赖线程状态的代码增加复杂性，需要手动传递和清理线程状态。</p>\n</li>\n<li><p>NOTE: Netflix公司内部认为线程隔离开销足够小，不会造成重大的成本或性能的影响。</p>\n</li>\n<li>Netflix 内部API 每天100亿的HystrixCommand依赖请求使用线程隔，每个应用大约40多个线程池，每个线程池大约5-20个线程。      </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"信号隔离\"><a href=\"#信号隔离\" class=\"headerlink\" title=\"信号隔离\"></a>信号隔离</h4><p>信号隔离也可以用于限制并发访问，防止阻塞扩散, 与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请）.</p>\n<p>如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离,降低开销.<br>信号量的大小可以动态调整, 线程池大小不可以.</p>\n<p>线程隔离与信号隔离区别如下图:</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/isolation-options-640.png\" alt=\"\"></p>\n<h3 id=\"Monitor-Dashboard\"><a href=\"#Monitor-Dashboard\" class=\"headerlink\" title=\"Monitor Dashboard\"></a>Monitor Dashboard</h3><p>Docker Image for this dashboard:</p>\n<p>$ docker run -d -p 7979:7979 kennedyoliveira/hystrix-dashboard hystrix-dashboard</p>\n<p>在Hystrix Dashboard中输入Hystrix Stream: <a href=\"http://localhost:8080/hystrix.stream\" target=\"_blank\" rel=\"external\">http://localhost:8080/hystrix.stream</a></p>\n<h4 id=\"Hystrix-指标流-Hystrix-Metrics-Stream\"><a href=\"#Hystrix-指标流-Hystrix-Metrics-Stream\" class=\"headerlink\" title=\"Hystrix 指标流(Hystrix Metrics Stream)\"></a>Hystrix 指标流(Hystrix Metrics Stream)</h4><p>To enable the Hystrix metrics stream include a dependency on spring-boot-starter-actuator. This will expose the /hystrix.stream as a management endpoint.</p>\n<p>使Hystrix指标流包括依赖于spring-boot-starter-actuator。这将使/hystrix.stream流作为一个管理端点。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</div><div class=\"line\">    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;</div><div class=\"line\">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>\n<h4 id=\"断路器-Hystrix-仪表盘-Circuit-Breaker-Hystrix-Dashboard\"><a href=\"#断路器-Hystrix-仪表盘-Circuit-Breaker-Hystrix-Dashboard\" class=\"headerlink\" title=\"断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)\"></a>断路器: Hystrix 仪表盘(Circuit Breaker: Hystrix Dashboard)</h4><p>Hystrix的主要作用是会采集每一个HystrixCommand的信息指标,把每一个断路器的信息指标显示的Hystrix仪表盘上。<br>运行Hystrix仪表板需要在spring boot主类上标注@EnableHystrixDashboard。然后访问/hystrix查看仪表盘，在hystrix客户端应用使用/hystrix.stream监控。</p>\n<p><img src=\"https://github.com/Netflix/Hystrix/wiki/images/dashboard-annoted-circuit-640.png\" alt=\"dashboard\"></p>\n"},{"title":"Consul","date":"2017-01-09T12:46:25.000Z","_content":"\n### Consul 服务注册管理\n#### 启动Consul Docker\n\n```\ndocker run -d -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e 'CONSUL_LOCAL_CONFIG={\"acl_datacenter\":\"dc1\",\"acl_default_policy”:\"write\",\"acl_down_policy\":\"extend-cache\",\"acl_master_token\":\"the_one_ring\",\"bootstrap_expect\":1,\"datacenter\":\"dc1\",\"data_dir\":\"/usr/local/bin/consul.d/data\",\"server\":true}' consul agent -server -bind=127.0.0.1 -client=0.0.0.0\n```\n\nor\n\n```\ndocker run -d --name myconsul -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e 'CONSUL_LOCAL_CONFIG={\"acl_datacenter\":\"dc1\",\"acl_default_policy\":\"allow\",\"acl_down_policy\":\"extend-cache\",\"acl_master_token\":\"the_one_ring\",\"bootstrap_expect\":1,\"datacenter\":\"dc1\",\"data_dir\":\"/usr/local/bin/consul.d/data\",\"server\":true}' consul agent -server -bind=127.0.0.1 -client=0.0.0.0 -ui\n```\n\n#### 1. 注册服务:\n\nhttp://{IP}:8500/v1/agent/service/register\n\nPUT\n```\n{\n    \"id\": \"Analytics_Job_Free_JobID0001\",\n    \"name\": \"Analytics_Job_Free_JobID0001\",\n    \"tags\": [\n        \"RTI\"\n    ]\n}\n```\n\n#### 2. 注销服务：\n\nhttp://{IP}:8500/v1/agent/service/deregister/jetty\n\n\n#### 3. 注册check\n\nhttp://{IP}:8500/v1/agent/check/register\n\nPUT\n```\n{  \n            \"http\": \"http://192.168.1.200:8080/health?appId=app001\",  \n            \"interval\": “20s\",\n            \"id\": \"app001\",\n              \"name\": \"Analytics YARN Application 001\",\n            \"applicationId\":\"app001\",\n            \"service_id\":\"Analytics_Job_Free_JobID0001\"\n        }\n\n{  \n            \"http\": \"http://192.168.1.200:8080/health?appId=app002\",  \n            \"interval\": “20s\",\n            \"id\": \"app002\",\n              \"name\": \"Analytics YARN Application 002\",\n            \"applicationId\":\"app002”,\n        \"status\": \"passing\",\n            \"service_id\":\"Analytics_Job_Free_JobID0001\"\n        }\n```\n#### 4. 注销check\n\nhttp://{IP}:8500/v1/agent/check/deregister/app002\n\n\n### 例子\n\n```\n<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-consul-discovery</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency\n```\n\napplication.properties\n```\nserver.port=9955  \n\nspring.application.name=SampleClient\nspring.cloud.consul.host=127.0.0.1\nspring.cloud.consul.port=8500\nspring.cloud.consul.enabled=true\nspring.cloud.consul.discovery.register=false\nspring.cloud.consul.discovery.enabled=true\nspring.cloud.consul.discovery.instanceId=tomcat1\nspring.cloud.consul.discovery.serviceName=tomcat1  \nspring.cloud.consul.discovery.hostname=127.0.0.1\nspring.cloud.consul.discovery.port=${server.port}\nspring.cloud.consul.discovery.healthCheckUrl=http://127.0.0.1:9955/health  \nspring.cloud.consul.discovery.healthCheckInterval=10s  \nspring.cloud.consul.discovery.tags=dev\n```\n\napplication.yml\n```\nsay-hello:\n ribbon:\n  okhttp:\n    enabled: true\n```  \nEnable\n```\n@SpringBootApplication\n@EnableDiscoveryClient\n@RibbonClient(name = \"say-hello\", configuration = SayHelloConfiguration.class)\npublic class Application {\n```\n\nFull code\n```\npackage hello;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.client.discovery.DiscoveryClient;\nimport org.springframework.cloud.client.loadbalancer.LoadBalanced;\nimport org.springframework.cloud.client.loadbalancer.LoadBalancerClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\n@RestController\npublic class HelloController {\n\n  @Autowired\n  private LoadBalancerClient loadBalancer;\n  @Autowired\n  private DiscoveryClient discoveryClient;\n\n  @LoadBalanced\n  @Bean\n  RestTemplate restTemplate() {\n    return new RestTemplate();\n  }\n\n  @Autowired\n  RestTemplate restTemplate;\n\n  /**\n   * 从所有服务中选择一个服务（轮询）\n   */\n  @RequestMapping(\"/discover\")\n  public Object discover() {\n    // return loadBalancer.choose(\"application\").getUri().toString();\n    String greeting = this.restTemplate.getForObject(\"http://application\", String.class);\n    return String.format(\"%s!\", greeting);\n  }\n\n  @RequestMapping(\"/d\")\n  public Object d() {\n    return loadBalancer.choose(\"application\").getUri().toString();\n\n  }\n\n  /**\n   * 获取所有服务\n   */\n  @RequestMapping(\"/services\")\n  public Object services() {\n    return discoveryClient.getInstances(\"application\").stream().findFirst().get().getUri();\n  }\n\n}\n\n```\n","source":"_posts/consul.md","raw":"---\ntitle: Consul\ndate: 2017-1-9 20:46:25\n---\n\n### Consul 服务注册管理\n#### 启动Consul Docker\n\n```\ndocker run -d -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e 'CONSUL_LOCAL_CONFIG={\"acl_datacenter\":\"dc1\",\"acl_default_policy”:\"write\",\"acl_down_policy\":\"extend-cache\",\"acl_master_token\":\"the_one_ring\",\"bootstrap_expect\":1,\"datacenter\":\"dc1\",\"data_dir\":\"/usr/local/bin/consul.d/data\",\"server\":true}' consul agent -server -bind=127.0.0.1 -client=0.0.0.0\n```\n\nor\n\n```\ndocker run -d --name myconsul -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e 'CONSUL_LOCAL_CONFIG={\"acl_datacenter\":\"dc1\",\"acl_default_policy\":\"allow\",\"acl_down_policy\":\"extend-cache\",\"acl_master_token\":\"the_one_ring\",\"bootstrap_expect\":1,\"datacenter\":\"dc1\",\"data_dir\":\"/usr/local/bin/consul.d/data\",\"server\":true}' consul agent -server -bind=127.0.0.1 -client=0.0.0.0 -ui\n```\n\n#### 1. 注册服务:\n\nhttp://{IP}:8500/v1/agent/service/register\n\nPUT\n```\n{\n    \"id\": \"Analytics_Job_Free_JobID0001\",\n    \"name\": \"Analytics_Job_Free_JobID0001\",\n    \"tags\": [\n        \"RTI\"\n    ]\n}\n```\n\n#### 2. 注销服务：\n\nhttp://{IP}:8500/v1/agent/service/deregister/jetty\n\n\n#### 3. 注册check\n\nhttp://{IP}:8500/v1/agent/check/register\n\nPUT\n```\n{  \n            \"http\": \"http://192.168.1.200:8080/health?appId=app001\",  \n            \"interval\": “20s\",\n            \"id\": \"app001\",\n              \"name\": \"Analytics YARN Application 001\",\n            \"applicationId\":\"app001\",\n            \"service_id\":\"Analytics_Job_Free_JobID0001\"\n        }\n\n{  \n            \"http\": \"http://192.168.1.200:8080/health?appId=app002\",  \n            \"interval\": “20s\",\n            \"id\": \"app002\",\n              \"name\": \"Analytics YARN Application 002\",\n            \"applicationId\":\"app002”,\n        \"status\": \"passing\",\n            \"service_id\":\"Analytics_Job_Free_JobID0001\"\n        }\n```\n#### 4. 注销check\n\nhttp://{IP}:8500/v1/agent/check/deregister/app002\n\n\n### 例子\n\n```\n<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-consul-discovery</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency\n```\n\napplication.properties\n```\nserver.port=9955  \n\nspring.application.name=SampleClient\nspring.cloud.consul.host=127.0.0.1\nspring.cloud.consul.port=8500\nspring.cloud.consul.enabled=true\nspring.cloud.consul.discovery.register=false\nspring.cloud.consul.discovery.enabled=true\nspring.cloud.consul.discovery.instanceId=tomcat1\nspring.cloud.consul.discovery.serviceName=tomcat1  \nspring.cloud.consul.discovery.hostname=127.0.0.1\nspring.cloud.consul.discovery.port=${server.port}\nspring.cloud.consul.discovery.healthCheckUrl=http://127.0.0.1:9955/health  \nspring.cloud.consul.discovery.healthCheckInterval=10s  \nspring.cloud.consul.discovery.tags=dev\n```\n\napplication.yml\n```\nsay-hello:\n ribbon:\n  okhttp:\n    enabled: true\n```  \nEnable\n```\n@SpringBootApplication\n@EnableDiscoveryClient\n@RibbonClient(name = \"say-hello\", configuration = SayHelloConfiguration.class)\npublic class Application {\n```\n\nFull code\n```\npackage hello;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.client.discovery.DiscoveryClient;\nimport org.springframework.cloud.client.loadbalancer.LoadBalanced;\nimport org.springframework.cloud.client.loadbalancer.LoadBalancerClient;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\n@RestController\npublic class HelloController {\n\n  @Autowired\n  private LoadBalancerClient loadBalancer;\n  @Autowired\n  private DiscoveryClient discoveryClient;\n\n  @LoadBalanced\n  @Bean\n  RestTemplate restTemplate() {\n    return new RestTemplate();\n  }\n\n  @Autowired\n  RestTemplate restTemplate;\n\n  /**\n   * 从所有服务中选择一个服务（轮询）\n   */\n  @RequestMapping(\"/discover\")\n  public Object discover() {\n    // return loadBalancer.choose(\"application\").getUri().toString();\n    String greeting = this.restTemplate.getForObject(\"http://application\", String.class);\n    return String.format(\"%s!\", greeting);\n  }\n\n  @RequestMapping(\"/d\")\n  public Object d() {\n    return loadBalancer.choose(\"application\").getUri().toString();\n\n  }\n\n  /**\n   * 获取所有服务\n   */\n  @RequestMapping(\"/services\")\n  public Object services() {\n    return discoveryClient.getInstances(\"application\").stream().findFirst().get().getUri();\n  }\n\n}\n\n```\n","slug":"consul","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bfs0006i272xnmkmyks","content":"<h3 id=\"Consul-服务注册管理\"><a href=\"#Consul-服务注册管理\" class=\"headerlink\" title=\"Consul 服务注册管理\"></a>Consul 服务注册管理</h3><h4 id=\"启动Consul-Docker\"><a href=\"#启动Consul-Docker\" class=\"headerlink\" title=\"启动Consul Docker\"></a>启动Consul Docker</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run -d -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e &apos;CONSUL_LOCAL_CONFIG=&#123;&quot;acl_datacenter&quot;:&quot;dc1&quot;,&quot;acl_default_policy”:&quot;write&quot;,&quot;acl_down_policy&quot;:&quot;extend-cache&quot;,&quot;acl_master_token&quot;:&quot;the_one_ring&quot;,&quot;bootstrap_expect&quot;:1,&quot;datacenter&quot;:&quot;dc1&quot;,&quot;data_dir&quot;:&quot;/usr/local/bin/consul.d/data&quot;,&quot;server&quot;:true&#125;&apos; consul agent -server -bind=127.0.0.1 -client=0.0.0.0</div></pre></td></tr></table></figure>\n<p>or</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run -d --name myconsul -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e &apos;CONSUL_LOCAL_CONFIG=&#123;&quot;acl_datacenter&quot;:&quot;dc1&quot;,&quot;acl_default_policy&quot;:&quot;allow&quot;,&quot;acl_down_policy&quot;:&quot;extend-cache&quot;,&quot;acl_master_token&quot;:&quot;the_one_ring&quot;,&quot;bootstrap_expect&quot;:1,&quot;datacenter&quot;:&quot;dc1&quot;,&quot;data_dir&quot;:&quot;/usr/local/bin/consul.d/data&quot;,&quot;server&quot;:true&#125;&apos; consul agent -server -bind=127.0.0.1 -client=0.0.0.0 -ui</div></pre></td></tr></table></figure>\n<h4 id=\"1-注册服务\"><a href=\"#1-注册服务\" class=\"headerlink\" title=\"1. 注册服务:\"></a>1. 注册服务:</h4><p><a href=\"http://{IP}:8500/v1/agent/service/register\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/service/register</a></p>\n<p>PUT<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    &quot;id&quot;: &quot;Analytics_Job_Free_JobID0001&quot;,</div><div class=\"line\">    &quot;name&quot;: &quot;Analytics_Job_Free_JobID0001&quot;,</div><div class=\"line\">    &quot;tags&quot;: [</div><div class=\"line\">        &quot;RTI&quot;</div><div class=\"line\">    ]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"2-注销服务：\"><a href=\"#2-注销服务：\" class=\"headerlink\" title=\"2. 注销服务：\"></a>2. 注销服务：</h4><p><a href=\"http://{IP}:8500/v1/agent/service/deregister/jetty\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/service/deregister/jetty</a></p>\n<h4 id=\"3-注册check\"><a href=\"#3-注册check\" class=\"headerlink\" title=\"3. 注册check\"></a>3. 注册check</h4><p><a href=\"http://{IP}:8500/v1/agent/check/register\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/check/register</a></p>\n<p>PUT<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;  </div><div class=\"line\">            &quot;http&quot;: &quot;http://192.168.1.200:8080/health?appId=app001&quot;,  </div><div class=\"line\">            &quot;interval&quot;: “20s&quot;,</div><div class=\"line\">            &quot;id&quot;: &quot;app001&quot;,</div><div class=\"line\">              &quot;name&quot;: &quot;Analytics YARN Application 001&quot;,</div><div class=\"line\">            &quot;applicationId&quot;:&quot;app001&quot;,</div><div class=\"line\">            &quot;service_id&quot;:&quot;Analytics_Job_Free_JobID0001&quot;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">&#123;  </div><div class=\"line\">            &quot;http&quot;: &quot;http://192.168.1.200:8080/health?appId=app002&quot;,  </div><div class=\"line\">            &quot;interval&quot;: “20s&quot;,</div><div class=\"line\">            &quot;id&quot;: &quot;app002&quot;,</div><div class=\"line\">              &quot;name&quot;: &quot;Analytics YARN Application 002&quot;,</div><div class=\"line\">            &quot;applicationId&quot;:&quot;app002”,</div><div class=\"line\">        &quot;status&quot;: &quot;passing&quot;,</div><div class=\"line\">            &quot;service_id&quot;:&quot;Analytics_Job_Free_JobID0001&quot;</div><div class=\"line\">        &#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"4-注销check\"><a href=\"#4-注销check\" class=\"headerlink\" title=\"4. 注销check\"></a>4. 注销check</h4><p><a href=\"http://{IP}:8500/v1/agent/check/deregister/app002\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/check/deregister/app002</a></p>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency</div></pre></td></tr></table></figure>\n<p>application.properties<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">server.port=9955  </div><div class=\"line\"></div><div class=\"line\">spring.application.name=SampleClient</div><div class=\"line\">spring.cloud.consul.host=127.0.0.1</div><div class=\"line\">spring.cloud.consul.port=8500</div><div class=\"line\">spring.cloud.consul.enabled=true</div><div class=\"line\">spring.cloud.consul.discovery.register=false</div><div class=\"line\">spring.cloud.consul.discovery.enabled=true</div><div class=\"line\">spring.cloud.consul.discovery.instanceId=tomcat1</div><div class=\"line\">spring.cloud.consul.discovery.serviceName=tomcat1  </div><div class=\"line\">spring.cloud.consul.discovery.hostname=127.0.0.1</div><div class=\"line\">spring.cloud.consul.discovery.port=$&#123;server.port&#125;</div><div class=\"line\">spring.cloud.consul.discovery.healthCheckUrl=http://127.0.0.1:9955/health  </div><div class=\"line\">spring.cloud.consul.discovery.healthCheckInterval=10s  </div><div class=\"line\">spring.cloud.consul.discovery.tags=dev</div></pre></td></tr></table></figure></p>\n<p>application.yml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">say-hello:</div><div class=\"line\"> ribbon:</div><div class=\"line\">  okhttp:</div><div class=\"line\">    enabled: true</div><div class=\"line\">```  </div><div class=\"line\">Enable</div></pre></td></tr></table></figure></p>\n<p>@SpringBootApplication<br>@EnableDiscoveryClient<br>@RibbonClient(name = “say-hello”, configuration = SayHelloConfiguration.class)<br>public class Application {<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">Full code</div></pre></td></tr></table></figure></p>\n<p>package hello;</p>\n<p>import org.springframework.beans.factory.annotation.Autowired;<br>import org.springframework.cloud.client.discovery.DiscoveryClient;<br>import org.springframework.cloud.client.loadbalancer.LoadBalanced;<br>import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;<br>import org.springframework.context.annotation.Bean;<br>import org.springframework.web.bind.annotation.RequestMapping;<br>import org.springframework.web.bind.annotation.RestController;<br>import org.springframework.web.client.RestTemplate;</p>\n<p>@RestController<br>public class HelloController {</p>\n<p>  @Autowired<br>  private LoadBalancerClient loadBalancer;<br>  @Autowired<br>  private DiscoveryClient discoveryClient;</p>\n<p>  @LoadBalanced<br>  @Bean<br>  RestTemplate restTemplate() {<br>    return new RestTemplate();<br>  }</p>\n<p>  @Autowired<br>  RestTemplate restTemplate;</p>\n<p>  /**</p>\n<ul>\n<li><p>从所有服务中选择一个服务（轮询）<br>*/<br>@RequestMapping(“/discover”)<br>public Object discover() {<br>// return loadBalancer.choose(“application”).getUri().toString();<br>String greeting = this.restTemplate.getForObject(“<a href=\"http://application\" target=\"_blank\" rel=\"external\">http://application</a>“, String.class);<br>return String.format(“%s!”, greeting);<br>}</p>\n<p>@RequestMapping(“/d”)<br>public Object d() {<br>return loadBalancer.choose(“application”).getUri().toString();</p>\n<p>}</p>\n<p>/**</p>\n</li>\n<li>获取所有服务<br>*/<br>@RequestMapping(“/services”)<br>public Object services() {<br>return discoveryClient.getInstances(“application”).stream().findFirst().get().getUri();<br>}</li>\n</ul>\n<p>}</p>\n<p>```</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Consul-服务注册管理\"><a href=\"#Consul-服务注册管理\" class=\"headerlink\" title=\"Consul 服务注册管理\"></a>Consul 服务注册管理</h3><h4 id=\"启动Consul-Docker\"><a href=\"#启动Consul-Docker\" class=\"headerlink\" title=\"启动Consul Docker\"></a>启动Consul Docker</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run -d -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e &apos;CONSUL_LOCAL_CONFIG=&#123;&quot;acl_datacenter&quot;:&quot;dc1&quot;,&quot;acl_default_policy”:&quot;write&quot;,&quot;acl_down_policy&quot;:&quot;extend-cache&quot;,&quot;acl_master_token&quot;:&quot;the_one_ring&quot;,&quot;bootstrap_expect&quot;:1,&quot;datacenter&quot;:&quot;dc1&quot;,&quot;data_dir&quot;:&quot;/usr/local/bin/consul.d/data&quot;,&quot;server&quot;:true&#125;&apos; consul agent -server -bind=127.0.0.1 -client=0.0.0.0</div></pre></td></tr></table></figure>\n<p>or</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run -d --name myconsul -p 8400:8400 -p 8500:8500/tcp -p 8600:53/udp -e &apos;CONSUL_LOCAL_CONFIG=&#123;&quot;acl_datacenter&quot;:&quot;dc1&quot;,&quot;acl_default_policy&quot;:&quot;allow&quot;,&quot;acl_down_policy&quot;:&quot;extend-cache&quot;,&quot;acl_master_token&quot;:&quot;the_one_ring&quot;,&quot;bootstrap_expect&quot;:1,&quot;datacenter&quot;:&quot;dc1&quot;,&quot;data_dir&quot;:&quot;/usr/local/bin/consul.d/data&quot;,&quot;server&quot;:true&#125;&apos; consul agent -server -bind=127.0.0.1 -client=0.0.0.0 -ui</div></pre></td></tr></table></figure>\n<h4 id=\"1-注册服务\"><a href=\"#1-注册服务\" class=\"headerlink\" title=\"1. 注册服务:\"></a>1. 注册服务:</h4><p><a href=\"http://{IP}:8500/v1/agent/service/register\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/service/register</a></p>\n<p>PUT<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;</div><div class=\"line\">    &quot;id&quot;: &quot;Analytics_Job_Free_JobID0001&quot;,</div><div class=\"line\">    &quot;name&quot;: &quot;Analytics_Job_Free_JobID0001&quot;,</div><div class=\"line\">    &quot;tags&quot;: [</div><div class=\"line\">        &quot;RTI&quot;</div><div class=\"line\">    ]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"2-注销服务：\"><a href=\"#2-注销服务：\" class=\"headerlink\" title=\"2. 注销服务：\"></a>2. 注销服务：</h4><p><a href=\"http://{IP}:8500/v1/agent/service/deregister/jetty\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/service/deregister/jetty</a></p>\n<h4 id=\"3-注册check\"><a href=\"#3-注册check\" class=\"headerlink\" title=\"3. 注册check\"></a>3. 注册check</h4><p><a href=\"http://{IP}:8500/v1/agent/check/register\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/check/register</a></p>\n<p>PUT<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;  </div><div class=\"line\">            &quot;http&quot;: &quot;http://192.168.1.200:8080/health?appId=app001&quot;,  </div><div class=\"line\">            &quot;interval&quot;: “20s&quot;,</div><div class=\"line\">            &quot;id&quot;: &quot;app001&quot;,</div><div class=\"line\">              &quot;name&quot;: &quot;Analytics YARN Application 001&quot;,</div><div class=\"line\">            &quot;applicationId&quot;:&quot;app001&quot;,</div><div class=\"line\">            &quot;service_id&quot;:&quot;Analytics_Job_Free_JobID0001&quot;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">&#123;  </div><div class=\"line\">            &quot;http&quot;: &quot;http://192.168.1.200:8080/health?appId=app002&quot;,  </div><div class=\"line\">            &quot;interval&quot;: “20s&quot;,</div><div class=\"line\">            &quot;id&quot;: &quot;app002&quot;,</div><div class=\"line\">              &quot;name&quot;: &quot;Analytics YARN Application 002&quot;,</div><div class=\"line\">            &quot;applicationId&quot;:&quot;app002”,</div><div class=\"line\">        &quot;status&quot;: &quot;passing&quot;,</div><div class=\"line\">            &quot;service_id&quot;:&quot;Analytics_Job_Free_JobID0001&quot;</div><div class=\"line\">        &#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"4-注销check\"><a href=\"#4-注销check\" class=\"headerlink\" title=\"4. 注销check\"></a>4. 注销check</h4><p><a href=\"http://{IP}:8500/v1/agent/check/deregister/app002\" target=\"_blank\" rel=\"external\">http://{IP}:8500/v1/agent/check/deregister/app002</a></p>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency</div></pre></td></tr></table></figure>\n<p>application.properties<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">server.port=9955  </div><div class=\"line\"></div><div class=\"line\">spring.application.name=SampleClient</div><div class=\"line\">spring.cloud.consul.host=127.0.0.1</div><div class=\"line\">spring.cloud.consul.port=8500</div><div class=\"line\">spring.cloud.consul.enabled=true</div><div class=\"line\">spring.cloud.consul.discovery.register=false</div><div class=\"line\">spring.cloud.consul.discovery.enabled=true</div><div class=\"line\">spring.cloud.consul.discovery.instanceId=tomcat1</div><div class=\"line\">spring.cloud.consul.discovery.serviceName=tomcat1  </div><div class=\"line\">spring.cloud.consul.discovery.hostname=127.0.0.1</div><div class=\"line\">spring.cloud.consul.discovery.port=$&#123;server.port&#125;</div><div class=\"line\">spring.cloud.consul.discovery.healthCheckUrl=http://127.0.0.1:9955/health  </div><div class=\"line\">spring.cloud.consul.discovery.healthCheckInterval=10s  </div><div class=\"line\">spring.cloud.consul.discovery.tags=dev</div></pre></td></tr></table></figure></p>\n<p>application.yml<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">say-hello:</div><div class=\"line\"> ribbon:</div><div class=\"line\">  okhttp:</div><div class=\"line\">    enabled: true</div><div class=\"line\">```  </div><div class=\"line\">Enable</div></pre></td></tr></table></figure></p>\n<p>@SpringBootApplication<br>@EnableDiscoveryClient<br>@RibbonClient(name = “say-hello”, configuration = SayHelloConfiguration.class)<br>public class Application {<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">Full code</div></pre></td></tr></table></figure></p>\n<p>package hello;</p>\n<p>import org.springframework.beans.factory.annotation.Autowired;<br>import org.springframework.cloud.client.discovery.DiscoveryClient;<br>import org.springframework.cloud.client.loadbalancer.LoadBalanced;<br>import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;<br>import org.springframework.context.annotation.Bean;<br>import org.springframework.web.bind.annotation.RequestMapping;<br>import org.springframework.web.bind.annotation.RestController;<br>import org.springframework.web.client.RestTemplate;</p>\n<p>@RestController<br>public class HelloController {</p>\n<p>  @Autowired<br>  private LoadBalancerClient loadBalancer;<br>  @Autowired<br>  private DiscoveryClient discoveryClient;</p>\n<p>  @LoadBalanced<br>  @Bean<br>  RestTemplate restTemplate() {<br>    return new RestTemplate();<br>  }</p>\n<p>  @Autowired<br>  RestTemplate restTemplate;</p>\n<p>  /**</p>\n<ul>\n<li><p>从所有服务中选择一个服务（轮询）<br>*/<br>@RequestMapping(“/discover”)<br>public Object discover() {<br>// return loadBalancer.choose(“application”).getUri().toString();<br>String greeting = this.restTemplate.getForObject(“<a href=\"http://application\" target=\"_blank\" rel=\"external\">http://application</a>“, String.class);<br>return String.format(“%s!”, greeting);<br>}</p>\n<p>@RequestMapping(“/d”)<br>public Object d() {<br>return loadBalancer.choose(“application”).getUri().toString();</p>\n<p>}</p>\n<p>/**</p>\n</li>\n<li>获取所有服务<br>*/<br>@RequestMapping(“/services”)<br>public Object services() {<br>return discoveryClient.getInstances(“application”).stream().findFirst().get().getUri();<br>}</li>\n</ul>\n<p>}</p>\n<p>```</p>\n"},{"title":"Gateway","date":"2017-02-08T12:46:25.000Z","_content":"\n![zuul](https://camo.githubusercontent.com/4eb7754152028cdebd5c09d1c6f5acc7683f0094/687474703a2f2f6e6574666c69782e6769746875622e696f2f7a75756c2f696d616765732f7a75756c2d726571756573742d6c6966656379636c652e706e67)\n","source":"_posts/gateway.md","raw":"---\ntitle: Gateway\ndate: 2017-2-8 20:46:25\n---\n\n![zuul](https://camo.githubusercontent.com/4eb7754152028cdebd5c09d1c6f5acc7683f0094/687474703a2f2f6e6574666c69782e6769746875622e696f2f7a75756c2f696d616765732f7a75756c2d726571756573742d6c6966656379636c652e706e67)\n","slug":"gateway","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bfu0008i272h65g1q82","content":"<p><img src=\"https://camo.githubusercontent.com/4eb7754152028cdebd5c09d1c6f5acc7683f0094/687474703a2f2f6e6574666c69782e6769746875622e696f2f7a75756c2f696d616765732f7a75756c2d726571756573742d6c6966656379636c652e706e67\" alt=\"zuul\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"https://camo.githubusercontent.com/4eb7754152028cdebd5c09d1c6f5acc7683f0094/687474703a2f2f6e6574666c69782e6769746875622e696f2f7a75756c2f696d616765732f7a75756c2d726571756573742d6c6966656379636c652e706e67\" alt=\"zuul\"></p>\n"},{"title":"HDFS","date":"2017-02-09T12:46:25.000Z","_content":"\n## 简介\nHDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。\n\nHDFS有很多特点：\n-    ① 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。\n-    ② 运行在廉价的机器上。\n-    ③ 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。\n\n![hdfs1](images/hdfs1.jpg)\n如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。\n\n- NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；\n- SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。\n- DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。\n- 热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。\n- 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。\n- fsimage:元数据镜像文件（文件系统的目录树。）\n- edits：元数据的操作日志（针对文件系统做的修改操作记录）\n\nNamenode内存中存储的是=fsimage+edits。\n\nSecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。\n\n## HDFS写操作\n有一个文件FileA，100M大小。Client将FileA写入到HDFS上。HDFS按默认配置。HDFS分布在三个机架上Rack1，Rack2，Rack3。\n![hdfs-write](images/hdfs-write.jpg)\n- a. Client将FileA按64M分块。分成两块，block1和Block2;\n- b. Client向nameNode发送写数据请求，如图蓝色虚线①------>。\n- c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。\n\n    Block1: host2,host1,host3\n\n    Block2: host7,host8,host4\n\n    原理：\n\n        NameNode具有RackAware机架感知功能，这个可以配置。\n        若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。\n        若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。\n\n- d. client向DataNode发送block1；发送过程是以流式写入。\n\n    流式写入过程，\n\n        1>将64M的block1按64k的package划分;\n\n        2>然后将第一个package发送给host2;\n\n        3>host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；\n\n        4>host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。\n\n        5>以此类推，如图红线实线所示，直到将block1发送完毕。\n\n        6>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。\n\n        7>client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线\n\n        8>发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。\n\n        9>发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。\n\n        10>client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。\n\n分析，通过写过程，我们可以了解到：\n\n    ①写1T文件，我们需要3T的存储，3T的网络流量贷款。\n\n    ②在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。\n\n    ③挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。\n\n## HDFS读操作\n![hdfs-read](images/hdfs-read.jpg)\n\n读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。\n\n\n\n那么，读操作流程为：\n\na. client向namenode发送读请求。\n\nb. namenode查看Metadata信息，返回fileA的block的位置。\n\n    block1:host2,host1,host3\n\n    block2:host7,host8,host4\n\nc. block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；\n\n\n\n上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：\n\n优选读取本机架上的数据。\n\n\n## HDFS中常用到的命令\n1、hadoop fs\n```\nhadoop fs -ls /\nhadoop fs -lsr\nhadoop fs -mkdir /user/hadoop\nhadoop fs -put a.txt /user/hadoop/\nhadoop fs -get /user/hadoop/a.txt /\nhadoop fs -cp src dst\nhadoop fs -mv src dst\nhadoop fs -cat /user/hadoop/a.txt\nhadoop fs -rm /user/hadoop/a.txt\nhadoop fs -rmr /user/hadoop/a.txt\nhadoop fs -text /user/hadoop/a.txt\nhadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。\nhadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。\n```\n2、hadoop fsadmin\n```\nhadoop dfsadmin -report\nhadoop dfsadmin -safemode enter | leave | get | wait\nhadoop dfsadmin -setBalancerBandwidth 1000\n```\n\n3、hadoop fsck\n\n4、start-balancer.sh\n","source":"_posts/hdfs.md","raw":"---\ntitle: HDFS\ndate: 2017-2-9 20:46:25\n---\n\n## 简介\nHDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。\n\nHDFS有很多特点：\n-    ① 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。\n-    ② 运行在廉价的机器上。\n-    ③ 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。\n\n![hdfs1](images/hdfs1.jpg)\n如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。\n\n- NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；\n- SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。\n- DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。\n- 热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。\n- 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。\n- fsimage:元数据镜像文件（文件系统的目录树。）\n- edits：元数据的操作日志（针对文件系统做的修改操作记录）\n\nNamenode内存中存储的是=fsimage+edits。\n\nSecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。\n\n## HDFS写操作\n有一个文件FileA，100M大小。Client将FileA写入到HDFS上。HDFS按默认配置。HDFS分布在三个机架上Rack1，Rack2，Rack3。\n![hdfs-write](images/hdfs-write.jpg)\n- a. Client将FileA按64M分块。分成两块，block1和Block2;\n- b. Client向nameNode发送写数据请求，如图蓝色虚线①------>。\n- c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。\n\n    Block1: host2,host1,host3\n\n    Block2: host7,host8,host4\n\n    原理：\n\n        NameNode具有RackAware机架感知功能，这个可以配置。\n        若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。\n        若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。\n\n- d. client向DataNode发送block1；发送过程是以流式写入。\n\n    流式写入过程，\n\n        1>将64M的block1按64k的package划分;\n\n        2>然后将第一个package发送给host2;\n\n        3>host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；\n\n        4>host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。\n\n        5>以此类推，如图红线实线所示，直到将block1发送完毕。\n\n        6>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。\n\n        7>client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线\n\n        8>发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。\n\n        9>发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。\n\n        10>client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。\n\n分析，通过写过程，我们可以了解到：\n\n    ①写1T文件，我们需要3T的存储，3T的网络流量贷款。\n\n    ②在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。\n\n    ③挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。\n\n## HDFS读操作\n![hdfs-read](images/hdfs-read.jpg)\n\n读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。\n\n\n\n那么，读操作流程为：\n\na. client向namenode发送读请求。\n\nb. namenode查看Metadata信息，返回fileA的block的位置。\n\n    block1:host2,host1,host3\n\n    block2:host7,host8,host4\n\nc. block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；\n\n\n\n上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：\n\n优选读取本机架上的数据。\n\n\n## HDFS中常用到的命令\n1、hadoop fs\n```\nhadoop fs -ls /\nhadoop fs -lsr\nhadoop fs -mkdir /user/hadoop\nhadoop fs -put a.txt /user/hadoop/\nhadoop fs -get /user/hadoop/a.txt /\nhadoop fs -cp src dst\nhadoop fs -mv src dst\nhadoop fs -cat /user/hadoop/a.txt\nhadoop fs -rm /user/hadoop/a.txt\nhadoop fs -rmr /user/hadoop/a.txt\nhadoop fs -text /user/hadoop/a.txt\nhadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。\nhadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。\n```\n2、hadoop fsadmin\n```\nhadoop dfsadmin -report\nhadoop dfsadmin -safemode enter | leave | get | wait\nhadoop dfsadmin -setBalancerBandwidth 1000\n```\n\n3、hadoop fsck\n\n4、start-balancer.sh\n","slug":"hdfs","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bfx0009i272x7481mhb","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。</p>\n<p>HDFS有很多特点：</p>\n<ul>\n<li>① 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。</li>\n<li>② 运行在廉价的机器上。</li>\n<li>③ 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</li>\n</ul>\n<p><img src=\"images/hdfs1.jpg\" alt=\"hdfs1\"><br>如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。</p>\n<ul>\n<li>NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；</li>\n<li>SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。</li>\n<li>DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。</li>\n<li>热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。</li>\n<li>冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。</li>\n<li>fsimage:元数据镜像文件（文件系统的目录树。）</li>\n<li>edits：元数据的操作日志（针对文件系统做的修改操作记录）</li>\n</ul>\n<p>Namenode内存中存储的是=fsimage+edits。</p>\n<p>SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。</p>\n<h2 id=\"HDFS写操作\"><a href=\"#HDFS写操作\" class=\"headerlink\" title=\"HDFS写操作\"></a>HDFS写操作</h2><p>有一个文件FileA，100M大小。Client将FileA写入到HDFS上。HDFS按默认配置。HDFS分布在三个机架上Rack1，Rack2，Rack3。<br><img src=\"images/hdfs-write.jpg\" alt=\"hdfs-write\"></p>\n<ul>\n<li>a. Client将FileA按64M分块。分成两块，block1和Block2;</li>\n<li>b. Client向nameNode发送写数据请求，如图蓝色虚线①——&gt;。</li>\n<li><p>c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。</p>\n<p>  Block1: host2,host1,host3</p>\n<p>  Block2: host7,host8,host4</p>\n<p>  原理：</p>\n<pre><code>NameNode具有RackAware机架感知功能，这个可以配置。\n若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。\n若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。\n</code></pre></li>\n<li><p>d. client向DataNode发送block1；发送过程是以流式写入。</p>\n<p>  流式写入过程，</p>\n<pre><code>1&gt;将64M的block1按64k的package划分;\n\n2&gt;然后将第一个package发送给host2;\n\n3&gt;host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；\n\n4&gt;host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。\n\n5&gt;以此类推，如图红线实线所示，直到将block1发送完毕。\n\n6&gt;host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。\n\n7&gt;client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线\n\n8&gt;发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。\n\n9&gt;发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。\n\n10&gt;client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。\n</code></pre></li>\n</ul>\n<p>分析，通过写过程，我们可以了解到：</p>\n<pre><code>①写1T文件，我们需要3T的存储，3T的网络流量贷款。\n\n②在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。\n\n③挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。\n</code></pre><h2 id=\"HDFS读操作\"><a href=\"#HDFS读操作\" class=\"headerlink\" title=\"HDFS读操作\"></a>HDFS读操作</h2><p><img src=\"images/hdfs-read.jpg\" alt=\"hdfs-read\"></p>\n<p>读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。</p>\n<p>那么，读操作流程为：</p>\n<p>a. client向namenode发送读请求。</p>\n<p>b. namenode查看Metadata信息，返回fileA的block的位置。</p>\n<pre><code>block1:host2,host1,host3\n\nblock2:host7,host8,host4\n</code></pre><p>c. block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；</p>\n<p>上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：</p>\n<p>优选读取本机架上的数据。</p>\n<h2 id=\"HDFS中常用到的命令\"><a href=\"#HDFS中常用到的命令\" class=\"headerlink\" title=\"HDFS中常用到的命令\"></a>HDFS中常用到的命令</h2><p>1、hadoop fs<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -ls /</div><div class=\"line\">hadoop fs -lsr</div><div class=\"line\">hadoop fs -mkdir /user/hadoop</div><div class=\"line\">hadoop fs -put a.txt /user/hadoop/</div><div class=\"line\">hadoop fs -get /user/hadoop/a.txt /</div><div class=\"line\">hadoop fs -cp src dst</div><div class=\"line\">hadoop fs -mv src dst</div><div class=\"line\">hadoop fs -cat /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -rm /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -rmr /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -text /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。</div><div class=\"line\">hadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。</div></pre></td></tr></table></figure></p>\n<p>2、hadoop fsadmin<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop dfsadmin -report</div><div class=\"line\">hadoop dfsadmin -safemode enter | leave | get | wait</div><div class=\"line\">hadoop dfsadmin -setBalancerBandwidth 1000</div></pre></td></tr></table></figure></p>\n<p>3、hadoop fsck</p>\n<p>4、start-balancer.sh</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。</p>\n<p>HDFS有很多特点：</p>\n<ul>\n<li>① 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。</li>\n<li>② 运行在廉价的机器上。</li>\n<li>③ 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</li>\n</ul>\n<p><img src=\"images/hdfs1.jpg\" alt=\"hdfs1\"><br>如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。</p>\n<ul>\n<li>NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；</li>\n<li>SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。</li>\n<li>DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。</li>\n<li>热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。</li>\n<li>冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。</li>\n<li>fsimage:元数据镜像文件（文件系统的目录树。）</li>\n<li>edits：元数据的操作日志（针对文件系统做的修改操作记录）</li>\n</ul>\n<p>Namenode内存中存储的是=fsimage+edits。</p>\n<p>SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。</p>\n<h2 id=\"HDFS写操作\"><a href=\"#HDFS写操作\" class=\"headerlink\" title=\"HDFS写操作\"></a>HDFS写操作</h2><p>有一个文件FileA，100M大小。Client将FileA写入到HDFS上。HDFS按默认配置。HDFS分布在三个机架上Rack1，Rack2，Rack3。<br><img src=\"images/hdfs-write.jpg\" alt=\"hdfs-write\"></p>\n<ul>\n<li>a. Client将FileA按64M分块。分成两块，block1和Block2;</li>\n<li>b. Client向nameNode发送写数据请求，如图蓝色虚线①——&gt;。</li>\n<li><p>c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。</p>\n<p>  Block1: host2,host1,host3</p>\n<p>  Block2: host7,host8,host4</p>\n<p>  原理：</p>\n<pre><code>NameNode具有RackAware机架感知功能，这个可以配置。\n若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。\n若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。\n</code></pre></li>\n<li><p>d. client向DataNode发送block1；发送过程是以流式写入。</p>\n<p>  流式写入过程，</p>\n<pre><code>1&gt;将64M的block1按64k的package划分;\n\n2&gt;然后将第一个package发送给host2;\n\n3&gt;host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；\n\n4&gt;host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。\n\n5&gt;以此类推，如图红线实线所示，直到将block1发送完毕。\n\n6&gt;host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。\n\n7&gt;client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线\n\n8&gt;发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。\n\n9&gt;发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。\n\n10&gt;client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。\n</code></pre></li>\n</ul>\n<p>分析，通过写过程，我们可以了解到：</p>\n<pre><code>①写1T文件，我们需要3T的存储，3T的网络流量贷款。\n\n②在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。\n\n③挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。\n</code></pre><h2 id=\"HDFS读操作\"><a href=\"#HDFS读操作\" class=\"headerlink\" title=\"HDFS读操作\"></a>HDFS读操作</h2><p><img src=\"images/hdfs-read.jpg\" alt=\"hdfs-read\"></p>\n<p>读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。</p>\n<p>那么，读操作流程为：</p>\n<p>a. client向namenode发送读请求。</p>\n<p>b. namenode查看Metadata信息，返回fileA的block的位置。</p>\n<pre><code>block1:host2,host1,host3\n\nblock2:host7,host8,host4\n</code></pre><p>c. block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取；</p>\n<p>上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：</p>\n<p>优选读取本机架上的数据。</p>\n<h2 id=\"HDFS中常用到的命令\"><a href=\"#HDFS中常用到的命令\" class=\"headerlink\" title=\"HDFS中常用到的命令\"></a>HDFS中常用到的命令</h2><p>1、hadoop fs<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -ls /</div><div class=\"line\">hadoop fs -lsr</div><div class=\"line\">hadoop fs -mkdir /user/hadoop</div><div class=\"line\">hadoop fs -put a.txt /user/hadoop/</div><div class=\"line\">hadoop fs -get /user/hadoop/a.txt /</div><div class=\"line\">hadoop fs -cp src dst</div><div class=\"line\">hadoop fs -mv src dst</div><div class=\"line\">hadoop fs -cat /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -rm /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -rmr /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -text /user/hadoop/a.txt</div><div class=\"line\">hadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。</div><div class=\"line\">hadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。</div></pre></td></tr></table></figure></p>\n<p>2、hadoop fsadmin<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop dfsadmin -report</div><div class=\"line\">hadoop dfsadmin -safemode enter | leave | get | wait</div><div class=\"line\">hadoop dfsadmin -setBalancerBandwidth 1000</div></pre></td></tr></table></figure></p>\n<p>3、hadoop fsck</p>\n<p>4、start-balancer.sh</p>\n"},{"title":"Kafka","date":"2016-05-09T12:46:25.000Z","_content":"\n![](http://kafka.apache.org/images/logo.png)\n\n### kafka\n\n1. Kafka Connect\n    - Overview (http://kafka.apache.org/documentation/#connect)\n    - User Guide\n    - Connector Development Guide\n\n2. Kafka Streams\n\n    - Overview\n    - Core Concepts\n    - Architecture\n    ![](http://kafka.apache.org/0102/images/streams-architecture-overview.jpg)\n    - Developer Guide\n        - Low-Level Processor API\n        - High-Level Streams DSL\n        - Application Configuration and Execution\n    - Upgrade Guide and API Changes\n","source":"_posts/kafka.md","raw":"---\ntitle: Kafka\ndate: 2016-5-9 20:46:25\n---\n\n![](http://kafka.apache.org/images/logo.png)\n\n### kafka\n\n1. Kafka Connect\n    - Overview (http://kafka.apache.org/documentation/#connect)\n    - User Guide\n    - Connector Development Guide\n\n2. Kafka Streams\n\n    - Overview\n    - Core Concepts\n    - Architecture\n    ![](http://kafka.apache.org/0102/images/streams-architecture-overview.jpg)\n    - Developer Guide\n        - Low-Level Processor API\n        - High-Level Streams DSL\n        - Application Configuration and Execution\n    - Upgrade Guide and API Changes\n","slug":"kafka","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bfy000bi27203e6op3f","content":"<p><img src=\"http://kafka.apache.org/images/logo.png\" alt=\"\"></p>\n<h3 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h3><ol>\n<li><p>Kafka Connect</p>\n<ul>\n<li>Overview (<a href=\"http://kafka.apache.org/documentation/#connect\" target=\"_blank\" rel=\"external\">http://kafka.apache.org/documentation/#connect</a>)</li>\n<li>User Guide</li>\n<li>Connector Development Guide</li>\n</ul>\n</li>\n<li><p>Kafka Streams</p>\n<ul>\n<li>Overview</li>\n<li>Core Concepts</li>\n<li>Architecture<br><img src=\"http://kafka.apache.org/0102/images/streams-architecture-overview.jpg\" alt=\"\"></li>\n<li>Developer Guide<ul>\n<li>Low-Level Processor API</li>\n<li>High-Level Streams DSL</li>\n<li>Application Configuration and Execution</li>\n</ul>\n</li>\n<li>Upgrade Guide and API Changes</li>\n</ul>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"http://kafka.apache.org/images/logo.png\" alt=\"\"></p>\n<h3 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h3><ol>\n<li><p>Kafka Connect</p>\n<ul>\n<li>Overview (<a href=\"http://kafka.apache.org/documentation/#connect\" target=\"_blank\" rel=\"external\">http://kafka.apache.org/documentation/#connect</a>)</li>\n<li>User Guide</li>\n<li>Connector Development Guide</li>\n</ul>\n</li>\n<li><p>Kafka Streams</p>\n<ul>\n<li>Overview</li>\n<li>Core Concepts</li>\n<li>Architecture<br><img src=\"http://kafka.apache.org/0102/images/streams-architecture-overview.jpg\" alt=\"\"></li>\n<li>Developer Guide<ul>\n<li>Low-Level Processor API</li>\n<li>High-Level Streams DSL</li>\n<li>Application Configuration and Execution</li>\n</ul>\n</li>\n<li>Upgrade Guide and API Changes</li>\n</ul>\n</li>\n</ol>\n"},{"title":"容器集群、网络连接、自动化部署","date":"2017-05-12T12:46:25.000Z","_content":"\n## 容器与编排\n毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。\n\n## 用Kubernetes来进行负载均衡\nService在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。\n\n将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。\n\n参见--> [Kubernetes Ingress](/kubernetes-ingress/)\n\n![ingress-nginx](/images/ingress-nginx.png)\n","source":"_posts/kubernetes-deployment.md","raw":"---\ntitle: 容器集群、网络连接、自动化部署\ndate: 2017-5-12 20:46:25\ncategories:\n  - 分布式&云计算\n  - Kubernetes\ntags:\n  - 分布式\n  - Kubernetes\n  - container\n  - 容器\n  - PaaS\n---\n\n## 容器与编排\n毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。\n\n## 用Kubernetes来进行负载均衡\nService在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。\n\n将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。\n\n参见--> [Kubernetes Ingress](/kubernetes-ingress/)\n\n![ingress-nginx](/images/ingress-nginx.png)\n","slug":"kubernetes-deployment","published":1,"updated":"2017-05-10T07:07:10.000Z","_id":"cj2in1bfz000di272exv0bhy5","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"容器与编排\"><a href=\"#容器与编排\" class=\"headerlink\" title=\"容器与编排\"></a>容器与编排</h2><p>毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。</p>\n<h2 id=\"用Kubernetes来进行负载均衡\"><a href=\"#用Kubernetes来进行负载均衡\" class=\"headerlink\" title=\"用Kubernetes来进行负载均衡\"></a>用Kubernetes来进行负载均衡</h2><p>Service在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。</p>\n<p>将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。</p>\n<p>参见–&gt; <a href=\"/kubernetes-ingress/\">Kubernetes Ingress</a></p>\n<p><img src=\"/images/ingress-nginx.png\" alt=\"ingress-nginx\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"容器与编排\"><a href=\"#容器与编排\" class=\"headerlink\" title=\"容器与编排\"></a>容器与编排</h2><p>毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。</p>\n<h2 id=\"用Kubernetes来进行负载均衡\"><a href=\"#用Kubernetes来进行负载均衡\" class=\"headerlink\" title=\"用Kubernetes来进行负载均衡\"></a>用Kubernetes来进行负载均衡</h2><p>Service在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。</p>\n<p>将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。</p>\n<p>参见–&gt; <a href=\"/kubernetes-ingress/\">Kubernetes Ingress</a></p>\n<p><img src=\"/images/ingress-nginx.png\" alt=\"ingress-nginx\"></p>\n"},{"title":"HBase","date":"2017-02-09T12:46:25.000Z","_content":"\n## HBase QuickStart\nHBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。\n\nWith the completed config, issue the command: bin/start-hbase.sh\n\n```\n2017-05-03 11:54:25,802 INFO  [main] http.HttpServer: Jetty bound to port 16010\n2017-05-03 11:54:25,802 INFO  [main] mortbay.log: jetty-6.1.26\n2017-05-03 11:54:25,998 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010\n```\nGo to http://localhost:16010 to view the HBase Web UI.\n\n\n## HBase连接池管理\nConnectionFactory 是一个不可实例化的类，专门用于创建HBase的Connection。最简单的创建Connection实例的方式是ConnectionFactory.createConnection(config)，该方法创建了一个连接到集群的Connection实例，该实例被创建的程序管理。通过这个Connection实例，可以使用Connection.getTable()方法取得Table，例如:\n```\n Connection connection = ConnectionFactory.createConnection(config);\n Table table = connection.getTable(TableName.valueOf(\"table1\"));\n try {\n  // Use the table as needed, for a single operation and a single thread\n } finally {\n  table.close();\n  connection.close();\n }\n ```\n **** HConnectionManager在新版本中已经不建议使用。\n\n## HBase客户端的使用\n\n [客户端的使用Sample](samplecodes/hbaseclient/HBaseSample.java)\n\n## Cassandra\nApache Cassandra是高度可扩展的，高性能的分布式NoSQL数据库。 Cassandra旨在处理许多商品服务器上的大量数据，提供高可用性而无需担心单点故障。\n\nCassandra具有能够处理大量数据的分布式架构。 数据放置在具有多个复制因子的不同机器上，以获得高可用性，而无需担心单点故障。它在其节点之间具有对等分布式系统，数据分布在集群中的所有节点上。\n- 在Cassandra中，每个节点是独立的，同时与其他节点互连。 集群中的所有节点都扮演着相同的角色。\n- 集群中的每个节点都可以接受读取和写入请求，而不管数据实际位于集群中的位置。\n- 在一个节点发生故障的情况下，可以从网络中的其他节点提供读/写请求。\n\n## Cassandra和HBase对比\n\nCassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。\n\n- Cassandra部署更简单。Cassandra只有一种角色，而HBase除了Region Server外还需要Zookeeper来同步集群状态\n- 数据一致性是否可配置。Cassandra的数据一致性是可配置的，可以更改为最终一致性，而HBase是强一致性的\n- 负载均衡算法不同。Cassandra通过一致性哈希来决定数据存储的位置，而HBase靠Master节点管理数据的分配，将过热的节点上的Region动态分配给负载较低的节点。因此Cassandra的平均性能会优于HBase，但是HBase有Master节点，热数据的负载更均衡。\n- 单点问题。正是由于HBase存在Master节点，因此会存在单点问题。\n\n<table>\n    <th>\n        <td>HBase</td>\n        <td>Cassandra</td>\n    </th>\n    <tr>\n        <td>语言/License/交互协议</td>\n        <td>Java/Apache/HTTP/REST (also Thrift)\t</td>\n        <td>Java/Apache/Custom, binary (Thrift)</td>\n    </tr>\n    <tr>\n        <td>出发点</td>\n        <td>BigTable</td>\n        <td>BigTable and Dynamo</td>\n    </tr>\n    <tr>\n        <td>架构</td>\n        <td>master/slave\t</td>\n        <td>p2p</td>\n    </tr>\n    <tr>\n        <td>高可用性</td>\n        <td>NameNode是HDFS的单点故障点\t</td>\n        <td>P2P和去中心化设计，不会出现单点故障</td>\n    </tr>\n    <tr>\n        <td>伸缩性\t</td>\n        <td>Region Server扩容，通过将自身发布到Master，Master均匀分布Region\t</td>\n        <td>扩容需在Hash Ring上多个节点间调整数据分布</td>\n    </tr>\n    <tr>\n        <td>一致性\t</td>\n        <td>强一致性\t</td>\n        <td>最终一致性，Quorum NRW策略</td>\n    </tr>\n    <tr>\n        <td>存储目标/数据分布</td>\n        <td>大文件/表划分为多个region存在不同region server上\t</td>\n        <td>小文件/改进的一致性哈希（虚拟节点）</td>\n    </tr>\n    <tr>\n        <td>成员通信及错误检测\t</td>\n        <td>Zookeeper\t</td>\n        <td>基于Gossip/P2P</td>\n    </tr>\n    <tr>\n        <td>读写性能\t</td>\n        <td>数据读写定位可能要通过最多6次的网络RPC，性能较低。\t\t</td>\n        <td>数据读写定位非常快</td>\n    </tr>\n    <tr>\n        <td>数据冲突处理\t</td>\n        <td>乐观并发控制（optimistic concurrency control）\t\t</td>\n        <td>向量时钟</td>\n    </tr>\n    <tr>\n        <td>临时故障处理\t</td>\n        <td>Region Server宕机，重做HLog\t\t</td>\n        <td>数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。</td>\n    </tr>\n    <tr>\n        <td>永久故障恢复\t\t</td>\n        <td>Region Server恢复，master重新给其分配region\t</td>\n        <td>Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性</td>\n    </tr>\n    <tr>\n        <td>CAP\t</td>\n        <td>1，强一致性，0数据丢失。2，可用性低。3，扩容方便。\t\t\t</td>\n        <td>1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。</td>\n    </tr>\n\n</table>\n","source":"_posts/hbase.md","raw":"---\ntitle: HBase\ndate: 2017-2-9 20:46:25\n---\n\n## HBase QuickStart\nHBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。\n\nWith the completed config, issue the command: bin/start-hbase.sh\n\n```\n2017-05-03 11:54:25,802 INFO  [main] http.HttpServer: Jetty bound to port 16010\n2017-05-03 11:54:25,802 INFO  [main] mortbay.log: jetty-6.1.26\n2017-05-03 11:54:25,998 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010\n```\nGo to http://localhost:16010 to view the HBase Web UI.\n\n\n## HBase连接池管理\nConnectionFactory 是一个不可实例化的类，专门用于创建HBase的Connection。最简单的创建Connection实例的方式是ConnectionFactory.createConnection(config)，该方法创建了一个连接到集群的Connection实例，该实例被创建的程序管理。通过这个Connection实例，可以使用Connection.getTable()方法取得Table，例如:\n```\n Connection connection = ConnectionFactory.createConnection(config);\n Table table = connection.getTable(TableName.valueOf(\"table1\"));\n try {\n  // Use the table as needed, for a single operation and a single thread\n } finally {\n  table.close();\n  connection.close();\n }\n ```\n **** HConnectionManager在新版本中已经不建议使用。\n\n## HBase客户端的使用\n\n [客户端的使用Sample](samplecodes/hbaseclient/HBaseSample.java)\n\n## Cassandra\nApache Cassandra是高度可扩展的，高性能的分布式NoSQL数据库。 Cassandra旨在处理许多商品服务器上的大量数据，提供高可用性而无需担心单点故障。\n\nCassandra具有能够处理大量数据的分布式架构。 数据放置在具有多个复制因子的不同机器上，以获得高可用性，而无需担心单点故障。它在其节点之间具有对等分布式系统，数据分布在集群中的所有节点上。\n- 在Cassandra中，每个节点是独立的，同时与其他节点互连。 集群中的所有节点都扮演着相同的角色。\n- 集群中的每个节点都可以接受读取和写入请求，而不管数据实际位于集群中的位置。\n- 在一个节点发生故障的情况下，可以从网络中的其他节点提供读/写请求。\n\n## Cassandra和HBase对比\n\nCassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。\n\n- Cassandra部署更简单。Cassandra只有一种角色，而HBase除了Region Server外还需要Zookeeper来同步集群状态\n- 数据一致性是否可配置。Cassandra的数据一致性是可配置的，可以更改为最终一致性，而HBase是强一致性的\n- 负载均衡算法不同。Cassandra通过一致性哈希来决定数据存储的位置，而HBase靠Master节点管理数据的分配，将过热的节点上的Region动态分配给负载较低的节点。因此Cassandra的平均性能会优于HBase，但是HBase有Master节点，热数据的负载更均衡。\n- 单点问题。正是由于HBase存在Master节点，因此会存在单点问题。\n\n<table>\n    <th>\n        <td>HBase</td>\n        <td>Cassandra</td>\n    </th>\n    <tr>\n        <td>语言/License/交互协议</td>\n        <td>Java/Apache/HTTP/REST (also Thrift)\t</td>\n        <td>Java/Apache/Custom, binary (Thrift)</td>\n    </tr>\n    <tr>\n        <td>出发点</td>\n        <td>BigTable</td>\n        <td>BigTable and Dynamo</td>\n    </tr>\n    <tr>\n        <td>架构</td>\n        <td>master/slave\t</td>\n        <td>p2p</td>\n    </tr>\n    <tr>\n        <td>高可用性</td>\n        <td>NameNode是HDFS的单点故障点\t</td>\n        <td>P2P和去中心化设计，不会出现单点故障</td>\n    </tr>\n    <tr>\n        <td>伸缩性\t</td>\n        <td>Region Server扩容，通过将自身发布到Master，Master均匀分布Region\t</td>\n        <td>扩容需在Hash Ring上多个节点间调整数据分布</td>\n    </tr>\n    <tr>\n        <td>一致性\t</td>\n        <td>强一致性\t</td>\n        <td>最终一致性，Quorum NRW策略</td>\n    </tr>\n    <tr>\n        <td>存储目标/数据分布</td>\n        <td>大文件/表划分为多个region存在不同region server上\t</td>\n        <td>小文件/改进的一致性哈希（虚拟节点）</td>\n    </tr>\n    <tr>\n        <td>成员通信及错误检测\t</td>\n        <td>Zookeeper\t</td>\n        <td>基于Gossip/P2P</td>\n    </tr>\n    <tr>\n        <td>读写性能\t</td>\n        <td>数据读写定位可能要通过最多6次的网络RPC，性能较低。\t\t</td>\n        <td>数据读写定位非常快</td>\n    </tr>\n    <tr>\n        <td>数据冲突处理\t</td>\n        <td>乐观并发控制（optimistic concurrency control）\t\t</td>\n        <td>向量时钟</td>\n    </tr>\n    <tr>\n        <td>临时故障处理\t</td>\n        <td>Region Server宕机，重做HLog\t\t</td>\n        <td>数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。</td>\n    </tr>\n    <tr>\n        <td>永久故障恢复\t\t</td>\n        <td>Region Server恢复，master重新给其分配region\t</td>\n        <td>Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性</td>\n    </tr>\n    <tr>\n        <td>CAP\t</td>\n        <td>1，强一致性，0数据丢失。2，可用性低。3，扩容方便。\t\t\t</td>\n        <td>1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。</td>\n    </tr>\n\n</table>\n","slug":"hbase","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bg0000ei272qia5bwfa","content":"<h2 id=\"HBase-QuickStart\"><a href=\"#HBase-QuickStart\" class=\"headerlink\" title=\"HBase QuickStart\"></a>HBase QuickStart</h2><p>HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。</p>\n<p>With the completed config, issue the command: bin/start-hbase.sh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">2017-05-03 11:54:25,802 INFO  [main] http.HttpServer: Jetty bound to port 16010</div><div class=\"line\">2017-05-03 11:54:25,802 INFO  [main] mortbay.log: jetty-6.1.26</div><div class=\"line\">2017-05-03 11:54:25,998 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010</div></pre></td></tr></table></figure>\n<p>Go to <a href=\"http://localhost:16010\" target=\"_blank\" rel=\"external\">http://localhost:16010</a> to view the HBase Web UI.</p>\n<h2 id=\"HBase连接池管理\"><a href=\"#HBase连接池管理\" class=\"headerlink\" title=\"HBase连接池管理\"></a>HBase连接池管理</h2><p>ConnectionFactory 是一个不可实例化的类，专门用于创建HBase的Connection。最简单的创建Connection实例的方式是ConnectionFactory.createConnection(config)，该方法创建了一个连接到集群的Connection实例，该实例被创建的程序管理。通过这个Connection实例，可以使用Connection.getTable()方法取得Table，例如:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">Connection connection = ConnectionFactory.createConnection(config);</div><div class=\"line\">Table table = connection.getTable(TableName.valueOf(&quot;table1&quot;));</div><div class=\"line\">try &#123;</div><div class=\"line\"> // Use the table as needed, for a single operation and a single thread</div><div class=\"line\">&#125; finally &#123;</div><div class=\"line\"> table.close();</div><div class=\"line\"> connection.close();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p> <em>**</em> HConnectionManager在新版本中已经不建议使用。</p>\n<h2 id=\"HBase客户端的使用\"><a href=\"#HBase客户端的使用\" class=\"headerlink\" title=\"HBase客户端的使用\"></a>HBase客户端的使用</h2><p> <a href=\"samplecodes/hbaseclient/HBaseSample.java\">客户端的使用Sample</a></p>\n<h2 id=\"Cassandra\"><a href=\"#Cassandra\" class=\"headerlink\" title=\"Cassandra\"></a>Cassandra</h2><p>Apache Cassandra是高度可扩展的，高性能的分布式NoSQL数据库。 Cassandra旨在处理许多商品服务器上的大量数据，提供高可用性而无需担心单点故障。</p>\n<p>Cassandra具有能够处理大量数据的分布式架构。 数据放置在具有多个复制因子的不同机器上，以获得高可用性，而无需担心单点故障。它在其节点之间具有对等分布式系统，数据分布在集群中的所有节点上。</p>\n<ul>\n<li>在Cassandra中，每个节点是独立的，同时与其他节点互连。 集群中的所有节点都扮演着相同的角色。</li>\n<li>集群中的每个节点都可以接受读取和写入请求，而不管数据实际位于集群中的位置。</li>\n<li>在一个节点发生故障的情况下，可以从网络中的其他节点提供读/写请求。</li>\n</ul>\n<h2 id=\"Cassandra和HBase对比\"><a href=\"#Cassandra和HBase对比\" class=\"headerlink\" title=\"Cassandra和HBase对比\"></a>Cassandra和HBase对比</h2><p>Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。</p>\n<ul>\n<li>Cassandra部署更简单。Cassandra只有一种角色，而HBase除了Region Server外还需要Zookeeper来同步集群状态</li>\n<li>数据一致性是否可配置。Cassandra的数据一致性是可配置的，可以更改为最终一致性，而HBase是强一致性的</li>\n<li>负载均衡算法不同。Cassandra通过一致性哈希来决定数据存储的位置，而HBase靠Master节点管理数据的分配，将过热的节点上的Region动态分配给负载较低的节点。因此Cassandra的平均性能会优于HBase，但是HBase有Master节点，热数据的负载更均衡。</li>\n<li>单点问题。正是由于HBase存在Master节点，因此会存在单点问题。</li>\n</ul>\n<table><br>    <th><br>        </th><td>HBase</td><br>        <td>Cassandra</td><br>    <br>    <tr><br>        <td>语言/License/交互协议</td><br>        <td>Java/Apache/HTTP/REST (also Thrift)    </td><br>        <td>Java/Apache/Custom, binary (Thrift)</td><br>    </tr><br>    <tr><br>        <td>出发点</td><br>        <td>BigTable</td><br>        <td>BigTable and Dynamo</td><br>    </tr><br>    <tr><br>        <td>架构</td><br>        <td>master/slave    </td><br>        <td>p2p</td><br>    </tr><br>    <tr><br>        <td>高可用性</td><br>        <td>NameNode是HDFS的单点故障点    </td><br>        <td>P2P和去中心化设计，不会出现单点故障</td><br>    </tr><br>    <tr><br>        <td>伸缩性    </td><br>        <td>Region Server扩容，通过将自身发布到Master，Master均匀分布Region    </td><br>        <td>扩容需在Hash Ring上多个节点间调整数据分布</td><br>    </tr><br>    <tr><br>        <td>一致性    </td><br>        <td>强一致性    </td><br>        <td>最终一致性，Quorum NRW策略</td><br>    </tr><br>    <tr><br>        <td>存储目标/数据分布</td><br>        <td>大文件/表划分为多个region存在不同region server上    </td><br>        <td>小文件/改进的一致性哈希（虚拟节点）</td><br>    </tr><br>    <tr><br>        <td>成员通信及错误检测    </td><br>        <td>Zookeeper    </td><br>        <td>基于Gossip/P2P</td><br>    </tr><br>    <tr><br>        <td>读写性能    </td><br>        <td>数据读写定位可能要通过最多6次的网络RPC，性能较低。        </td><br>        <td>数据读写定位非常快</td><br>    </tr><br>    <tr><br>        <td>数据冲突处理    </td><br>        <td>乐观并发控制（optimistic concurrency control）        </td><br>        <td>向量时钟</td><br>    </tr><br>    <tr><br>        <td>临时故障处理    </td><br>        <td>Region Server宕机，重做HLog        </td><br>        <td>数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。</td><br>    </tr><br>    <tr><br>        <td>永久故障恢复        </td><br>        <td>Region Server恢复，master重新给其分配region    </td><br>        <td>Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性</td><br>    </tr><br>    <tr><br>        <td>CAP    </td><br>        <td>1，强一致性，0数据丢失。2，可用性低。3，扩容方便。            </td><br>        <td>1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。</td><br>    </tr><br><br></table>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"HBase-QuickStart\"><a href=\"#HBase-QuickStart\" class=\"headerlink\" title=\"HBase QuickStart\"></a>HBase QuickStart</h2><p>HBase是一个开源的分布式存储系统。他可以看作是Google的Bigtable的开源实现。如同Google的Bigtable使用Google File System一样，HBase构建于和Google File System类似的Hadoop HDFS之上。</p>\n<p>With the completed config, issue the command: bin/start-hbase.sh</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">2017-05-03 11:54:25,802 INFO  [main] http.HttpServer: Jetty bound to port 16010</div><div class=\"line\">2017-05-03 11:54:25,802 INFO  [main] mortbay.log: jetty-6.1.26</div><div class=\"line\">2017-05-03 11:54:25,998 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010</div></pre></td></tr></table></figure>\n<p>Go to <a href=\"http://localhost:16010\" target=\"_blank\" rel=\"external\">http://localhost:16010</a> to view the HBase Web UI.</p>\n<h2 id=\"HBase连接池管理\"><a href=\"#HBase连接池管理\" class=\"headerlink\" title=\"HBase连接池管理\"></a>HBase连接池管理</h2><p>ConnectionFactory 是一个不可实例化的类，专门用于创建HBase的Connection。最简单的创建Connection实例的方式是ConnectionFactory.createConnection(config)，该方法创建了一个连接到集群的Connection实例，该实例被创建的程序管理。通过这个Connection实例，可以使用Connection.getTable()方法取得Table，例如:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">Connection connection = ConnectionFactory.createConnection(config);</div><div class=\"line\">Table table = connection.getTable(TableName.valueOf(&quot;table1&quot;));</div><div class=\"line\">try &#123;</div><div class=\"line\"> // Use the table as needed, for a single operation and a single thread</div><div class=\"line\">&#125; finally &#123;</div><div class=\"line\"> table.close();</div><div class=\"line\"> connection.close();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p> <em>**</em> HConnectionManager在新版本中已经不建议使用。</p>\n<h2 id=\"HBase客户端的使用\"><a href=\"#HBase客户端的使用\" class=\"headerlink\" title=\"HBase客户端的使用\"></a>HBase客户端的使用</h2><p> <a href=\"samplecodes/hbaseclient/HBaseSample.java\">客户端的使用Sample</a></p>\n<h2 id=\"Cassandra\"><a href=\"#Cassandra\" class=\"headerlink\" title=\"Cassandra\"></a>Cassandra</h2><p>Apache Cassandra是高度可扩展的，高性能的分布式NoSQL数据库。 Cassandra旨在处理许多商品服务器上的大量数据，提供高可用性而无需担心单点故障。</p>\n<p>Cassandra具有能够处理大量数据的分布式架构。 数据放置在具有多个复制因子的不同机器上，以获得高可用性，而无需担心单点故障。它在其节点之间具有对等分布式系统，数据分布在集群中的所有节点上。</p>\n<ul>\n<li>在Cassandra中，每个节点是独立的，同时与其他节点互连。 集群中的所有节点都扮演着相同的角色。</li>\n<li>集群中的每个节点都可以接受读取和写入请求，而不管数据实际位于集群中的位置。</li>\n<li>在一个节点发生故障的情况下，可以从网络中的其他节点提供读/写请求。</li>\n</ul>\n<h2 id=\"Cassandra和HBase对比\"><a href=\"#Cassandra和HBase对比\" class=\"headerlink\" title=\"Cassandra和HBase对比\"></a>Cassandra和HBase对比</h2><p>Cassandra可以看作是Amazon Dynamo的开源实现。和Dynamo不同之处在于，Cassandra结合了Google Bigtable的ColumnFamily的数据模型。可以简单地认为，Cassandra是一个P2P的，高可靠性并具有丰富的数据模型的分布式文件系统。</p>\n<ul>\n<li>Cassandra部署更简单。Cassandra只有一种角色，而HBase除了Region Server外还需要Zookeeper来同步集群状态</li>\n<li>数据一致性是否可配置。Cassandra的数据一致性是可配置的，可以更改为最终一致性，而HBase是强一致性的</li>\n<li>负载均衡算法不同。Cassandra通过一致性哈希来决定数据存储的位置，而HBase靠Master节点管理数据的分配，将过热的节点上的Region动态分配给负载较低的节点。因此Cassandra的平均性能会优于HBase，但是HBase有Master节点，热数据的负载更均衡。</li>\n<li>单点问题。正是由于HBase存在Master节点，因此会存在单点问题。</li>\n</ul>\n<table><br>    <th><br>        </th><td>HBase</td><br>        <td>Cassandra</td><br>    <br>    <tr><br>        <td>语言/License/交互协议</td><br>        <td>Java/Apache/HTTP/REST (also Thrift)    </td><br>        <td>Java/Apache/Custom, binary (Thrift)</td><br>    </tr><br>    <tr><br>        <td>出发点</td><br>        <td>BigTable</td><br>        <td>BigTable and Dynamo</td><br>    </tr><br>    <tr><br>        <td>架构</td><br>        <td>master/slave    </td><br>        <td>p2p</td><br>    </tr><br>    <tr><br>        <td>高可用性</td><br>        <td>NameNode是HDFS的单点故障点    </td><br>        <td>P2P和去中心化设计，不会出现单点故障</td><br>    </tr><br>    <tr><br>        <td>伸缩性    </td><br>        <td>Region Server扩容，通过将自身发布到Master，Master均匀分布Region    </td><br>        <td>扩容需在Hash Ring上多个节点间调整数据分布</td><br>    </tr><br>    <tr><br>        <td>一致性    </td><br>        <td>强一致性    </td><br>        <td>最终一致性，Quorum NRW策略</td><br>    </tr><br>    <tr><br>        <td>存储目标/数据分布</td><br>        <td>大文件/表划分为多个region存在不同region server上    </td><br>        <td>小文件/改进的一致性哈希（虚拟节点）</td><br>    </tr><br>    <tr><br>        <td>成员通信及错误检测    </td><br>        <td>Zookeeper    </td><br>        <td>基于Gossip/P2P</td><br>    </tr><br>    <tr><br>        <td>读写性能    </td><br>        <td>数据读写定位可能要通过最多6次的网络RPC，性能较低。        </td><br>        <td>数据读写定位非常快</td><br>    </tr><br>    <tr><br>        <td>数据冲突处理    </td><br>        <td>乐观并发控制（optimistic concurrency control）        </td><br>        <td>向量时钟</td><br>    </tr><br>    <tr><br>        <td>临时故障处理    </td><br>        <td>Region Server宕机，重做HLog        </td><br>        <td>数据回传机制：某节点宕机，hash到该节点的新数据自动路由到下一节点做 hinted handoff，源节点恢复后，推送回源节点。</td><br>    </tr><br>    <tr><br>        <td>永久故障恢复        </td><br>        <td>Region Server恢复，master重新给其分配region    </td><br>        <td>Merkle 哈希树，通过Gossip协议同步Merkle Tree，维护集群节点间的数据一致性</td><br>    </tr><br>    <tr><br>        <td>CAP    </td><br>        <td>1，强一致性，0数据丢失。2，可用性低。3，扩容方便。            </td><br>        <td>1，弱一致性，数据可能丢失。2，可用性高。3，扩容方便。</td><br>    </tr><br><br></table>\n"},{"title":"Kubernetes Ingress","date":"2017-03-09T12:46:25.000Z","_content":"\n## 相关术语\n- 节点：Kubernetes集群中的一台物理机或者虚拟机。\n- 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。\n- 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。\n- 集群网络：一组逻辑或物理链接，可根据Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的 OVS 。\n- 服务：使用标签选择器标识一组pod成为的Kubernetes 服务 。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。\n\n毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。\n\nService在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。\n\n将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。\n\n\n## 什么是Ingress\n通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：\n```\n    internet\n        |\n  ------------\n  [ Services ]\n```\nIngress是授权入站连接到达集群服务的规则集合。\n\n```\n    internet\n        |\n   [ Ingress ]\n   --|-----|--\n   [ Services ]\n```\n可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller 负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。\n\n## 自定义Secrets\n最简化的Ingress配置：\n\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        backend:\n          serviceName: test\n          servicePort: 80\n```\n1-4行：跟Kubernetes的其他配置一样，ingress的配置也需要 apiVersion ， kind 和 metadata 字段。配置文件的详细说明请查看 部署应用 , 配置容器 和 使用resources .\n\n5-7行: Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。\n\n8-9行：每条http规则包含以下信息：一个 host 配置项（比如for.bar.com，在这个例子中默认是*）， path 列表（比如：/testpath），每个path都关联一个 backend (比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。\n\n10-12行：正如 services doc 中描述的那样，backend是一个 service:port 的组合。Ingress的流量被转发到它所匹配的backend。\n\n## Ingress类型\n### 单Service Ingress\nKubernetes中已经存在一些概念可以暴露单个service（查看 替代方案 ），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。\n\ningress.yaml定义文件：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  backend:\n    serviceName: testsvc\n    servicePort: 80\n```\n\n使用 kubectl create -f 命令创建，然后查看ingress：\n```\n$ kubectl get ing\nNAME                RULE          BACKEND        ADDRESS\ntest-ingress        -             testsvc:80     107.178.254.228\n```\n\n### 简单展开\n如前面描述的那样，kubernete pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，嫁入你想要创建这样的一个设置：\n```\nfoo.bar.com -> 178.91.123.132 -> / foo    s1:80\n                                 / bar    s2:80\n```\n需要一个这样的ingress：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        backend:\n          serviceName: s1\n          servicePort: 80\n      - path: /bar\n        backend:\n          serviceName: s2\n          servicePort: 80\n```\n\n使用 kubectl create -f 创建完ingress后：\n```\n$ kubectl get ing\nNAME      RULE          BACKEND   ADDRESS\ntest      -\n          foo.bar.com\n          /foo          s1:80\n          /bar          s2:80\n```\n\n### 基于名称的虚拟主机\nName-based的虚拟主机在同一个IP地址下拥有多个主机名。\n```\nfoo.bar.com --|                 |-> foo.bar.com s1:80\n              | 178.91.123.132  |\nbar.foo.com --|                 |-> bar.foo.com s2:80\n```\n下面这个ingress说明基于 Host header 的后端loadbalancer的路由请求：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - backend:\n          serviceName: s1\n          servicePort: 80\n  - host: bar.foo.com\n    http:\n      paths:\n      - backend:\n          serviceName: s2\n          servicePort: 80\n```\n## 使用Nginx设置负载均衡\n在任意情况下，当创建新的Kubernetes services的时候，需要一个机制来动态地重新部署负载均衡器.\n\n通过使用confd来检测配置在etcd内的修改，并基于一个模版生成一个新的nginx配置文件.\n\n![ingress-nginx](/images/ingress-nginx.png)\n\n## 替代方案\n可以通过很多种方式暴露service而不必直接使用ingress：\n- 使用 Service.Type=LoadBalancer\n- 使用 Service.Type=NodePort\n- 使用 Port Proxy\n- 部署一个 Service loadbalancer 这允许你在多个service之间共享单个IP，并通过Service Annotations实现更高级的负载平衡。\n","source":"_posts/kubernetes-ingress.md","raw":"---\ntitle: Kubernetes Ingress\ndate: 2017-3-9 20:46:25\ncategories:\n  - 分布式&云计算\n  - Kubernetes\ntags:\n  - 分布式\n  - Kubernetes\n  - 路由\n  - 容器\n  - Ingress\n---\n\n## 相关术语\n- 节点：Kubernetes集群中的一台物理机或者虚拟机。\n- 集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。\n- 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。\n- 集群网络：一组逻辑或物理链接，可根据Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的 OVS 。\n- 服务：使用标签选择器标识一组pod成为的Kubernetes 服务 。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。\n\n毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。\n\nService在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。\n\n将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。\n\n\n## 什么是Ingress\n通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：\n```\n    internet\n        |\n  ------------\n  [ Services ]\n```\nIngress是授权入站连接到达集群服务的规则集合。\n\n```\n    internet\n        |\n   [ Ingress ]\n   --|-----|--\n   [ Services ]\n```\n可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller 负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。\n\n## 自定义Secrets\n最简化的Ingress配置：\n\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        backend:\n          serviceName: test\n          servicePort: 80\n```\n1-4行：跟Kubernetes的其他配置一样，ingress的配置也需要 apiVersion ， kind 和 metadata 字段。配置文件的详细说明请查看 部署应用 , 配置容器 和 使用resources .\n\n5-7行: Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。\n\n8-9行：每条http规则包含以下信息：一个 host 配置项（比如for.bar.com，在这个例子中默认是*）， path 列表（比如：/testpath），每个path都关联一个 backend (比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。\n\n10-12行：正如 services doc 中描述的那样，backend是一个 service:port 的组合。Ingress的流量被转发到它所匹配的backend。\n\n## Ingress类型\n### 单Service Ingress\nKubernetes中已经存在一些概念可以暴露单个service（查看 替代方案 ），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。\n\ningress.yaml定义文件：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  backend:\n    serviceName: testsvc\n    servicePort: 80\n```\n\n使用 kubectl create -f 命令创建，然后查看ingress：\n```\n$ kubectl get ing\nNAME                RULE          BACKEND        ADDRESS\ntest-ingress        -             testsvc:80     107.178.254.228\n```\n\n### 简单展开\n如前面描述的那样，kubernete pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，嫁入你想要创建这样的一个设置：\n```\nfoo.bar.com -> 178.91.123.132 -> / foo    s1:80\n                                 / bar    s2:80\n```\n需要一个这样的ingress：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        backend:\n          serviceName: s1\n          servicePort: 80\n      - path: /bar\n        backend:\n          serviceName: s2\n          servicePort: 80\n```\n\n使用 kubectl create -f 创建完ingress后：\n```\n$ kubectl get ing\nNAME      RULE          BACKEND   ADDRESS\ntest      -\n          foo.bar.com\n          /foo          s1:80\n          /bar          s2:80\n```\n\n### 基于名称的虚拟主机\nName-based的虚拟主机在同一个IP地址下拥有多个主机名。\n```\nfoo.bar.com --|                 |-> foo.bar.com s1:80\n              | 178.91.123.132  |\nbar.foo.com --|                 |-> bar.foo.com s2:80\n```\n下面这个ingress说明基于 Host header 的后端loadbalancer的路由请求：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - backend:\n          serviceName: s1\n          servicePort: 80\n  - host: bar.foo.com\n    http:\n      paths:\n      - backend:\n          serviceName: s2\n          servicePort: 80\n```\n## 使用Nginx设置负载均衡\n在任意情况下，当创建新的Kubernetes services的时候，需要一个机制来动态地重新部署负载均衡器.\n\n通过使用confd来检测配置在etcd内的修改，并基于一个模版生成一个新的nginx配置文件.\n\n![ingress-nginx](/images/ingress-nginx.png)\n\n## 替代方案\n可以通过很多种方式暴露service而不必直接使用ingress：\n- 使用 Service.Type=LoadBalancer\n- 使用 Service.Type=NodePort\n- 使用 Port Proxy\n- 部署一个 Service loadbalancer 这允许你在多个service之间共享单个IP，并通过Service Annotations实现更高级的负载平衡。\n","slug":"kubernetes-ingress","published":1,"updated":"2017-05-10T07:07:03.000Z","_id":"cj2in1bg1000hi272akte0wzm","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"相关术语\"><a href=\"#相关术语\" class=\"headerlink\" title=\"相关术语\"></a>相关术语</h2><ul>\n<li>节点：Kubernetes集群中的一台物理机或者虚拟机。</li>\n<li>集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。</li>\n<li>边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。</li>\n<li>集群网络：一组逻辑或物理链接，可根据Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的 OVS 。</li>\n<li>服务：使用标签选择器标识一组pod成为的Kubernetes 服务 。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。</li>\n</ul>\n<p>毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。</p>\n<p>Service在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。</p>\n<p>将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。</p>\n<h2 id=\"什么是Ingress\"><a href=\"#什么是Ingress\" class=\"headerlink\" title=\"什么是Ingress\"></a>什么是Ingress</h2><p>通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">  internet</div><div class=\"line\">      |</div><div class=\"line\">------------</div><div class=\"line\">[ Services ]</div></pre></td></tr></table></figure></p>\n<p>Ingress是授权入站连接到达集群服务的规则集合。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"> internet</div><div class=\"line\">     |</div><div class=\"line\">[ Ingress ]</div><div class=\"line\">--|-----|--</div><div class=\"line\">[ Services ]</div></pre></td></tr></table></figure>\n<p>可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller 负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。</p>\n<h2 id=\"自定义Secrets\"><a href=\"#自定义Secrets\" class=\"headerlink\" title=\"自定义Secrets\"></a>自定义Secrets</h2><p>最简化的Ingress配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-ingress</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - path: /testpath</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: test</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure>\n<p>1-4行：跟Kubernetes的其他配置一样，ingress的配置也需要 apiVersion ， kind 和 metadata 字段。配置文件的详细说明请查看 部署应用 , 配置容器 和 使用resources .</p>\n<p>5-7行: Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。</p>\n<p>8-9行：每条http规则包含以下信息：一个 host 配置项（比如for.bar.com，在这个例子中默认是*）， path 列表（比如：/testpath），每个path都关联一个 backend (比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。</p>\n<p>10-12行：正如 services doc 中描述的那样，backend是一个 service:port 的组合。Ingress的流量被转发到它所匹配的backend。</p>\n<h2 id=\"Ingress类型\"><a href=\"#Ingress类型\" class=\"headerlink\" title=\"Ingress类型\"></a>Ingress类型</h2><h3 id=\"单Service-Ingress\"><a href=\"#单Service-Ingress\" class=\"headerlink\" title=\"单Service Ingress\"></a>单Service Ingress</h3><p>Kubernetes中已经存在一些概念可以暴露单个service（查看 替代方案 ），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。</p>\n<p>ingress.yaml定义文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-ingress</div><div class=\"line\">spec:</div><div class=\"line\">  backend:</div><div class=\"line\">    serviceName: testsvc</div><div class=\"line\">    servicePort: 80</div></pre></td></tr></table></figure></p>\n<p>使用 kubectl create -f 命令创建，然后查看ingress：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ kubectl get ing</div><div class=\"line\">NAME                RULE          BACKEND        ADDRESS</div><div class=\"line\">test-ingress        -             testsvc:80     107.178.254.228</div></pre></td></tr></table></figure></p>\n<h3 id=\"简单展开\"><a href=\"#简单展开\" class=\"headerlink\" title=\"简单展开\"></a>简单展开</h3><p>如前面描述的那样，kubernete pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，嫁入你想要创建这样的一个设置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    s1:80</div><div class=\"line\">                                 / bar    s2:80</div></pre></td></tr></table></figure></p>\n<p>需要一个这样的ingress：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - host: foo.bar.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - path: /foo</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: s1</div><div class=\"line\">          servicePort: 80</div><div class=\"line\">      - path: /bar</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: s2</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure></p>\n<p>使用 kubectl create -f 创建完ingress后：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ kubectl get ing</div><div class=\"line\">NAME      RULE          BACKEND   ADDRESS</div><div class=\"line\">test      -</div><div class=\"line\">          foo.bar.com</div><div class=\"line\">          /foo          s1:80</div><div class=\"line\">          /bar          s2:80</div></pre></td></tr></table></figure></p>\n<h3 id=\"基于名称的虚拟主机\"><a href=\"#基于名称的虚拟主机\" class=\"headerlink\" title=\"基于名称的虚拟主机\"></a>基于名称的虚拟主机</h3><p>Name-based的虚拟主机在同一个IP地址下拥有多个主机名。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">foo.bar.com --|                 |-&gt; foo.bar.com s1:80</div><div class=\"line\">              | 178.91.123.132  |</div><div class=\"line\">bar.foo.com --|                 |-&gt; bar.foo.com s2:80</div></pre></td></tr></table></figure></p>\n<p>下面这个ingress说明基于 Host header 的后端loadbalancer的路由请求：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - host: foo.bar.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - backend:</div><div class=\"line\">          serviceName: s1</div><div class=\"line\">          servicePort: 80</div><div class=\"line\">  - host: bar.foo.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - backend:</div><div class=\"line\">          serviceName: s2</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用Nginx设置负载均衡\"><a href=\"#使用Nginx设置负载均衡\" class=\"headerlink\" title=\"使用Nginx设置负载均衡\"></a>使用Nginx设置负载均衡</h2><p>在任意情况下，当创建新的Kubernetes services的时候，需要一个机制来动态地重新部署负载均衡器.</p>\n<p>通过使用confd来检测配置在etcd内的修改，并基于一个模版生成一个新的nginx配置文件.</p>\n<p><img src=\"/images/ingress-nginx.png\" alt=\"ingress-nginx\"></p>\n<h2 id=\"替代方案\"><a href=\"#替代方案\" class=\"headerlink\" title=\"替代方案\"></a>替代方案</h2><p>可以通过很多种方式暴露service而不必直接使用ingress：</p>\n<ul>\n<li>使用 Service.Type=LoadBalancer</li>\n<li>使用 Service.Type=NodePort</li>\n<li>使用 Port Proxy</li>\n<li>部署一个 Service loadbalancer 这允许你在多个service之间共享单个IP，并通过Service Annotations实现更高级的负载平衡。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"相关术语\"><a href=\"#相关术语\" class=\"headerlink\" title=\"相关术语\"></a>相关术语</h2><ul>\n<li>节点：Kubernetes集群中的一台物理机或者虚拟机。</li>\n<li>集群：位于Internet防火墙后的节点，这是kubernetes管理的主要计算资源。</li>\n<li>边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。</li>\n<li>集群网络：一组逻辑或物理链接，可根据Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括Overlay模型的 flannel 和基于SDN的 OVS 。</li>\n<li>服务：使用标签选择器标识一组pod成为的Kubernetes 服务 。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟IP访问。</li>\n</ul>\n<p>毋庸置疑,容器是未来的最为重要的配置编排格式之一, 打包应用程序也将会变得更加容易。虽然像Docker这样的工具提供真实的容器，但是也需要其他工具来处理如replication，failover以及API来自动化部署到多个机器。</p>\n<p>Service在部署之前存在一个IP地址，但是这个地址只存在于Kubernetes集群之内。这也就意味着service对于网络来说根本不可用！当运行在谷歌GCE上的时候（像我们一样），Kubernetes能够自动配置一个负载均衡器来访问应用程序。如果你不是在谷歌GCE上面的话，你就需要做些额外的工作来使负载均衡运行起来。</p>\n<p>将service直接暴露到一个主机端口也可以，这就是经常使用的方式，但这会令很多Kubernetes的优势无法充分发挥。如果依赖主机上的端口，那么当部署多个应用程序的时候，就会陷入端口冲突,这也使得调度集群或者替代主机变得更加困难。</p>\n<h2 id=\"什么是Ingress\"><a href=\"#什么是Ingress\" class=\"headerlink\" title=\"什么是Ingress\"></a>什么是Ingress</h2><p>通常情况下，service和pod仅可在集群内部网络中通过IP地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">  internet</div><div class=\"line\">      |</div><div class=\"line\">------------</div><div class=\"line\">[ Services ]</div></pre></td></tr></table></figure></p>\n<p>Ingress是授权入站连接到达集群服务的规则集合。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"> internet</div><div class=\"line\">     |</div><div class=\"line\">[ Ingress ]</div><div class=\"line\">--|-----|--</div><div class=\"line\">[ Services ]</div></pre></td></tr></table></figure>\n<p>可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过POST Ingress资源到API server的方式来请求ingress。 Ingress controller 负责实现Ingress，通常使用负载平衡器，它还可以配置边界路由和其他前端，这有助于以HA方式处理流量。</p>\n<h2 id=\"自定义Secrets\"><a href=\"#自定义Secrets\" class=\"headerlink\" title=\"自定义Secrets\"></a>自定义Secrets</h2><p>最简化的Ingress配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-ingress</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - path: /testpath</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: test</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure>\n<p>1-4行：跟Kubernetes的其他配置一样，ingress的配置也需要 apiVersion ， kind 和 metadata 字段。配置文件的详细说明请查看 部署应用 , 配置容器 和 使用resources .</p>\n<p>5-7行: Ingress spec 中包含配置一个loadbalancer或proxy server的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前ingress只支持http规则。</p>\n<p>8-9行：每条http规则包含以下信息：一个 host 配置项（比如for.bar.com，在这个例子中默认是*）， path 列表（比如：/testpath），每个path都关联一个 backend (比如test:80)。在loadbalancer将流量转发到backend之前，所有的入站请求都要先匹配host和path。</p>\n<p>10-12行：正如 services doc 中描述的那样，backend是一个 service:port 的组合。Ingress的流量被转发到它所匹配的backend。</p>\n<h2 id=\"Ingress类型\"><a href=\"#Ingress类型\" class=\"headerlink\" title=\"Ingress类型\"></a>Ingress类型</h2><h3 id=\"单Service-Ingress\"><a href=\"#单Service-Ingress\" class=\"headerlink\" title=\"单Service Ingress\"></a>单Service Ingress</h3><p>Kubernetes中已经存在一些概念可以暴露单个service（查看 替代方案 ），但是你仍然可以通过Ingress来实现，通过指定一个没有rule的默认backend的方式。</p>\n<p>ingress.yaml定义文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-ingress</div><div class=\"line\">spec:</div><div class=\"line\">  backend:</div><div class=\"line\">    serviceName: testsvc</div><div class=\"line\">    servicePort: 80</div></pre></td></tr></table></figure></p>\n<p>使用 kubectl create -f 命令创建，然后查看ingress：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ kubectl get ing</div><div class=\"line\">NAME                RULE          BACKEND        ADDRESS</div><div class=\"line\">test-ingress        -             testsvc:80     107.178.254.228</div></pre></td></tr></table></figure></p>\n<h3 id=\"简单展开\"><a href=\"#简单展开\" class=\"headerlink\" title=\"简单展开\"></a>简单展开</h3><p>如前面描述的那样，kubernete pod中的IP只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收ingress的流量并将它们转发到正确的端点上。这个东西一般是高可用的loadbalancer。使用Ingress能够允许你将loadbalancer的个数降低到最少，例如，嫁入你想要创建这样的一个设置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    s1:80</div><div class=\"line\">                                 / bar    s2:80</div></pre></td></tr></table></figure></p>\n<p>需要一个这样的ingress：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - host: foo.bar.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - path: /foo</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: s1</div><div class=\"line\">          servicePort: 80</div><div class=\"line\">      - path: /bar</div><div class=\"line\">        backend:</div><div class=\"line\">          serviceName: s2</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure></p>\n<p>使用 kubectl create -f 创建完ingress后：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ kubectl get ing</div><div class=\"line\">NAME      RULE          BACKEND   ADDRESS</div><div class=\"line\">test      -</div><div class=\"line\">          foo.bar.com</div><div class=\"line\">          /foo          s1:80</div><div class=\"line\">          /bar          s2:80</div></pre></td></tr></table></figure></p>\n<h3 id=\"基于名称的虚拟主机\"><a href=\"#基于名称的虚拟主机\" class=\"headerlink\" title=\"基于名称的虚拟主机\"></a>基于名称的虚拟主机</h3><p>Name-based的虚拟主机在同一个IP地址下拥有多个主机名。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">foo.bar.com --|                 |-&gt; foo.bar.com s1:80</div><div class=\"line\">              | 178.91.123.132  |</div><div class=\"line\">bar.foo.com --|                 |-&gt; bar.foo.com s2:80</div></pre></td></tr></table></figure></p>\n<p>下面这个ingress说明基于 Host header 的后端loadbalancer的路由请求：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: extensions/v1beta1</div><div class=\"line\">kind: Ingress</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test</div><div class=\"line\">spec:</div><div class=\"line\">  rules:</div><div class=\"line\">  - host: foo.bar.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - backend:</div><div class=\"line\">          serviceName: s1</div><div class=\"line\">          servicePort: 80</div><div class=\"line\">  - host: bar.foo.com</div><div class=\"line\">    http:</div><div class=\"line\">      paths:</div><div class=\"line\">      - backend:</div><div class=\"line\">          serviceName: s2</div><div class=\"line\">          servicePort: 80</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用Nginx设置负载均衡\"><a href=\"#使用Nginx设置负载均衡\" class=\"headerlink\" title=\"使用Nginx设置负载均衡\"></a>使用Nginx设置负载均衡</h2><p>在任意情况下，当创建新的Kubernetes services的时候，需要一个机制来动态地重新部署负载均衡器.</p>\n<p>通过使用confd来检测配置在etcd内的修改，并基于一个模版生成一个新的nginx配置文件.</p>\n<p><img src=\"/images/ingress-nginx.png\" alt=\"ingress-nginx\"></p>\n<h2 id=\"替代方案\"><a href=\"#替代方案\" class=\"headerlink\" title=\"替代方案\"></a>替代方案</h2><p>可以通过很多种方式暴露service而不必直接使用ingress：</p>\n<ul>\n<li>使用 Service.Type=LoadBalancer</li>\n<li>使用 Service.Type=NodePort</li>\n<li>使用 Port Proxy</li>\n<li>部署一个 Service loadbalancer 这允许你在多个service之间共享单个IP，并通过Service Annotations实现更高级的负载平衡。</li>\n</ul>\n"},{"title":"Kubernetes","date":"2017-02-12T12:46:25.000Z","_content":"\n## 基础架构\nKubernetes将底层的计算资源连接在一起对外体现为一个计算集群，并将资源高度抽象化。部署应用时Kubernetes会以更高效的方式自动的将应用分发到集群内的机器上面，并调度运行。\n\nKubernetes集群包含两种类型的资源：\n- Master节点：协调控制整个集群。Master负责管理整个集群，协调集群内的所有行为。比如调度应用，监控应用的状态等。\n- Nodes节点：运行应用的工作节点。Node节点负责运行应用，一般是一台物理机或者虚机。每个Node节点上面都有一个Kubelet，它是一个代理程序，用来管理该节点以及和Master节点通信。除此以外，Node节点上还会有一些管理容器的工具，比如Docker或者rkt等。生产环境中一个Kubernetes集群至少应该包含三个Nodes节点。\n\n当部署应用的时候，我们通知Master节点启动应用容器。然后Master会调度这些应用将它们运行在Node节点上面。Node节点和Master节点通过Master节点暴露的Kubernetes API通信。当然我们也可以直接通过这些API和集群交互。\n![kubernetes Cluster](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_01_cluster.svg)\n\n### Master\nMaster节点上面主要由四个模块组成：APIServer、scheduler、controller manager、etcd。\n- APIServer\nAPIServer负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。\n- Scheduler\nScheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。\n- Controller manager\n如果说APIServer做的是“前台”的工作的话，那controller manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller manager去保证了。\n- etcd\netcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。\n\n### Node\n每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。\n- kubelet。Kubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上面的所有容器，但是如果容器不是通过Kubernetes创建的，它并不会管理。本质上，它负责使Pod得运行状态与期望的状态一致。\n- kube-proxy。该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。\n- runtime。runtime指的是容器运行环境，目前Kubernetes支持docker和rkt两种容器。\n\n\n## 搭建MiniKube\n环境: MacOS, virtualbox, minikube v0.18.0, kubectl v1.6.0\n\nKubernetes提供了一个轻量级的Minikube应用，利用它我们可以很容器的创建一个只包含一个Node节点的Kubernetes Cluster用于日常的开发测试。\n\n### 安装\nMinikube的安装可以参考: Minikube的Github：https://github.com/kubernetes/minikube\n\n要正常使用，还必须安装kubectl，并且放在PATH里面。kubectl是一个通过Kubernetes API和Kubernetes集群交互的命令行工具。\n\n### 启动\n可以通过minikube查看cluster运行状态,启动或者停止cluster.例如:\n\n```\nminikube start\n```\n\n### ONLY FOR CHINESE\nKubernetes在部署容器应用的时候会先拉一个pause镜像，这个是一个基础容器，主要是负责网络部分的功能的，具体这里不展开讨论。最关键的是Kubernetes里面镜像默认都是从Google的镜像仓库拉的（就跟docker默认从docker hub拉的一样），但是因为GFW的原因，中国用户是访问不了Google的镜像仓库gcr.io的（如果你可以ping通，那恭喜你）。庆幸的是这个镜像被传到了docker hub上面，虽然中国用户访问后者也非常艰难，但通过一些加速器之类的还是可以pull下来的。如果没有VPN等科学上网的工具的话，请先做如下操作：\n\nSee: https://github.com/kubernetes/kubernetes/issues/6888\n\n```\nminikube ssh    # 登录到我们的Kubernetes VM里面去\ndocker pull registry.hnaresearch.com/public/pause-amd64:3.0  \ndocker tag registry.hnaresearch.com/public/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0  \n```\n这样Kubernetes VM就不会从gcr.io拉镜像了，而是会直接使用本地的镜像。\n\n## 部署应用\n\n在Kubernetes Cluster上面部署应用，我们需要先创建一个Kubernetes Deployment。这个Deployment负责创建和更新我们的应用实例。当这个Deployment创建之后，Kubernetes master就会将这个Deployment创建出来的应用实例部署到集群内某个Node节点上。而且自应用实例创建后，Deployment controller还会持续监控应用，直到应用被删除或者部署应用的Node节点不存在。\n>A Deployment is responsible for creating and updating instances of your application.\n\n![](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg)\n\n使用kubectl来创建Deployment，创建的时候需要制定容器镜像以及我们要启动的个数（replicas），当然这些信息后面可以再更新。这里我用Go写了一个简单的Webserver，返回“Hello World”，监听端口是8090.我们就来启动这个应用.\n```\nkubectl run helloworld --image=registry.hnaresearch.com/public/hello-world:v1.0 --port=8090\n```\n执行后master寻找一个合适的node来部署我们的应用实例（我们只有一个node）。我们可以使用kubectl get deployment来查看我们创建的Deployment：\n```\nkubectl get deployment\n```\n默认应用部署好之后是只在Kubernetes Cluster内部可见的，有多种方法可以让我们的应用暴露到外部，这里先介绍一种简单的：我们可以通过kubectl proxy命令在我们的终端和Kubernetes Cluster直接创建一个代理。然后，打开一个新的终端，通过Pod名(Pod后面会有讲到，可以通过kubectl get pod查看Pod名字)就可以访问了：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get pod\nNAME                             READY     STATUS             RESTARTS   AGE\nhello-minikube-938614450-xjl4s   0/1       ImagePullBackOff   0          15h\nhelloworld-2790924137-bvfhn      1/1       Running            0          2m\n\nxis-macbook-pro:~ xiningwang$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/helloworld-2790924137-bvfhn/\nHello world !\nhostname:helloworld-2790924137-bvfhn\n```\n\n## Pod\nPod是Kubernetes中一个非常重要的概念，也是区别于其他编排系统的一个设计. Deployment执行时并不是直接创建了容器实例，而是先在Node上面创建了Pod，然后再在Pod里面创建容器。那Pod到底是什么？Pod是Kubernetes里面抽象出来的一个概念，它是能够被创建、调度和管理的最小单元；每个Pod都有一个独立的IP；一个Pod由若干个容器构成。一个Pod之内的容器共享Pod的所有资源，这些资源主要包括：共享存储（以Volumes的形式）、共享网络、共享端口等。Kubernetes虽然也是一个容器编排系统，但不同于其他系统，它的最小操作单元不是单个容器，而是Pod。这个特性给Kubernetes带来了很多优势，比如最显而易见的是同一个Pod内的容器可以非常方便的互相访问（通过localhost就可以访问）和共享数据。\n\n> A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them.\n\n![pod](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_pods.svg)\n\n>A Node is a group of one ore more pods and includes the kubelet and container engine.  \n>\n>Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.\n\n![node](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_nodes.svg)\n\n## Job\n从程序的运行形态上来区分，我们可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。Replication Controller创建的Pod都是长时运行的服务，而Job创建的Pod都是一次性任务。\n\n在Job的定义中，restartPolicy（重启策略）只能是Never和OnFailure。Job可以控制一次性任务的Pod的完成次数（Job-->spec-->completions）和并发执行数（Job-->spec-->parallelism），当Pod成功执行指定次数后，即认为Job执行完毕。\n\n## Replication Controller\nReplication Controller（RC）是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。\n\n### Replica Set\n新一代副本控制器replica set，可以被认为 是“升级版”的Replication Controller。也就是说。replica set也是用于保证与label selector匹配的pod数量维持在期望状态。区别在于，replica set引入了对基于子集的selector查询条件，而Replication Controller仅支持基于值相等的selecto条件查询。这是目前从用户角度肴，两者唯一的显著差异。 社区引入这一API的初衷是用于取代vl中的Replication Controller，也就是说．当v1版本被废弃时，Replication Controller就完成了它的历史使命，而由replica set来接管其工作。虽然replica set可以被单独使用，但是目前它多被Deployment用于进行pod的创建、更新与删除。\n\n## Service\n### 原理\n[Service](id:service)是Kubernetes里面抽象出来的一层，它定义了由多个Pods组成的逻辑组（logical set），可以对组内的Pod做一些事情：\n- 对外暴露流量\n- 做负载均衡（load balancing）\n- 服务发现（service-discovery）\n\n> A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.\n\n在Kubernetes中，在受到Replication Controller调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。\n\nService同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod-->metadata-->labeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。\n\n需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。\n\n### Service代理服务\n\n![service](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg)\n\n\n使用kubectl get service可以查看目前已有的service，Minikube默认创建了一个kubernetes Service。我们使用expose命令再创建一个Service：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhello-minikube   10.0.0.188   <nodes>       8080:32710/TCP   16h\nkubernetes       10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl expose deployment/helloworld --type=\"NodePort\" --port 8090\nservice \"helloworld\" exposed\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhello-minikube   10.0.0.188   <nodes>       8080:32710/TCP   16h\nhelloworld       10.0.0.249   <nodes>       8090:31240/TCP   2m\nkubernetes       10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl delete service hello-minikube\nservice \"hello-minikube\" deleted\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   2m\nkubernetes   10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl describe service/helloworld\nName:  \t\t\thelloworld\nNamespace:     \t\tdefault\nLabels:\t\t\trun=helloworld\nAnnotations:   \t\t<none>\nSelector:      \t\trun=helloworld\nType:  \t\t\tNodePort\nIP:    \t\t\t10.0.0.249\nPort:  \t\t\t<unset>\t8090/TCP\nNodePort:      \t\t<unset>\t31240/TCP\nEndpoints:     \t\t172.17.0.3:8090\nSession Affinity:      \tNone\nEvents:\t\t\t<none>\nxis-macbook-pro:~ xiningwang$ minikube docker-env\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.100:2376\"\nexport DOCKER_CERT_PATH=\"/Users/xiningwang/.minikube/certs\"\nexport DOCKER_API_VERSION=\"1.23\"\n# Run this command to configure your shell:\n# eval $(minikube docker-env)\nxis-macbook-pro:~ xiningwang$ curl http://192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\n```\n\n### Service内部负载均衡\n当Service的Endpoints包含多个IP的时候，及服务代理存在多个后端，将进行请求的负载均衡。默认的负载均衡策略是轮训或者随机（有kube-proxy的模式决定）。同时，Service上通过设置Service-->spec-->sessionAffinity=ClientIP，来实现基于源IP地址的会话保持。\n\n### 发布Service\nService的虚拟IP是由Kubernetes虚拟出来的内部网络，外部是无法寻址到的。但是有些服务又需要被外部访问到，例如web前段。这时候就需要加一层网络转发，即外网到内网的转发。Kubernetes提供了NodePort、LoadBalancer、Ingress三种方式。\n- NodePort\n在之前的Guestbook示例中，已经延时了NodePort的用法。NodePort的原理是，Kubernetes会在每一个Node上暴露出一个端口：nodePort，外部网络可以通过（任一Node）[NodeIP]:[NodePort]访问到后端的Service。Minikube只支持这种方式。\n- LoadBalancer\n在NodePort基础上，Kubernetes可以请求底层云平台创建一个负载均衡器，将每个Node作为后端，进行服务分发。该模式需要底层云平台（例如GCE）支持。\n- Ingress\n是一种HTTP方式的路由转发机制，由Ingress Controller和HTTP代理服务器组合而成。Ingress Controller实时监控Kubernetes API，实时更新HTTP代理服务器的转发规则。HTTP代理服务器有GCE Load-Balancer、HaProxy、Nginx等开源方案。\n\n### 自发性\nKubernetes中有一个很重要的服务自发现特性。一旦一个service被创建，该service的service IP和service port等信息都可以被注入到pod中供它们使用。Kubernetes主要支持两种service发现 机制：环境变量和DNS。\n- 环境变量方式\nKubernetes创建Pod时会自动添加所有可用的service环境变量到该Pod中，如有需要．这些环境变量就被注入Pod内的容器里。需要注意的是，环境变量的注入只发送在Pod创建时，且不会被自动更新。这个特点暗含了service和访问该service的Pod的创建时间的先后顺序，即任何想要访问service的pod都需要在service已经存在后创建，否则与service相关的环境变量就无法注入该Pod的容器中，这样先创建的容器就无法发现后创建的service。\n- DNS方式\nKubernetes集群现在支持增加一个可选的组件——DNS服务器。这个DNS服务器使用Kubernetes的watchAPI，不间断的监测新的service的创建并为每个service新建一个DNS记录。如果DNS在整个集群范围内都可用，那么所有的Pod都能够自动解析service的域名。\n\n### 其他\n- 多个service如何避免地址和端口冲突\n此处设计思想是，Kubernetes通过为每个service分配一个唯一的ClusterIP，所以当使用ClusterIP：port的组合访问一个service的时候，不管port是什么，这个组合是一定不会发生重复的。另一方面，kube-proxy为每个service真正打开的是一个绝对不会重复的随机端口，用户在service描述文件中指定的访问端口会被映射到这个随机端口上。这就是为什么用户可以在创建service时随意指定访问端口。\n\n- 目前存在的不足\nKubernetes使用iptables和kube-proxy解析service的入口地址，在中小规模的集群中运行良好，但是当service的数量超过一定规模时，仍然有一些小问题。首当其冲的便是service环境变量泛滥，以及service与使用service的pod两者创建时间先后的制约关系。目前来看，很多使用者在使用Kubernetes时往往会开发一套自己的Router组件来替代service，以便更好地掌控和定制这部分功能。\n\n## Label\nService就是靠Label选择器（Label Selectors）来匹配组内的Pod的，而且很多命令都可以操作Label。Label是绑定在对象上（比如Pod）的键值对，主要用来把一些相关的对象组织在一起，并且对于用户来说label是有含义的，比如：\n- Production environment (production, test, dev)\n- Application version (beta, v1.3)\n- Type of service/server (frontend, backend, database)\n\n>    Labels are key/value pairs that are attached to objects\n\n![Label](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg)\n\n### Pod创建时默认创建的Label\n```\nxis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn\nName:  \t\thelloworld-2790924137-bvfhn\nNamespace:     \tdefault\nNode:  \t\tminikube/192.168.99.100\nStart Time:    \tFri, 05 May 2017 14:03:56 +0800\nLabels:\t\tpod-template-hash=2790924137\n       \t\trun=helloworld\n```\n\n### 新增一个Label\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl label pod helloworld-2790924137-bvfhn app=v1\npod \"helloworld-2790924137-bvfhn\" labeled\nxis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn\nName:  \t\thelloworld-2790924137-bvfhn\nNamespace:     \tdefault\nNode:  \t\tminikube/192.168.99.100\nStart Time:    \tFri, 05 May 2017 14:03:56 +0800\nLabels:\t\tapp=v1\n       \t\tpod-template-hash=2790924137\n       \t\trun=helloworld\n```\n\n### 使用Label的例子\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   46m\nkubernetes   10.0.0.1     <none>        443/TCP          17h\nxis-macbook-pro:~ xiningwang$ kubectl get service  -l run=helloworld\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   46m\nxis-macbook-pro:~ xiningwang$ kubectl get pod -l app=v1\nNAME                          READY     STATUS    RESTARTS   AGE\nhelloworld-2790924137-bvfhn   1/1       Running   0          1h\nxis-macbook-pro:~ xiningwang$ kubectl get pod\nNAME                             READY     STATUS             RESTARTS   AGE\nhello-minikube-938614450-b7xwm   0/1       ImagePullBackOff   0          38m\nhelloworld-2790924137-bvfhn      1/1       Running            0          1h\n```\n## Pet Sets/StatefulSet\nK8s在1.3版本里发布了Alpha版的PetSet功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而PetSet是用来控制有状态服务，PetSet中的每个Pod的名字都是事先确定的，不能更改。PetSet中Pod的名字的作用，是用来关联与该Pod对应的状态。\n\n对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别；对于PetSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂在上原来Pod的存储继续以它的状态提供服务。\n\n适合于PetSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。PetSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用PetSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。\n\n## Volume\n在Docker的设计实现中，容器中的数据是临时的，即当容器被销毁时，其中的数据将会丢失。如果需要持久化数据，需要使用Docker数据卷挂载宿主机上的文件或者目录到容器中。在Kubernetes中，当Pod重建的时候，数据是会丢失的，Kubernetes也是通过数据卷挂载来提供Pod数据的持久化的。Kubernetes数据卷是对Docker数据卷的扩展，Kubernetes数据卷是Pod级别的，可以用来实现Pod中容器的文件共享。目前，Kubernetes支持的数据卷类型如下：\n-  本地数据卷\nEmptyDir、HostPath这两种类型的数据卷，只能最用于本地文件系统。本地数据卷中的数据只会存在于一台机器上，所以当Pod发生迁移的时候，数据便会丢失。该类型Volume的用途是：Pod中容器间的文件共享、共享宿主机的文件系统。\n- 网络数据卷\nKubernetes提供了很多类型的数据卷以集成第三方的存储系统，包括一些非常流行的分布式文件系统，也有在IaaS平台上提供的存储支持，这些存储系统都是分布式的，通过网络共享文件系统，因此我们称这一类数据卷为网络数据卷。\n网络数据卷能够满足数据的持久化需求，Pod通过配置使用网络数据卷，每次Pod创建的时候都会将存储系统的远端文件目录挂载到容器中，数据卷中的数据将被水久保存，即使Pod被删除，只是除去挂载数据卷，数据卷中的数据仍然保存在存储系统中，且当新的Pod被创建的时候，仍是挂载同样的数据卷。网络数据卷包含以下几种：NFS、iSCISI、GlusterFS、RBD（Ceph Block Device）、Flocker、AWS Elastic Block Store、GCE Persistent Disk.\n- 信息数据卷\nKubernetes中有一些数据卷，主要用来给容器传递配置信息，我们称之为信息数据卷，比如Secret（处理敏感配置信息，密码、Token等）、Downward API（通过环境变量的方式告诉容器Pod的信息）、Git Repo（将Git仓库下载到Pod中），都是将Pod的信息以文件形式保存，然后以数据卷方式挂载到容器中，容器通过读取文件获取相应的信息。\n\n## Deployment\nKubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：\n- Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。\n- 事件和状态查看：可以查看Deployment的升级详细进度和状态。\n- 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。\n- 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。\n- 暂停和启动：对于每一次升级，都能够随时暂停和启动。\n- 多种升级方案：Recreate----删除所有已存在的pod,重新创建新的; RollingUpdate----滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。\n\n\n### Scale\n随着流量的增加，我们可能需要增加我们应用的规模来满足用户的需求。Kubernetes的Scale功能就可以实现这个需求。\n>    Scaling is accomplished by changing the number of replicas in a Deployment.\n\n扩大应用的规模时，Kubernetes将会在Nodes上面使用可用的资源来创建新的Pod，并运行新增加的应用，缩小规模时做相反的操作。Kubernetes也支持自动规模化Pod。当然我们也可以将应用的数量变为0，这样就会终止所有部署该应用的Pods。应用数量增加后，Service内的负载均衡就会变得非常有用了.\n\n![scale](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling1.svg)\n\n\n![scale](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling2.svg)\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   1         1         1            1           2h\n```\n可以看到，现在我们只有一个Pod，\n- DESIRED字段表示我们配置的replicas的个数，即实例的个数。\n- CURRENT字段表示目前处于running状态的replicas的个数。\n- UP-TO-DATE字段表示表示和预先配置的期望状态相符的replicas的个数。\n- AVAILABLE字段表示目前实际对用户可用的replicas的个数。\n\n下面我们使用kubectl scale命令将启动4个复制品，语法规则是kubectl scale deployment-type name replicas-number：\n```\nxis-macbook-pro:~ xiningwang$ kubectl scale deployment/helloworld --replicas=4\ndeployment \"helloworld\" scaled\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   4         4         4            4           2h\n\nxis-macbook-pro:~ xiningwang$ kubectl get pod -o wide\nNAME                          READY     STATUS    RESTARTS   AGE       IP           NODE\nhelloworld-2790924137-2kg70   1/1       Running   0          3m        172.17.0.4   minikube\nhelloworld-2790924137-bvfhn   1/1       Running   0          2h        172.17.0.3   minikube\nhelloworld-2790924137-jg15m   1/1       Running   0          3m        172.17.0.5   minikube\nhelloworld-2790924137-tgqr9   1/1       Running   0          3m        172.17.0.2   minikube\n```\n验证一下这个Service是有负载均衡的：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   4         4         4            4           2h\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-tgqr9\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-2kg70\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-tgqr9\n```\n### Rolling Update\n滚动更新（Rolling update）特性的好处就是我们不用停止服务就可以实现应用更新。默认更新的时候是一个Pod一个Pod更新的，所以整个过程服务不会中断。当然你也可以设置一次更新的Pod的百分比。而且更新过程中，Service只会将流量转发到可用的节点上面。更加重要的是，我们可以随时回退到旧版本。\n>Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones.\nIf a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.\n\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates1.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates2.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates3.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates4.svg)\n\n### Set image\n在原来程序的基础上，多输出一个v2作为新版本，使用set image命令指定新版本镜像.\n```\nxis-macbook-pro:~ xiningwang$ kubectl set image deployments/helloworld helloworld=registry.hnaresearch.com/public/hello-world:v2.0\ndeployment \"helloworld\" image updated\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS              RESTARTS   AGE\nhelloworld-2790924137-2kg70   1/1       Running             0          54m\nhelloworld-2790924137-bvfhn   1/1       Running             0          3h\nhelloworld-2790924137-tgqr9   1/1       Running             0          54m\nhelloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m\nhelloworld-2889228138-q68vx   0/1       ContainerCreating   0          9m\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS              RESTARTS   AGE\nhelloworld-2790924137-2kg70   1/1       Terminating         0          54m\nhelloworld-2790924137-bvfhn   1/1       Running             0          3h\nhelloworld-2790924137-tgqr9   0/1       Terminating         0          54m\nhelloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m\nhelloworld-2889228138-bj38m   0/1       Pending             0          9m\nhelloworld-2889228138-dv3ch   1/1       Running             0          9m\nhelloworld-2889228138-q68vx   1/1       Running             0          9m\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS    RESTARTS   AGE\nhelloworld-2889228138-65lmj   1/1       Running   0          10m\nhelloworld-2889228138-bj38m   1/1       Running   0          10m\nhelloworld-2889228138-dv3ch   1/1       Running   0          10m\nhelloworld-2889228138-q68vx   1/1       Running   0          10m\n```\n### Rollout undo\n使用kubectl rollout undo命令回滚到之前的版本：\n```\nxis-macbook-pro:~ xiningwang$ kubectl rollout undo deployment/helloworld\ndeployment \"helloworld\" rolled back\n```\n\n## Autoscaling\n系统能够根据负载的变化对计算资源的分配进行自动的扩增或者收缩，无疑是一个非常吸引人的特征，它能够最大可能地减少费用或者其他代价（如电力损耗）。自动扩展主要分为两种，其一为水平扩展，针对于实例数目的增减；其二为垂直扩展，即单个实例可以使用的资源的增减。Horizontal Pod Autoscaler（HPA）属于前者。\n\nHorizontal Pod Autoscaler的操作对象是Replication Controller、ReplicaSet或Deployment对应的Pod，根据观察到的CPU实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。\n\n### 决策策略\n在HPA Controller检测到CPU的实际使用量之后，会求出当前的CPU使用率（实际使用量与pod 请求量的比率)。然后，HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近．\n\n另外，考虑到自动扩展的决策可能需要一段时间才会生效，甚至在短时间内会引入一些噪声．\n- 例如当pod所需要的CPU负荷过大，从而运行一个新的pod进行分流，在创建的过程中，系统的CPU使用量可能会有一个攀升的过程。所以，在每一次作出决策后的一段时间内，将不再进行扩展决策。对于ScaleUp而言，这个时间段为3分钟，Scaledown为5分钟。\n- 再者HPA Controller允许一定范围内的CPU使用量的不稳定，也就是说，只有当aVg（CurrentPodConsumption／Target低于0.9或者高于1.1时才进行实例调整，这也是出于维护系统稳定性的考虑。\n","source":"_posts/kubernetes.md","raw":"---\ntitle: Kubernetes\ndate: 2017-2-12 20:46:25\ncategories:\n  - 分布式&云计算\n  - Kubernetes\ntags:\n  - 分布式\n  - Kubernetes\n  - container\n  - 容器\n  - PaaS\n---\n\n## 基础架构\nKubernetes将底层的计算资源连接在一起对外体现为一个计算集群，并将资源高度抽象化。部署应用时Kubernetes会以更高效的方式自动的将应用分发到集群内的机器上面，并调度运行。\n\nKubernetes集群包含两种类型的资源：\n- Master节点：协调控制整个集群。Master负责管理整个集群，协调集群内的所有行为。比如调度应用，监控应用的状态等。\n- Nodes节点：运行应用的工作节点。Node节点负责运行应用，一般是一台物理机或者虚机。每个Node节点上面都有一个Kubelet，它是一个代理程序，用来管理该节点以及和Master节点通信。除此以外，Node节点上还会有一些管理容器的工具，比如Docker或者rkt等。生产环境中一个Kubernetes集群至少应该包含三个Nodes节点。\n\n当部署应用的时候，我们通知Master节点启动应用容器。然后Master会调度这些应用将它们运行在Node节点上面。Node节点和Master节点通过Master节点暴露的Kubernetes API通信。当然我们也可以直接通过这些API和集群交互。\n![kubernetes Cluster](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_01_cluster.svg)\n\n### Master\nMaster节点上面主要由四个模块组成：APIServer、scheduler、controller manager、etcd。\n- APIServer\nAPIServer负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。\n- Scheduler\nScheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。\n- Controller manager\n如果说APIServer做的是“前台”的工作的话，那controller manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller manager去保证了。\n- etcd\netcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。\n\n### Node\n每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。\n- kubelet。Kubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上面的所有容器，但是如果容器不是通过Kubernetes创建的，它并不会管理。本质上，它负责使Pod得运行状态与期望的状态一致。\n- kube-proxy。该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。\n- runtime。runtime指的是容器运行环境，目前Kubernetes支持docker和rkt两种容器。\n\n\n## 搭建MiniKube\n环境: MacOS, virtualbox, minikube v0.18.0, kubectl v1.6.0\n\nKubernetes提供了一个轻量级的Minikube应用，利用它我们可以很容器的创建一个只包含一个Node节点的Kubernetes Cluster用于日常的开发测试。\n\n### 安装\nMinikube的安装可以参考: Minikube的Github：https://github.com/kubernetes/minikube\n\n要正常使用，还必须安装kubectl，并且放在PATH里面。kubectl是一个通过Kubernetes API和Kubernetes集群交互的命令行工具。\n\n### 启动\n可以通过minikube查看cluster运行状态,启动或者停止cluster.例如:\n\n```\nminikube start\n```\n\n### ONLY FOR CHINESE\nKubernetes在部署容器应用的时候会先拉一个pause镜像，这个是一个基础容器，主要是负责网络部分的功能的，具体这里不展开讨论。最关键的是Kubernetes里面镜像默认都是从Google的镜像仓库拉的（就跟docker默认从docker hub拉的一样），但是因为GFW的原因，中国用户是访问不了Google的镜像仓库gcr.io的（如果你可以ping通，那恭喜你）。庆幸的是这个镜像被传到了docker hub上面，虽然中国用户访问后者也非常艰难，但通过一些加速器之类的还是可以pull下来的。如果没有VPN等科学上网的工具的话，请先做如下操作：\n\nSee: https://github.com/kubernetes/kubernetes/issues/6888\n\n```\nminikube ssh    # 登录到我们的Kubernetes VM里面去\ndocker pull registry.hnaresearch.com/public/pause-amd64:3.0  \ndocker tag registry.hnaresearch.com/public/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0  \n```\n这样Kubernetes VM就不会从gcr.io拉镜像了，而是会直接使用本地的镜像。\n\n## 部署应用\n\n在Kubernetes Cluster上面部署应用，我们需要先创建一个Kubernetes Deployment。这个Deployment负责创建和更新我们的应用实例。当这个Deployment创建之后，Kubernetes master就会将这个Deployment创建出来的应用实例部署到集群内某个Node节点上。而且自应用实例创建后，Deployment controller还会持续监控应用，直到应用被删除或者部署应用的Node节点不存在。\n>A Deployment is responsible for creating and updating instances of your application.\n\n![](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg)\n\n使用kubectl来创建Deployment，创建的时候需要制定容器镜像以及我们要启动的个数（replicas），当然这些信息后面可以再更新。这里我用Go写了一个简单的Webserver，返回“Hello World”，监听端口是8090.我们就来启动这个应用.\n```\nkubectl run helloworld --image=registry.hnaresearch.com/public/hello-world:v1.0 --port=8090\n```\n执行后master寻找一个合适的node来部署我们的应用实例（我们只有一个node）。我们可以使用kubectl get deployment来查看我们创建的Deployment：\n```\nkubectl get deployment\n```\n默认应用部署好之后是只在Kubernetes Cluster内部可见的，有多种方法可以让我们的应用暴露到外部，这里先介绍一种简单的：我们可以通过kubectl proxy命令在我们的终端和Kubernetes Cluster直接创建一个代理。然后，打开一个新的终端，通过Pod名(Pod后面会有讲到，可以通过kubectl get pod查看Pod名字)就可以访问了：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get pod\nNAME                             READY     STATUS             RESTARTS   AGE\nhello-minikube-938614450-xjl4s   0/1       ImagePullBackOff   0          15h\nhelloworld-2790924137-bvfhn      1/1       Running            0          2m\n\nxis-macbook-pro:~ xiningwang$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/helloworld-2790924137-bvfhn/\nHello world !\nhostname:helloworld-2790924137-bvfhn\n```\n\n## Pod\nPod是Kubernetes中一个非常重要的概念，也是区别于其他编排系统的一个设计. Deployment执行时并不是直接创建了容器实例，而是先在Node上面创建了Pod，然后再在Pod里面创建容器。那Pod到底是什么？Pod是Kubernetes里面抽象出来的一个概念，它是能够被创建、调度和管理的最小单元；每个Pod都有一个独立的IP；一个Pod由若干个容器构成。一个Pod之内的容器共享Pod的所有资源，这些资源主要包括：共享存储（以Volumes的形式）、共享网络、共享端口等。Kubernetes虽然也是一个容器编排系统，但不同于其他系统，它的最小操作单元不是单个容器，而是Pod。这个特性给Kubernetes带来了很多优势，比如最显而易见的是同一个Pod内的容器可以非常方便的互相访问（通过localhost就可以访问）和共享数据。\n\n> A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them.\n\n![pod](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_pods.svg)\n\n>A Node is a group of one ore more pods and includes the kubelet and container engine.  \n>\n>Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.\n\n![node](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_nodes.svg)\n\n## Job\n从程序的运行形态上来区分，我们可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。Replication Controller创建的Pod都是长时运行的服务，而Job创建的Pod都是一次性任务。\n\n在Job的定义中，restartPolicy（重启策略）只能是Never和OnFailure。Job可以控制一次性任务的Pod的完成次数（Job-->spec-->completions）和并发执行数（Job-->spec-->parallelism），当Pod成功执行指定次数后，即认为Job执行完毕。\n\n## Replication Controller\nReplication Controller（RC）是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。\n\n### Replica Set\n新一代副本控制器replica set，可以被认为 是“升级版”的Replication Controller。也就是说。replica set也是用于保证与label selector匹配的pod数量维持在期望状态。区别在于，replica set引入了对基于子集的selector查询条件，而Replication Controller仅支持基于值相等的selecto条件查询。这是目前从用户角度肴，两者唯一的显著差异。 社区引入这一API的初衷是用于取代vl中的Replication Controller，也就是说．当v1版本被废弃时，Replication Controller就完成了它的历史使命，而由replica set来接管其工作。虽然replica set可以被单独使用，但是目前它多被Deployment用于进行pod的创建、更新与删除。\n\n## Service\n### 原理\n[Service](id:service)是Kubernetes里面抽象出来的一层，它定义了由多个Pods组成的逻辑组（logical set），可以对组内的Pod做一些事情：\n- 对外暴露流量\n- 做负载均衡（load balancing）\n- 服务发现（service-discovery）\n\n> A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.\n\n在Kubernetes中，在受到Replication Controller调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。\n\nService同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod-->metadata-->labeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。\n\n需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。\n\n### Service代理服务\n\n![service](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg)\n\n\n使用kubectl get service可以查看目前已有的service，Minikube默认创建了一个kubernetes Service。我们使用expose命令再创建一个Service：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhello-minikube   10.0.0.188   <nodes>       8080:32710/TCP   16h\nkubernetes       10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl expose deployment/helloworld --type=\"NodePort\" --port 8090\nservice \"helloworld\" exposed\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhello-minikube   10.0.0.188   <nodes>       8080:32710/TCP   16h\nhelloworld       10.0.0.249   <nodes>       8090:31240/TCP   2m\nkubernetes       10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl delete service hello-minikube\nservice \"hello-minikube\" deleted\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   2m\nkubernetes   10.0.0.1     <none>        443/TCP          16h\nxis-macbook-pro:~ xiningwang$ kubectl describe service/helloworld\nName:  \t\t\thelloworld\nNamespace:     \t\tdefault\nLabels:\t\t\trun=helloworld\nAnnotations:   \t\t<none>\nSelector:      \t\trun=helloworld\nType:  \t\t\tNodePort\nIP:    \t\t\t10.0.0.249\nPort:  \t\t\t<unset>\t8090/TCP\nNodePort:      \t\t<unset>\t31240/TCP\nEndpoints:     \t\t172.17.0.3:8090\nSession Affinity:      \tNone\nEvents:\t\t\t<none>\nxis-macbook-pro:~ xiningwang$ minikube docker-env\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://192.168.99.100:2376\"\nexport DOCKER_CERT_PATH=\"/Users/xiningwang/.minikube/certs\"\nexport DOCKER_API_VERSION=\"1.23\"\n# Run this command to configure your shell:\n# eval $(minikube docker-env)\nxis-macbook-pro:~ xiningwang$ curl http://192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\n```\n\n### Service内部负载均衡\n当Service的Endpoints包含多个IP的时候，及服务代理存在多个后端，将进行请求的负载均衡。默认的负载均衡策略是轮训或者随机（有kube-proxy的模式决定）。同时，Service上通过设置Service-->spec-->sessionAffinity=ClientIP，来实现基于源IP地址的会话保持。\n\n### 发布Service\nService的虚拟IP是由Kubernetes虚拟出来的内部网络，外部是无法寻址到的。但是有些服务又需要被外部访问到，例如web前段。这时候就需要加一层网络转发，即外网到内网的转发。Kubernetes提供了NodePort、LoadBalancer、Ingress三种方式。\n- NodePort\n在之前的Guestbook示例中，已经延时了NodePort的用法。NodePort的原理是，Kubernetes会在每一个Node上暴露出一个端口：nodePort，外部网络可以通过（任一Node）[NodeIP]:[NodePort]访问到后端的Service。Minikube只支持这种方式。\n- LoadBalancer\n在NodePort基础上，Kubernetes可以请求底层云平台创建一个负载均衡器，将每个Node作为后端，进行服务分发。该模式需要底层云平台（例如GCE）支持。\n- Ingress\n是一种HTTP方式的路由转发机制，由Ingress Controller和HTTP代理服务器组合而成。Ingress Controller实时监控Kubernetes API，实时更新HTTP代理服务器的转发规则。HTTP代理服务器有GCE Load-Balancer、HaProxy、Nginx等开源方案。\n\n### 自发性\nKubernetes中有一个很重要的服务自发现特性。一旦一个service被创建，该service的service IP和service port等信息都可以被注入到pod中供它们使用。Kubernetes主要支持两种service发现 机制：环境变量和DNS。\n- 环境变量方式\nKubernetes创建Pod时会自动添加所有可用的service环境变量到该Pod中，如有需要．这些环境变量就被注入Pod内的容器里。需要注意的是，环境变量的注入只发送在Pod创建时，且不会被自动更新。这个特点暗含了service和访问该service的Pod的创建时间的先后顺序，即任何想要访问service的pod都需要在service已经存在后创建，否则与service相关的环境变量就无法注入该Pod的容器中，这样先创建的容器就无法发现后创建的service。\n- DNS方式\nKubernetes集群现在支持增加一个可选的组件——DNS服务器。这个DNS服务器使用Kubernetes的watchAPI，不间断的监测新的service的创建并为每个service新建一个DNS记录。如果DNS在整个集群范围内都可用，那么所有的Pod都能够自动解析service的域名。\n\n### 其他\n- 多个service如何避免地址和端口冲突\n此处设计思想是，Kubernetes通过为每个service分配一个唯一的ClusterIP，所以当使用ClusterIP：port的组合访问一个service的时候，不管port是什么，这个组合是一定不会发生重复的。另一方面，kube-proxy为每个service真正打开的是一个绝对不会重复的随机端口，用户在service描述文件中指定的访问端口会被映射到这个随机端口上。这就是为什么用户可以在创建service时随意指定访问端口。\n\n- 目前存在的不足\nKubernetes使用iptables和kube-proxy解析service的入口地址，在中小规模的集群中运行良好，但是当service的数量超过一定规模时，仍然有一些小问题。首当其冲的便是service环境变量泛滥，以及service与使用service的pod两者创建时间先后的制约关系。目前来看，很多使用者在使用Kubernetes时往往会开发一套自己的Router组件来替代service，以便更好地掌控和定制这部分功能。\n\n## Label\nService就是靠Label选择器（Label Selectors）来匹配组内的Pod的，而且很多命令都可以操作Label。Label是绑定在对象上（比如Pod）的键值对，主要用来把一些相关的对象组织在一起，并且对于用户来说label是有含义的，比如：\n- Production environment (production, test, dev)\n- Application version (beta, v1.3)\n- Type of service/server (frontend, backend, database)\n\n>    Labels are key/value pairs that are attached to objects\n\n![Label](http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg)\n\n### Pod创建时默认创建的Label\n```\nxis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn\nName:  \t\thelloworld-2790924137-bvfhn\nNamespace:     \tdefault\nNode:  \t\tminikube/192.168.99.100\nStart Time:    \tFri, 05 May 2017 14:03:56 +0800\nLabels:\t\tpod-template-hash=2790924137\n       \t\trun=helloworld\n```\n\n### 新增一个Label\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl label pod helloworld-2790924137-bvfhn app=v1\npod \"helloworld-2790924137-bvfhn\" labeled\nxis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn\nName:  \t\thelloworld-2790924137-bvfhn\nNamespace:     \tdefault\nNode:  \t\tminikube/192.168.99.100\nStart Time:    \tFri, 05 May 2017 14:03:56 +0800\nLabels:\t\tapp=v1\n       \t\tpod-template-hash=2790924137\n       \t\trun=helloworld\n```\n\n### 使用Label的例子\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl get service\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   46m\nkubernetes   10.0.0.1     <none>        443/TCP          17h\nxis-macbook-pro:~ xiningwang$ kubectl get service  -l run=helloworld\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE\nhelloworld   10.0.0.249   <nodes>       8090:31240/TCP   46m\nxis-macbook-pro:~ xiningwang$ kubectl get pod -l app=v1\nNAME                          READY     STATUS    RESTARTS   AGE\nhelloworld-2790924137-bvfhn   1/1       Running   0          1h\nxis-macbook-pro:~ xiningwang$ kubectl get pod\nNAME                             READY     STATUS             RESTARTS   AGE\nhello-minikube-938614450-b7xwm   0/1       ImagePullBackOff   0          38m\nhelloworld-2790924137-bvfhn      1/1       Running            0          1h\n```\n## Pet Sets/StatefulSet\nK8s在1.3版本里发布了Alpha版的PetSet功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而PetSet是用来控制有状态服务，PetSet中的每个Pod的名字都是事先确定的，不能更改。PetSet中Pod的名字的作用，是用来关联与该Pod对应的状态。\n\n对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别；对于PetSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂在上原来Pod的存储继续以它的状态提供服务。\n\n适合于PetSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。PetSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用PetSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。\n\n## Volume\n在Docker的设计实现中，容器中的数据是临时的，即当容器被销毁时，其中的数据将会丢失。如果需要持久化数据，需要使用Docker数据卷挂载宿主机上的文件或者目录到容器中。在Kubernetes中，当Pod重建的时候，数据是会丢失的，Kubernetes也是通过数据卷挂载来提供Pod数据的持久化的。Kubernetes数据卷是对Docker数据卷的扩展，Kubernetes数据卷是Pod级别的，可以用来实现Pod中容器的文件共享。目前，Kubernetes支持的数据卷类型如下：\n-  本地数据卷\nEmptyDir、HostPath这两种类型的数据卷，只能最用于本地文件系统。本地数据卷中的数据只会存在于一台机器上，所以当Pod发生迁移的时候，数据便会丢失。该类型Volume的用途是：Pod中容器间的文件共享、共享宿主机的文件系统。\n- 网络数据卷\nKubernetes提供了很多类型的数据卷以集成第三方的存储系统，包括一些非常流行的分布式文件系统，也有在IaaS平台上提供的存储支持，这些存储系统都是分布式的，通过网络共享文件系统，因此我们称这一类数据卷为网络数据卷。\n网络数据卷能够满足数据的持久化需求，Pod通过配置使用网络数据卷，每次Pod创建的时候都会将存储系统的远端文件目录挂载到容器中，数据卷中的数据将被水久保存，即使Pod被删除，只是除去挂载数据卷，数据卷中的数据仍然保存在存储系统中，且当新的Pod被创建的时候，仍是挂载同样的数据卷。网络数据卷包含以下几种：NFS、iSCISI、GlusterFS、RBD（Ceph Block Device）、Flocker、AWS Elastic Block Store、GCE Persistent Disk.\n- 信息数据卷\nKubernetes中有一些数据卷，主要用来给容器传递配置信息，我们称之为信息数据卷，比如Secret（处理敏感配置信息，密码、Token等）、Downward API（通过环境变量的方式告诉容器Pod的信息）、Git Repo（将Git仓库下载到Pod中），都是将Pod的信息以文件形式保存，然后以数据卷方式挂载到容器中，容器通过读取文件获取相应的信息。\n\n## Deployment\nKubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：\n- Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。\n- 事件和状态查看：可以查看Deployment的升级详细进度和状态。\n- 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。\n- 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。\n- 暂停和启动：对于每一次升级，都能够随时暂停和启动。\n- 多种升级方案：Recreate----删除所有已存在的pod,重新创建新的; RollingUpdate----滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。\n\n\n### Scale\n随着流量的增加，我们可能需要增加我们应用的规模来满足用户的需求。Kubernetes的Scale功能就可以实现这个需求。\n>    Scaling is accomplished by changing the number of replicas in a Deployment.\n\n扩大应用的规模时，Kubernetes将会在Nodes上面使用可用的资源来创建新的Pod，并运行新增加的应用，缩小规模时做相反的操作。Kubernetes也支持自动规模化Pod。当然我们也可以将应用的数量变为0，这样就会终止所有部署该应用的Pods。应用数量增加后，Service内的负载均衡就会变得非常有用了.\n\n![scale](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling1.svg)\n\n\n![scale](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling2.svg)\n\n```\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   1         1         1            1           2h\n```\n可以看到，现在我们只有一个Pod，\n- DESIRED字段表示我们配置的replicas的个数，即实例的个数。\n- CURRENT字段表示目前处于running状态的replicas的个数。\n- UP-TO-DATE字段表示表示和预先配置的期望状态相符的replicas的个数。\n- AVAILABLE字段表示目前实际对用户可用的replicas的个数。\n\n下面我们使用kubectl scale命令将启动4个复制品，语法规则是kubectl scale deployment-type name replicas-number：\n```\nxis-macbook-pro:~ xiningwang$ kubectl scale deployment/helloworld --replicas=4\ndeployment \"helloworld\" scaled\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   4         4         4            4           2h\n\nxis-macbook-pro:~ xiningwang$ kubectl get pod -o wide\nNAME                          READY     STATUS    RESTARTS   AGE       IP           NODE\nhelloworld-2790924137-2kg70   1/1       Running   0          3m        172.17.0.4   minikube\nhelloworld-2790924137-bvfhn   1/1       Running   0          2h        172.17.0.3   minikube\nhelloworld-2790924137-jg15m   1/1       Running   0          3m        172.17.0.5   minikube\nhelloworld-2790924137-tgqr9   1/1       Running   0          3m        172.17.0.2   minikube\n```\n验证一下这个Service是有负载均衡的：\n```\nxis-macbook-pro:~ xiningwang$ kubectl get deployment\nNAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nhelloworld   4         4         4            4           2h\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-tgqr9\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-2kg70\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-bvfhn\nxis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240\nHello world !\nhostname:helloworld-2790924137-tgqr9\n```\n### Rolling Update\n滚动更新（Rolling update）特性的好处就是我们不用停止服务就可以实现应用更新。默认更新的时候是一个Pod一个Pod更新的，所以整个过程服务不会中断。当然你也可以设置一次更新的Pod的百分比。而且更新过程中，Service只会将流量转发到可用的节点上面。更加重要的是，我们可以随时回退到旧版本。\n>Rolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones.\nIf a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.\n\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates1.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates2.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates3.svg)\n![rollingupdate](https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates4.svg)\n\n### Set image\n在原来程序的基础上，多输出一个v2作为新版本，使用set image命令指定新版本镜像.\n```\nxis-macbook-pro:~ xiningwang$ kubectl set image deployments/helloworld helloworld=registry.hnaresearch.com/public/hello-world:v2.0\ndeployment \"helloworld\" image updated\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS              RESTARTS   AGE\nhelloworld-2790924137-2kg70   1/1       Running             0          54m\nhelloworld-2790924137-bvfhn   1/1       Running             0          3h\nhelloworld-2790924137-tgqr9   1/1       Running             0          54m\nhelloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m\nhelloworld-2889228138-q68vx   0/1       ContainerCreating   0          9m\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS              RESTARTS   AGE\nhelloworld-2790924137-2kg70   1/1       Terminating         0          54m\nhelloworld-2790924137-bvfhn   1/1       Running             0          3h\nhelloworld-2790924137-tgqr9   0/1       Terminating         0          54m\nhelloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m\nhelloworld-2889228138-bj38m   0/1       Pending             0          9m\nhelloworld-2889228138-dv3ch   1/1       Running             0          9m\nhelloworld-2889228138-q68vx   1/1       Running             0          9m\nxis-macbook-pro:~ xiningwang$ kubectl get pods\nNAME                          READY     STATUS    RESTARTS   AGE\nhelloworld-2889228138-65lmj   1/1       Running   0          10m\nhelloworld-2889228138-bj38m   1/1       Running   0          10m\nhelloworld-2889228138-dv3ch   1/1       Running   0          10m\nhelloworld-2889228138-q68vx   1/1       Running   0          10m\n```\n### Rollout undo\n使用kubectl rollout undo命令回滚到之前的版本：\n```\nxis-macbook-pro:~ xiningwang$ kubectl rollout undo deployment/helloworld\ndeployment \"helloworld\" rolled back\n```\n\n## Autoscaling\n系统能够根据负载的变化对计算资源的分配进行自动的扩增或者收缩，无疑是一个非常吸引人的特征，它能够最大可能地减少费用或者其他代价（如电力损耗）。自动扩展主要分为两种，其一为水平扩展，针对于实例数目的增减；其二为垂直扩展，即单个实例可以使用的资源的增减。Horizontal Pod Autoscaler（HPA）属于前者。\n\nHorizontal Pod Autoscaler的操作对象是Replication Controller、ReplicaSet或Deployment对应的Pod，根据观察到的CPU实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。\n\n### 决策策略\n在HPA Controller检测到CPU的实际使用量之后，会求出当前的CPU使用率（实际使用量与pod 请求量的比率)。然后，HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近．\n\n另外，考虑到自动扩展的决策可能需要一段时间才会生效，甚至在短时间内会引入一些噪声．\n- 例如当pod所需要的CPU负荷过大，从而运行一个新的pod进行分流，在创建的过程中，系统的CPU使用量可能会有一个攀升的过程。所以，在每一次作出决策后的一段时间内，将不再进行扩展决策。对于ScaleUp而言，这个时间段为3分钟，Scaledown为5分钟。\n- 再者HPA Controller允许一定范围内的CPU使用量的不稳定，也就是说，只有当aVg（CurrentPodConsumption／Target低于0.9或者高于1.1时才进行实例调整，这也是出于维护系统稳定性的考虑。\n","slug":"kubernetes","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bg3000ki2724vyik1af","content":"<h2 id=\"基础架构\"><a href=\"#基础架构\" class=\"headerlink\" title=\"基础架构\"></a>基础架构</h2><p>Kubernetes将底层的计算资源连接在一起对外体现为一个计算集群，并将资源高度抽象化。部署应用时Kubernetes会以更高效的方式自动的将应用分发到集群内的机器上面，并调度运行。</p>\n<p>Kubernetes集群包含两种类型的资源：</p>\n<ul>\n<li>Master节点：协调控制整个集群。Master负责管理整个集群，协调集群内的所有行为。比如调度应用，监控应用的状态等。</li>\n<li>Nodes节点：运行应用的工作节点。Node节点负责运行应用，一般是一台物理机或者虚机。每个Node节点上面都有一个Kubelet，它是一个代理程序，用来管理该节点以及和Master节点通信。除此以外，Node节点上还会有一些管理容器的工具，比如Docker或者rkt等。生产环境中一个Kubernetes集群至少应该包含三个Nodes节点。</li>\n</ul>\n<p>当部署应用的时候，我们通知Master节点启动应用容器。然后Master会调度这些应用将它们运行在Node节点上面。Node节点和Master节点通过Master节点暴露的Kubernetes API通信。当然我们也可以直接通过这些API和集群交互。<br><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_01_cluster.svg\" alt=\"kubernetes Cluster\"></p>\n<h3 id=\"Master\"><a href=\"#Master\" class=\"headerlink\" title=\"Master\"></a>Master</h3><p>Master节点上面主要由四个模块组成：APIServer、scheduler、controller manager、etcd。</p>\n<ul>\n<li>APIServer<br>APIServer负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。</li>\n<li>Scheduler<br>Scheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。</li>\n<li>Controller manager<br>如果说APIServer做的是“前台”的工作的话，那controller manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller manager去保证了。</li>\n<li>etcd<br>etcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。</li>\n</ul>\n<h3 id=\"Node\"><a href=\"#Node\" class=\"headerlink\" title=\"Node\"></a>Node</h3><p>每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。</p>\n<ul>\n<li>kubelet。Kubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上面的所有容器，但是如果容器不是通过Kubernetes创建的，它并不会管理。本质上，它负责使Pod得运行状态与期望的状态一致。</li>\n<li>kube-proxy。该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。</li>\n<li>runtime。runtime指的是容器运行环境，目前Kubernetes支持docker和rkt两种容器。</li>\n</ul>\n<h2 id=\"搭建MiniKube\"><a href=\"#搭建MiniKube\" class=\"headerlink\" title=\"搭建MiniKube\"></a>搭建MiniKube</h2><p>环境: MacOS, virtualbox, minikube v0.18.0, kubectl v1.6.0</p>\n<p>Kubernetes提供了一个轻量级的Minikube应用，利用它我们可以很容器的创建一个只包含一个Node节点的Kubernetes Cluster用于日常的开发测试。</p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>Minikube的安装可以参考: Minikube的Github：<a href=\"https://github.com/kubernetes/minikube\" target=\"_blank\" rel=\"external\">https://github.com/kubernetes/minikube</a></p>\n<p>要正常使用，还必须安装kubectl，并且放在PATH里面。kubectl是一个通过Kubernetes API和Kubernetes集群交互的命令行工具。</p>\n<h3 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h3><p>可以通过minikube查看cluster运行状态,启动或者停止cluster.例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">minikube start</div></pre></td></tr></table></figure>\n<h3 id=\"ONLY-FOR-CHINESE\"><a href=\"#ONLY-FOR-CHINESE\" class=\"headerlink\" title=\"ONLY FOR CHINESE\"></a>ONLY FOR CHINESE</h3><p>Kubernetes在部署容器应用的时候会先拉一个pause镜像，这个是一个基础容器，主要是负责网络部分的功能的，具体这里不展开讨论。最关键的是Kubernetes里面镜像默认都是从Google的镜像仓库拉的（就跟docker默认从docker hub拉的一样），但是因为GFW的原因，中国用户是访问不了Google的镜像仓库gcr.io的（如果你可以ping通，那恭喜你）。庆幸的是这个镜像被传到了docker hub上面，虽然中国用户访问后者也非常艰难，但通过一些加速器之类的还是可以pull下来的。如果没有VPN等科学上网的工具的话，请先做如下操作：</p>\n<p>See: <a href=\"https://github.com/kubernetes/kubernetes/issues/6888\" target=\"_blank\" rel=\"external\">https://github.com/kubernetes/kubernetes/issues/6888</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">minikube ssh    # 登录到我们的Kubernetes VM里面去</div><div class=\"line\">docker pull registry.hnaresearch.com/public/pause-amd64:3.0  </div><div class=\"line\">docker tag registry.hnaresearch.com/public/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0</div></pre></td></tr></table></figure>\n<p>这样Kubernetes VM就不会从gcr.io拉镜像了，而是会直接使用本地的镜像。</p>\n<h2 id=\"部署应用\"><a href=\"#部署应用\" class=\"headerlink\" title=\"部署应用\"></a>部署应用</h2><p>在Kubernetes Cluster上面部署应用，我们需要先创建一个Kubernetes Deployment。这个Deployment负责创建和更新我们的应用实例。当这个Deployment创建之后，Kubernetes master就会将这个Deployment创建出来的应用实例部署到集群内某个Node节点上。而且自应用实例创建后，Deployment controller还会持续监控应用，直到应用被删除或者部署应用的Node节点不存在。</p>\n<blockquote>\n<p>A Deployment is responsible for creating and updating instances of your application.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg\" alt=\"\"></p>\n<p>使用kubectl来创建Deployment，创建的时候需要制定容器镜像以及我们要启动的个数（replicas），当然这些信息后面可以再更新。这里我用Go写了一个简单的Webserver，返回“Hello World”，监听端口是8090.我们就来启动这个应用.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl run helloworld --image=registry.hnaresearch.com/public/hello-world:v1.0 --port=8090</div></pre></td></tr></table></figure></p>\n<p>执行后master寻找一个合适的node来部署我们的应用实例（我们只有一个node）。我们可以使用kubectl get deployment来查看我们创建的Deployment：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl get deployment</div></pre></td></tr></table></figure></p>\n<p>默认应用部署好之后是只在Kubernetes Cluster内部可见的，有多种方法可以让我们的应用暴露到外部，这里先介绍一种简单的：我们可以通过kubectl proxy命令在我们的终端和Kubernetes Cluster直接创建一个代理。然后，打开一个新的终端，通过Pod名(Pod后面会有讲到，可以通过kubectl get pod查看Pod名字)就可以访问了：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod</div><div class=\"line\">NAME                             READY     STATUS             RESTARTS   AGE</div><div class=\"line\">hello-minikube-938614450-xjl4s   0/1       ImagePullBackOff   0          15h</div><div class=\"line\">helloworld-2790924137-bvfhn      1/1       Running            0          2m</div><div class=\"line\"></div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/helloworld-2790924137-bvfhn/</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div></pre></td></tr></table></figure></p>\n<h2 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h2><p>Pod是Kubernetes中一个非常重要的概念，也是区别于其他编排系统的一个设计. Deployment执行时并不是直接创建了容器实例，而是先在Node上面创建了Pod，然后再在Pod里面创建容器。那Pod到底是什么？Pod是Kubernetes里面抽象出来的一个概念，它是能够被创建、调度和管理的最小单元；每个Pod都有一个独立的IP；一个Pod由若干个容器构成。一个Pod之内的容器共享Pod的所有资源，这些资源主要包括：共享存储（以Volumes的形式）、共享网络、共享端口等。Kubernetes虽然也是一个容器编排系统，但不同于其他系统，它的最小操作单元不是单个容器，而是Pod。这个特性给Kubernetes带来了很多优势，比如最显而易见的是同一个Pod内的容器可以非常方便的互相访问（通过localhost就可以访问）和共享数据。</p>\n<blockquote>\n<p>A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_pods.svg\" alt=\"pod\"></p>\n<blockquote>\n<p>A Node is a group of one ore more pods and includes the kubelet and container engine.  </p>\n<p>Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_nodes.svg\" alt=\"node\"></p>\n<h2 id=\"Job\"><a href=\"#Job\" class=\"headerlink\" title=\"Job\"></a>Job</h2><p>从程序的运行形态上来区分，我们可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。Replication Controller创建的Pod都是长时运行的服务，而Job创建的Pod都是一次性任务。</p>\n<p>在Job的定义中，restartPolicy（重启策略）只能是Never和OnFailure。Job可以控制一次性任务的Pod的完成次数（Job–&gt;spec–&gt;completions）和并发执行数（Job–&gt;spec–&gt;parallelism），当Pod成功执行指定次数后，即认为Job执行完毕。</p>\n<h2 id=\"Replication-Controller\"><a href=\"#Replication-Controller\" class=\"headerlink\" title=\"Replication Controller\"></a>Replication Controller</h2><p>Replication Controller（RC）是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。</p>\n<h3 id=\"Replica-Set\"><a href=\"#Replica-Set\" class=\"headerlink\" title=\"Replica Set\"></a>Replica Set</h3><p>新一代副本控制器replica set，可以被认为 是“升级版”的Replication Controller。也就是说。replica set也是用于保证与label selector匹配的pod数量维持在期望状态。区别在于，replica set引入了对基于子集的selector查询条件，而Replication Controller仅支持基于值相等的selecto条件查询。这是目前从用户角度肴，两者唯一的显著差异。 社区引入这一API的初衷是用于取代vl中的Replication Controller，也就是说．当v1版本被废弃时，Replication Controller就完成了它的历史使命，而由replica set来接管其工作。虽然replica set可以被单独使用，但是目前它多被Deployment用于进行pod的创建、更新与删除。</p>\n<h2 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h2><h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p><a href=\"id:service\" target=\"_blank\" rel=\"external\">Service</a>是Kubernetes里面抽象出来的一层，它定义了由多个Pods组成的逻辑组（logical set），可以对组内的Pod做一些事情：</p>\n<ul>\n<li>对外暴露流量</li>\n<li>做负载均衡（load balancing）</li>\n<li>服务发现（service-discovery）</li>\n</ul>\n<blockquote>\n<p>A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.</p>\n</blockquote>\n<p>在Kubernetes中，在受到Replication Controller调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。</p>\n<p>Service同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod–&gt;metadata–&gt;labeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。</p>\n<p>需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。</p>\n<h3 id=\"Service代理服务\"><a href=\"#Service代理服务\" class=\"headerlink\" title=\"Service代理服务\"></a>Service代理服务</h3><p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg\" alt=\"service\"></p>\n<p>使用kubectl get service可以查看目前已有的service，Minikube默认创建了一个kubernetes Service。我们使用expose命令再创建一个Service：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">hello-minikube   10.0.0.188   &lt;nodes&gt;       8080:32710/TCP   16h</div><div class=\"line\">kubernetes       10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl expose deployment/helloworld --type=&quot;NodePort&quot; --port 8090</div><div class=\"line\">service &quot;helloworld&quot; exposed</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">hello-minikube   10.0.0.188   &lt;nodes&gt;       8080:32710/TCP   16h</div><div class=\"line\">helloworld       10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   2m</div><div class=\"line\">kubernetes       10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl delete service hello-minikube</div><div class=\"line\">service &quot;hello-minikube&quot; deleted</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   2m</div><div class=\"line\">kubernetes   10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe service/helloworld</div><div class=\"line\">Name:  \t\t\thelloworld</div><div class=\"line\">Namespace:     \t\tdefault</div><div class=\"line\">Labels:\t\t\trun=helloworld</div><div class=\"line\">Annotations:   \t\t&lt;none&gt;</div><div class=\"line\">Selector:      \t\trun=helloworld</div><div class=\"line\">Type:  \t\t\tNodePort</div><div class=\"line\">IP:    \t\t\t10.0.0.249</div><div class=\"line\">Port:  \t\t\t&lt;unset&gt;\t8090/TCP</div><div class=\"line\">NodePort:      \t\t&lt;unset&gt;\t31240/TCP</div><div class=\"line\">Endpoints:     \t\t172.17.0.3:8090</div><div class=\"line\">Session Affinity:      \tNone</div><div class=\"line\">Events:\t\t\t&lt;none&gt;</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ minikube docker-env</div><div class=\"line\">export DOCKER_TLS_VERIFY=&quot;1&quot;</div><div class=\"line\">export DOCKER_HOST=&quot;tcp://192.168.99.100:2376&quot;</div><div class=\"line\">export DOCKER_CERT_PATH=&quot;/Users/xiningwang/.minikube/certs&quot;</div><div class=\"line\">export DOCKER_API_VERSION=&quot;1.23&quot;</div><div class=\"line\"># Run this command to configure your shell:</div><div class=\"line\"># eval $(minikube docker-env)</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl http://192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div></pre></td></tr></table></figure></p>\n<h3 id=\"Service内部负载均衡\"><a href=\"#Service内部负载均衡\" class=\"headerlink\" title=\"Service内部负载均衡\"></a>Service内部负载均衡</h3><p>当Service的Endpoints包含多个IP的时候，及服务代理存在多个后端，将进行请求的负载均衡。默认的负载均衡策略是轮训或者随机（有kube-proxy的模式决定）。同时，Service上通过设置Service–&gt;spec–&gt;sessionAffinity=ClientIP，来实现基于源IP地址的会话保持。</p>\n<h3 id=\"发布Service\"><a href=\"#发布Service\" class=\"headerlink\" title=\"发布Service\"></a>发布Service</h3><p>Service的虚拟IP是由Kubernetes虚拟出来的内部网络，外部是无法寻址到的。但是有些服务又需要被外部访问到，例如web前段。这时候就需要加一层网络转发，即外网到内网的转发。Kubernetes提供了NodePort、LoadBalancer、Ingress三种方式。</p>\n<ul>\n<li>NodePort<br>在之前的Guestbook示例中，已经延时了NodePort的用法。NodePort的原理是，Kubernetes会在每一个Node上暴露出一个端口：nodePort，外部网络可以通过（任一Node）[NodeIP]:[NodePort]访问到后端的Service。Minikube只支持这种方式。</li>\n<li>LoadBalancer<br>在NodePort基础上，Kubernetes可以请求底层云平台创建一个负载均衡器，将每个Node作为后端，进行服务分发。该模式需要底层云平台（例如GCE）支持。</li>\n<li>Ingress<br>是一种HTTP方式的路由转发机制，由Ingress Controller和HTTP代理服务器组合而成。Ingress Controller实时监控Kubernetes API，实时更新HTTP代理服务器的转发规则。HTTP代理服务器有GCE Load-Balancer、HaProxy、Nginx等开源方案。</li>\n</ul>\n<h3 id=\"自发性\"><a href=\"#自发性\" class=\"headerlink\" title=\"自发性\"></a>自发性</h3><p>Kubernetes中有一个很重要的服务自发现特性。一旦一个service被创建，该service的service IP和service port等信息都可以被注入到pod中供它们使用。Kubernetes主要支持两种service发现 机制：环境变量和DNS。</p>\n<ul>\n<li>环境变量方式<br>Kubernetes创建Pod时会自动添加所有可用的service环境变量到该Pod中，如有需要．这些环境变量就被注入Pod内的容器里。需要注意的是，环境变量的注入只发送在Pod创建时，且不会被自动更新。这个特点暗含了service和访问该service的Pod的创建时间的先后顺序，即任何想要访问service的pod都需要在service已经存在后创建，否则与service相关的环境变量就无法注入该Pod的容器中，这样先创建的容器就无法发现后创建的service。</li>\n<li>DNS方式<br>Kubernetes集群现在支持增加一个可选的组件——DNS服务器。这个DNS服务器使用Kubernetes的watchAPI，不间断的监测新的service的创建并为每个service新建一个DNS记录。如果DNS在整个集群范围内都可用，那么所有的Pod都能够自动解析service的域名。</li>\n</ul>\n<h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><ul>\n<li><p>多个service如何避免地址和端口冲突<br>此处设计思想是，Kubernetes通过为每个service分配一个唯一的ClusterIP，所以当使用ClusterIP：port的组合访问一个service的时候，不管port是什么，这个组合是一定不会发生重复的。另一方面，kube-proxy为每个service真正打开的是一个绝对不会重复的随机端口，用户在service描述文件中指定的访问端口会被映射到这个随机端口上。这就是为什么用户可以在创建service时随意指定访问端口。</p>\n</li>\n<li><p>目前存在的不足<br>Kubernetes使用iptables和kube-proxy解析service的入口地址，在中小规模的集群中运行良好，但是当service的数量超过一定规模时，仍然有一些小问题。首当其冲的便是service环境变量泛滥，以及service与使用service的pod两者创建时间先后的制约关系。目前来看，很多使用者在使用Kubernetes时往往会开发一套自己的Router组件来替代service，以便更好地掌控和定制这部分功能。</p>\n</li>\n</ul>\n<h2 id=\"Label\"><a href=\"#Label\" class=\"headerlink\" title=\"Label\"></a>Label</h2><p>Service就是靠Label选择器（Label Selectors）来匹配组内的Pod的，而且很多命令都可以操作Label。Label是绑定在对象上（比如Pod）的键值对，主要用来把一些相关的对象组织在一起，并且对于用户来说label是有含义的，比如：</p>\n<ul>\n<li>Production environment (production, test, dev)</li>\n<li>Application version (beta, v1.3)</li>\n<li>Type of service/server (frontend, backend, database)</li>\n</ul>\n<blockquote>\n<p>   Labels are key/value pairs that are attached to objects</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg\" alt=\"Label\"></p>\n<h3 id=\"Pod创建时默认创建的Label\"><a href=\"#Pod创建时默认创建的Label\" class=\"headerlink\" title=\"Pod创建时默认创建的Label\"></a>Pod创建时默认创建的Label</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn</div><div class=\"line\">Name:  \t\thelloworld-2790924137-bvfhn</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Node:  \t\tminikube/192.168.99.100</div><div class=\"line\">Start Time:    \tFri, 05 May 2017 14:03:56 +0800</div><div class=\"line\">Labels:\t\tpod-template-hash=2790924137</div><div class=\"line\">       \t\trun=helloworld</div></pre></td></tr></table></figure>\n<h3 id=\"新增一个Label\"><a href=\"#新增一个Label\" class=\"headerlink\" title=\"新增一个Label\"></a>新增一个Label</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl label pod helloworld-2790924137-bvfhn app=v1</div><div class=\"line\">pod &quot;helloworld-2790924137-bvfhn&quot; labeled</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn</div><div class=\"line\">Name:  \t\thelloworld-2790924137-bvfhn</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Node:  \t\tminikube/192.168.99.100</div><div class=\"line\">Start Time:    \tFri, 05 May 2017 14:03:56 +0800</div><div class=\"line\">Labels:\t\tapp=v1</div><div class=\"line\">       \t\tpod-template-hash=2790924137</div><div class=\"line\">       \t\trun=helloworld</div></pre></td></tr></table></figure>\n<h3 id=\"使用Label的例子\"><a href=\"#使用Label的例子\" class=\"headerlink\" title=\"使用Label的例子\"></a>使用Label的例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   46m</div><div class=\"line\">kubernetes   10.0.0.1     &lt;none&gt;        443/TCP          17h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service  -l run=helloworld</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   46m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod -l app=v1</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running   0          1h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod</div><div class=\"line\">NAME                             READY     STATUS             RESTARTS   AGE</div><div class=\"line\">hello-minikube-938614450-b7xwm   0/1       ImagePullBackOff   0          38m</div><div class=\"line\">helloworld-2790924137-bvfhn      1/1       Running            0          1h</div></pre></td></tr></table></figure>\n<h2 id=\"Pet-Sets-StatefulSet\"><a href=\"#Pet-Sets-StatefulSet\" class=\"headerlink\" title=\"Pet Sets/StatefulSet\"></a>Pet Sets/StatefulSet</h2><p>K8s在1.3版本里发布了Alpha版的PetSet功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而PetSet是用来控制有状态服务，PetSet中的每个Pod的名字都是事先确定的，不能更改。PetSet中Pod的名字的作用，是用来关联与该Pod对应的状态。</p>\n<p>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别；对于PetSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂在上原来Pod的存储继续以它的状态提供服务。</p>\n<p>适合于PetSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。PetSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用PetSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。</p>\n<h2 id=\"Volume\"><a href=\"#Volume\" class=\"headerlink\" title=\"Volume\"></a>Volume</h2><p>在Docker的设计实现中，容器中的数据是临时的，即当容器被销毁时，其中的数据将会丢失。如果需要持久化数据，需要使用Docker数据卷挂载宿主机上的文件或者目录到容器中。在Kubernetes中，当Pod重建的时候，数据是会丢失的，Kubernetes也是通过数据卷挂载来提供Pod数据的持久化的。Kubernetes数据卷是对Docker数据卷的扩展，Kubernetes数据卷是Pod级别的，可以用来实现Pod中容器的文件共享。目前，Kubernetes支持的数据卷类型如下：</p>\n<ul>\n<li>本地数据卷<br>EmptyDir、HostPath这两种类型的数据卷，只能最用于本地文件系统。本地数据卷中的数据只会存在于一台机器上，所以当Pod发生迁移的时候，数据便会丢失。该类型Volume的用途是：Pod中容器间的文件共享、共享宿主机的文件系统。</li>\n<li>网络数据卷<br>Kubernetes提供了很多类型的数据卷以集成第三方的存储系统，包括一些非常流行的分布式文件系统，也有在IaaS平台上提供的存储支持，这些存储系统都是分布式的，通过网络共享文件系统，因此我们称这一类数据卷为网络数据卷。<br>网络数据卷能够满足数据的持久化需求，Pod通过配置使用网络数据卷，每次Pod创建的时候都会将存储系统的远端文件目录挂载到容器中，数据卷中的数据将被水久保存，即使Pod被删除，只是除去挂载数据卷，数据卷中的数据仍然保存在存储系统中，且当新的Pod被创建的时候，仍是挂载同样的数据卷。网络数据卷包含以下几种：NFS、iSCISI、GlusterFS、RBD（Ceph Block Device）、Flocker、AWS Elastic Block Store、GCE Persistent Disk.</li>\n<li>信息数据卷<br>Kubernetes中有一些数据卷，主要用来给容器传递配置信息，我们称之为信息数据卷，比如Secret（处理敏感配置信息，密码、Token等）、Downward API（通过环境变量的方式告诉容器Pod的信息）、Git Repo（将Git仓库下载到Pod中），都是将Pod的信息以文件形式保存，然后以数据卷方式挂载到容器中，容器通过读取文件获取相应的信息。</li>\n</ul>\n<h2 id=\"Deployment\"><a href=\"#Deployment\" class=\"headerlink\" title=\"Deployment\"></a>Deployment</h2><p>Kubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：</p>\n<ul>\n<li>Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。</li>\n<li>事件和状态查看：可以查看Deployment的升级详细进度和状态。</li>\n<li>回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。</li>\n<li>版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。</li>\n<li>暂停和启动：对于每一次升级，都能够随时暂停和启动。</li>\n<li>多种升级方案：Recreate—-删除所有已存在的pod,重新创建新的; RollingUpdate—-滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。</li>\n</ul>\n<h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>随着流量的增加，我们可能需要增加我们应用的规模来满足用户的需求。Kubernetes的Scale功能就可以实现这个需求。</p>\n<blockquote>\n<p>   Scaling is accomplished by changing the number of replicas in a Deployment.</p>\n</blockquote>\n<p>扩大应用的规模时，Kubernetes将会在Nodes上面使用可用的资源来创建新的Pod，并运行新增加的应用，缩小规模时做相反的操作。Kubernetes也支持自动规模化Pod。当然我们也可以将应用的数量变为0，这样就会终止所有部署该应用的Pods。应用数量增加后，Service内的负载均衡就会变得非常有用了.</p>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling1.svg\" alt=\"scale\"></p>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling2.svg\" alt=\"scale\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   1         1         1            1           2h</div></pre></td></tr></table></figure>\n<p>可以看到，现在我们只有一个Pod，</p>\n<ul>\n<li>DESIRED字段表示我们配置的replicas的个数，即实例的个数。</li>\n<li>CURRENT字段表示目前处于running状态的replicas的个数。</li>\n<li>UP-TO-DATE字段表示表示和预先配置的期望状态相符的replicas的个数。</li>\n<li>AVAILABLE字段表示目前实际对用户可用的replicas的个数。</li>\n</ul>\n<p>下面我们使用kubectl scale命令将启动4个复制品，语法规则是kubectl scale deployment-type name replicas-number：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl scale deployment/helloworld --replicas=4</div><div class=\"line\">deployment &quot;helloworld&quot; scaled</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   4         4         4            4           2h</div><div class=\"line\"></div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod -o wide</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE       IP           NODE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Running   0          3m        172.17.0.4   minikube</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running   0          2h        172.17.0.3   minikube</div><div class=\"line\">helloworld-2790924137-jg15m   1/1       Running   0          3m        172.17.0.5   minikube</div><div class=\"line\">helloworld-2790924137-tgqr9   1/1       Running   0          3m        172.17.0.2   minikube</div></pre></td></tr></table></figure></p>\n<p>验证一下这个Service是有负载均衡的：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   4         4         4            4           2h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-tgqr9</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-2kg70</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-tgqr9</div></pre></td></tr></table></figure></p>\n<h3 id=\"Rolling-Update\"><a href=\"#Rolling-Update\" class=\"headerlink\" title=\"Rolling Update\"></a>Rolling Update</h3><p>滚动更新（Rolling update）特性的好处就是我们不用停止服务就可以实现应用更新。默认更新的时候是一个Pod一个Pod更新的，所以整个过程服务不会中断。当然你也可以设置一次更新的Pod的百分比。而且更新过程中，Service只会将流量转发到可用的节点上面。更加重要的是，我们可以随时回退到旧版本。</p>\n<blockquote>\n<p>Rolling updates allow Deployments’ update to take place with zero downtime by incrementally updating Pods instances with new ones.<br>If a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.</p>\n</blockquote>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates1.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates2.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates3.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates4.svg\" alt=\"rollingupdate\"></p>\n<h3 id=\"Set-image\"><a href=\"#Set-image\" class=\"headerlink\" title=\"Set image\"></a>Set image</h3><p>在原来程序的基础上，多输出一个v2作为新版本，使用set image命令指定新版本镜像.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl set image deployments/helloworld helloworld=registry.hnaresearch.com/public/hello-world:v2.0</div><div class=\"line\">deployment &quot;helloworld&quot; image updated</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS              RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Running             0          54m</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running             0          3h</div><div class=\"line\">helloworld-2790924137-tgqr9   1/1       Running             0          54m</div><div class=\"line\">helloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m</div><div class=\"line\">helloworld-2889228138-q68vx   0/1       ContainerCreating   0          9m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS              RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Terminating         0          54m</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running             0          3h</div><div class=\"line\">helloworld-2790924137-tgqr9   0/1       Terminating         0          54m</div><div class=\"line\">helloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m</div><div class=\"line\">helloworld-2889228138-bj38m   0/1       Pending             0          9m</div><div class=\"line\">helloworld-2889228138-dv3ch   1/1       Running             0          9m</div><div class=\"line\">helloworld-2889228138-q68vx   1/1       Running             0          9m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE</div><div class=\"line\">helloworld-2889228138-65lmj   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-bj38m   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-dv3ch   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-q68vx   1/1       Running   0          10m</div></pre></td></tr></table></figure></p>\n<h3 id=\"Rollout-undo\"><a href=\"#Rollout-undo\" class=\"headerlink\" title=\"Rollout undo\"></a>Rollout undo</h3><p>使用kubectl rollout undo命令回滚到之前的版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl rollout undo deployment/helloworld</div><div class=\"line\">deployment &quot;helloworld&quot; rolled back</div></pre></td></tr></table></figure></p>\n<h2 id=\"Autoscaling\"><a href=\"#Autoscaling\" class=\"headerlink\" title=\"Autoscaling\"></a>Autoscaling</h2><p>系统能够根据负载的变化对计算资源的分配进行自动的扩增或者收缩，无疑是一个非常吸引人的特征，它能够最大可能地减少费用或者其他代价（如电力损耗）。自动扩展主要分为两种，其一为水平扩展，针对于实例数目的增减；其二为垂直扩展，即单个实例可以使用的资源的增减。Horizontal Pod Autoscaler（HPA）属于前者。</p>\n<p>Horizontal Pod Autoscaler的操作对象是Replication Controller、ReplicaSet或Deployment对应的Pod，根据观察到的CPU实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。</p>\n<h3 id=\"决策策略\"><a href=\"#决策策略\" class=\"headerlink\" title=\"决策策略\"></a>决策策略</h3><p>在HPA Controller检测到CPU的实际使用量之后，会求出当前的CPU使用率（实际使用量与pod 请求量的比率)。然后，HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近．</p>\n<p>另外，考虑到自动扩展的决策可能需要一段时间才会生效，甚至在短时间内会引入一些噪声．</p>\n<ul>\n<li>例如当pod所需要的CPU负荷过大，从而运行一个新的pod进行分流，在创建的过程中，系统的CPU使用量可能会有一个攀升的过程。所以，在每一次作出决策后的一段时间内，将不再进行扩展决策。对于ScaleUp而言，这个时间段为3分钟，Scaledown为5分钟。</li>\n<li>再者HPA Controller允许一定范围内的CPU使用量的不稳定，也就是说，只有当aVg（CurrentPodConsumption／Target低于0.9或者高于1.1时才进行实例调整，这也是出于维护系统稳定性的考虑。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"基础架构\"><a href=\"#基础架构\" class=\"headerlink\" title=\"基础架构\"></a>基础架构</h2><p>Kubernetes将底层的计算资源连接在一起对外体现为一个计算集群，并将资源高度抽象化。部署应用时Kubernetes会以更高效的方式自动的将应用分发到集群内的机器上面，并调度运行。</p>\n<p>Kubernetes集群包含两种类型的资源：</p>\n<ul>\n<li>Master节点：协调控制整个集群。Master负责管理整个集群，协调集群内的所有行为。比如调度应用，监控应用的状态等。</li>\n<li>Nodes节点：运行应用的工作节点。Node节点负责运行应用，一般是一台物理机或者虚机。每个Node节点上面都有一个Kubelet，它是一个代理程序，用来管理该节点以及和Master节点通信。除此以外，Node节点上还会有一些管理容器的工具，比如Docker或者rkt等。生产环境中一个Kubernetes集群至少应该包含三个Nodes节点。</li>\n</ul>\n<p>当部署应用的时候，我们通知Master节点启动应用容器。然后Master会调度这些应用将它们运行在Node节点上面。Node节点和Master节点通过Master节点暴露的Kubernetes API通信。当然我们也可以直接通过这些API和集群交互。<br><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_01_cluster.svg\" alt=\"kubernetes Cluster\"></p>\n<h3 id=\"Master\"><a href=\"#Master\" class=\"headerlink\" title=\"Master\"></a>Master</h3><p>Master节点上面主要由四个模块组成：APIServer、scheduler、controller manager、etcd。</p>\n<ul>\n<li>APIServer<br>APIServer负责对外提供RESTful的Kubernetes API服务，它是系统管理指令的统一入口，任何对资源进行增删改查的操作都要交给APIServer处理后再提交给etcd。如架构图中所示，kubectl（Kubernetes提供的客户端工具，该工具内部就是对Kubernetes API的调用）是直接和APIServer交互的。</li>\n<li>Scheduler<br>Scheduler的职责很明确，就是负责调度pod到合适的Node上。如果把scheduler看成一个黑匣子，那么它的输入是pod和由多个Node组成的列表，输出是Pod和一个Node的绑定，即将这个pod部署到这个Node上。Kubernetes目前提供了调度算法，但是同样也保留了接口，用户可以根据自己的需求定义自己的调度算法。</li>\n<li>Controller manager<br>如果说APIServer做的是“前台”的工作的话，那controller manager就是负责“后台”的。每个资源一般都对应有一个控制器，而controller manager就是负责管理这些控制器的。比如我们通过APIServer创建一个pod，当这个pod创建成功后，APIServer的任务就算完成了。而后面保证Pod的状态始终和我们预期的一样的重任就由controller manager去保证了。</li>\n<li>etcd<br>etcd是一个高可用的键值存储系统，Kubernetes使用它来存储各个资源的状态，从而实现了Restful的API。</li>\n</ul>\n<h3 id=\"Node\"><a href=\"#Node\" class=\"headerlink\" title=\"Node\"></a>Node</h3><p>每个Node节点主要由三个模块组成：kubelet、kube-proxy、runtime。</p>\n<ul>\n<li>kubelet。Kubelet是Master在每个Node节点上面的agent，是Node节点上面最重要的模块，它负责维护和管理该Node上面的所有容器，但是如果容器不是通过Kubernetes创建的，它并不会管理。本质上，它负责使Pod得运行状态与期望的状态一致。</li>\n<li>kube-proxy。该模块实现了Kubernetes中的服务发现和反向代理功能。反向代理方面：kube-proxy支持TCP和UDP连接转发，默认基于Round Robin算法将客户端流量转发到与service对应的一组后端pod。服务发现方面，kube-proxy使用etcd的watch机制，监控集群中service和endpoint对象数据的动态变化，并且维护一个service到endpoint的映射关系，从而保证了后端pod的IP变化不会对访问者造成影响。另外kube-proxy还支持session affinity。</li>\n<li>runtime。runtime指的是容器运行环境，目前Kubernetes支持docker和rkt两种容器。</li>\n</ul>\n<h2 id=\"搭建MiniKube\"><a href=\"#搭建MiniKube\" class=\"headerlink\" title=\"搭建MiniKube\"></a>搭建MiniKube</h2><p>环境: MacOS, virtualbox, minikube v0.18.0, kubectl v1.6.0</p>\n<p>Kubernetes提供了一个轻量级的Minikube应用，利用它我们可以很容器的创建一个只包含一个Node节点的Kubernetes Cluster用于日常的开发测试。</p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>Minikube的安装可以参考: Minikube的Github：<a href=\"https://github.com/kubernetes/minikube\" target=\"_blank\" rel=\"external\">https://github.com/kubernetes/minikube</a></p>\n<p>要正常使用，还必须安装kubectl，并且放在PATH里面。kubectl是一个通过Kubernetes API和Kubernetes集群交互的命令行工具。</p>\n<h3 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h3><p>可以通过minikube查看cluster运行状态,启动或者停止cluster.例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">minikube start</div></pre></td></tr></table></figure>\n<h3 id=\"ONLY-FOR-CHINESE\"><a href=\"#ONLY-FOR-CHINESE\" class=\"headerlink\" title=\"ONLY FOR CHINESE\"></a>ONLY FOR CHINESE</h3><p>Kubernetes在部署容器应用的时候会先拉一个pause镜像，这个是一个基础容器，主要是负责网络部分的功能的，具体这里不展开讨论。最关键的是Kubernetes里面镜像默认都是从Google的镜像仓库拉的（就跟docker默认从docker hub拉的一样），但是因为GFW的原因，中国用户是访问不了Google的镜像仓库gcr.io的（如果你可以ping通，那恭喜你）。庆幸的是这个镜像被传到了docker hub上面，虽然中国用户访问后者也非常艰难，但通过一些加速器之类的还是可以pull下来的。如果没有VPN等科学上网的工具的话，请先做如下操作：</p>\n<p>See: <a href=\"https://github.com/kubernetes/kubernetes/issues/6888\" target=\"_blank\" rel=\"external\">https://github.com/kubernetes/kubernetes/issues/6888</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">minikube ssh    # 登录到我们的Kubernetes VM里面去</div><div class=\"line\">docker pull registry.hnaresearch.com/public/pause-amd64:3.0  </div><div class=\"line\">docker tag registry.hnaresearch.com/public/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0</div></pre></td></tr></table></figure>\n<p>这样Kubernetes VM就不会从gcr.io拉镜像了，而是会直接使用本地的镜像。</p>\n<h2 id=\"部署应用\"><a href=\"#部署应用\" class=\"headerlink\" title=\"部署应用\"></a>部署应用</h2><p>在Kubernetes Cluster上面部署应用，我们需要先创建一个Kubernetes Deployment。这个Deployment负责创建和更新我们的应用实例。当这个Deployment创建之后，Kubernetes master就会将这个Deployment创建出来的应用实例部署到集群内某个Node节点上。而且自应用实例创建后，Deployment controller还会持续监控应用，直到应用被删除或者部署应用的Node节点不存在。</p>\n<blockquote>\n<p>A Deployment is responsible for creating and updating instances of your application.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_02_first_app.svg\" alt=\"\"></p>\n<p>使用kubectl来创建Deployment，创建的时候需要制定容器镜像以及我们要启动的个数（replicas），当然这些信息后面可以再更新。这里我用Go写了一个简单的Webserver，返回“Hello World”，监听端口是8090.我们就来启动这个应用.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl run helloworld --image=registry.hnaresearch.com/public/hello-world:v1.0 --port=8090</div></pre></td></tr></table></figure></p>\n<p>执行后master寻找一个合适的node来部署我们的应用实例（我们只有一个node）。我们可以使用kubectl get deployment来查看我们创建的Deployment：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl get deployment</div></pre></td></tr></table></figure></p>\n<p>默认应用部署好之后是只在Kubernetes Cluster内部可见的，有多种方法可以让我们的应用暴露到外部，这里先介绍一种简单的：我们可以通过kubectl proxy命令在我们的终端和Kubernetes Cluster直接创建一个代理。然后，打开一个新的终端，通过Pod名(Pod后面会有讲到，可以通过kubectl get pod查看Pod名字)就可以访问了：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod</div><div class=\"line\">NAME                             READY     STATUS             RESTARTS   AGE</div><div class=\"line\">hello-minikube-938614450-xjl4s   0/1       ImagePullBackOff   0          15h</div><div class=\"line\">helloworld-2790924137-bvfhn      1/1       Running            0          2m</div><div class=\"line\"></div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/helloworld-2790924137-bvfhn/</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div></pre></td></tr></table></figure></p>\n<h2 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h2><p>Pod是Kubernetes中一个非常重要的概念，也是区别于其他编排系统的一个设计. Deployment执行时并不是直接创建了容器实例，而是先在Node上面创建了Pod，然后再在Pod里面创建容器。那Pod到底是什么？Pod是Kubernetes里面抽象出来的一个概念，它是能够被创建、调度和管理的最小单元；每个Pod都有一个独立的IP；一个Pod由若干个容器构成。一个Pod之内的容器共享Pod的所有资源，这些资源主要包括：共享存储（以Volumes的形式）、共享网络、共享端口等。Kubernetes虽然也是一个容器编排系统，但不同于其他系统，它的最小操作单元不是单个容器，而是Pod。这个特性给Kubernetes带来了很多优势，比如最显而易见的是同一个Pod内的容器可以非常方便的互相访问（通过localhost就可以访问）和共享数据。</p>\n<blockquote>\n<p>A Pod is a group of one or more application containers (such as Docker or rkt) and includes shared storage (volumes), IP address and information about how to run them.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_pods.svg\" alt=\"pod\"></p>\n<blockquote>\n<p>A Node is a group of one ore more pods and includes the kubelet and container engine.  </p>\n<p>Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_03_nodes.svg\" alt=\"node\"></p>\n<h2 id=\"Job\"><a href=\"#Job\" class=\"headerlink\" title=\"Job\"></a>Job</h2><p>从程序的运行形态上来区分，我们可以将Pod分为两类：长时运行服务（jboss、mysql等）和一次性任务（数据计算、测试）。Replication Controller创建的Pod都是长时运行的服务，而Job创建的Pod都是一次性任务。</p>\n<p>在Job的定义中，restartPolicy（重启策略）只能是Never和OnFailure。Job可以控制一次性任务的Pod的完成次数（Job–&gt;spec–&gt;completions）和并发执行数（Job–&gt;spec–&gt;parallelism），当Pod成功执行指定次数后，即认为Job执行完毕。</p>\n<h2 id=\"Replication-Controller\"><a href=\"#Replication-Controller\" class=\"headerlink\" title=\"Replication Controller\"></a>Replication Controller</h2><p>Replication Controller（RC）是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。</p>\n<h3 id=\"Replica-Set\"><a href=\"#Replica-Set\" class=\"headerlink\" title=\"Replica Set\"></a>Replica Set</h3><p>新一代副本控制器replica set，可以被认为 是“升级版”的Replication Controller。也就是说。replica set也是用于保证与label selector匹配的pod数量维持在期望状态。区别在于，replica set引入了对基于子集的selector查询条件，而Replication Controller仅支持基于值相等的selecto条件查询。这是目前从用户角度肴，两者唯一的显著差异。 社区引入这一API的初衷是用于取代vl中的Replication Controller，也就是说．当v1版本被废弃时，Replication Controller就完成了它的历史使命，而由replica set来接管其工作。虽然replica set可以被单独使用，但是目前它多被Deployment用于进行pod的创建、更新与删除。</p>\n<h2 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h2><h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p><a href=\"id:service\" target=\"_blank\" rel=\"external\">Service</a>是Kubernetes里面抽象出来的一层，它定义了由多个Pods组成的逻辑组（logical set），可以对组内的Pod做一些事情：</p>\n<ul>\n<li>对外暴露流量</li>\n<li>做负载均衡（load balancing）</li>\n<li>服务发现（service-discovery）</li>\n</ul>\n<blockquote>\n<p>A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.</p>\n</blockquote>\n<p>在Kubernetes中，在受到Replication Controller调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。</p>\n<p>Service同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod–&gt;metadata–&gt;labeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。</p>\n<p>需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。</p>\n<h3 id=\"Service代理服务\"><a href=\"#Service代理服务\" class=\"headerlink\" title=\"Service代理服务\"></a>Service代理服务</h3><p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_services.svg\" alt=\"service\"></p>\n<p>使用kubectl get service可以查看目前已有的service，Minikube默认创建了一个kubernetes Service。我们使用expose命令再创建一个Service：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">hello-minikube   10.0.0.188   &lt;nodes&gt;       8080:32710/TCP   16h</div><div class=\"line\">kubernetes       10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl expose deployment/helloworld --type=&quot;NodePort&quot; --port 8090</div><div class=\"line\">service &quot;helloworld&quot; exposed</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">hello-minikube   10.0.0.188   &lt;nodes&gt;       8080:32710/TCP   16h</div><div class=\"line\">helloworld       10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   2m</div><div class=\"line\">kubernetes       10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl delete service hello-minikube</div><div class=\"line\">service &quot;hello-minikube&quot; deleted</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   2m</div><div class=\"line\">kubernetes   10.0.0.1     &lt;none&gt;        443/TCP          16h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe service/helloworld</div><div class=\"line\">Name:  \t\t\thelloworld</div><div class=\"line\">Namespace:     \t\tdefault</div><div class=\"line\">Labels:\t\t\trun=helloworld</div><div class=\"line\">Annotations:   \t\t&lt;none&gt;</div><div class=\"line\">Selector:      \t\trun=helloworld</div><div class=\"line\">Type:  \t\t\tNodePort</div><div class=\"line\">IP:    \t\t\t10.0.0.249</div><div class=\"line\">Port:  \t\t\t&lt;unset&gt;\t8090/TCP</div><div class=\"line\">NodePort:      \t\t&lt;unset&gt;\t31240/TCP</div><div class=\"line\">Endpoints:     \t\t172.17.0.3:8090</div><div class=\"line\">Session Affinity:      \tNone</div><div class=\"line\">Events:\t\t\t&lt;none&gt;</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ minikube docker-env</div><div class=\"line\">export DOCKER_TLS_VERIFY=&quot;1&quot;</div><div class=\"line\">export DOCKER_HOST=&quot;tcp://192.168.99.100:2376&quot;</div><div class=\"line\">export DOCKER_CERT_PATH=&quot;/Users/xiningwang/.minikube/certs&quot;</div><div class=\"line\">export DOCKER_API_VERSION=&quot;1.23&quot;</div><div class=\"line\"># Run this command to configure your shell:</div><div class=\"line\"># eval $(minikube docker-env)</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl http://192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div></pre></td></tr></table></figure></p>\n<h3 id=\"Service内部负载均衡\"><a href=\"#Service内部负载均衡\" class=\"headerlink\" title=\"Service内部负载均衡\"></a>Service内部负载均衡</h3><p>当Service的Endpoints包含多个IP的时候，及服务代理存在多个后端，将进行请求的负载均衡。默认的负载均衡策略是轮训或者随机（有kube-proxy的模式决定）。同时，Service上通过设置Service–&gt;spec–&gt;sessionAffinity=ClientIP，来实现基于源IP地址的会话保持。</p>\n<h3 id=\"发布Service\"><a href=\"#发布Service\" class=\"headerlink\" title=\"发布Service\"></a>发布Service</h3><p>Service的虚拟IP是由Kubernetes虚拟出来的内部网络，外部是无法寻址到的。但是有些服务又需要被外部访问到，例如web前段。这时候就需要加一层网络转发，即外网到内网的转发。Kubernetes提供了NodePort、LoadBalancer、Ingress三种方式。</p>\n<ul>\n<li>NodePort<br>在之前的Guestbook示例中，已经延时了NodePort的用法。NodePort的原理是，Kubernetes会在每一个Node上暴露出一个端口：nodePort，外部网络可以通过（任一Node）[NodeIP]:[NodePort]访问到后端的Service。Minikube只支持这种方式。</li>\n<li>LoadBalancer<br>在NodePort基础上，Kubernetes可以请求底层云平台创建一个负载均衡器，将每个Node作为后端，进行服务分发。该模式需要底层云平台（例如GCE）支持。</li>\n<li>Ingress<br>是一种HTTP方式的路由转发机制，由Ingress Controller和HTTP代理服务器组合而成。Ingress Controller实时监控Kubernetes API，实时更新HTTP代理服务器的转发规则。HTTP代理服务器有GCE Load-Balancer、HaProxy、Nginx等开源方案。</li>\n</ul>\n<h3 id=\"自发性\"><a href=\"#自发性\" class=\"headerlink\" title=\"自发性\"></a>自发性</h3><p>Kubernetes中有一个很重要的服务自发现特性。一旦一个service被创建，该service的service IP和service port等信息都可以被注入到pod中供它们使用。Kubernetes主要支持两种service发现 机制：环境变量和DNS。</p>\n<ul>\n<li>环境变量方式<br>Kubernetes创建Pod时会自动添加所有可用的service环境变量到该Pod中，如有需要．这些环境变量就被注入Pod内的容器里。需要注意的是，环境变量的注入只发送在Pod创建时，且不会被自动更新。这个特点暗含了service和访问该service的Pod的创建时间的先后顺序，即任何想要访问service的pod都需要在service已经存在后创建，否则与service相关的环境变量就无法注入该Pod的容器中，这样先创建的容器就无法发现后创建的service。</li>\n<li>DNS方式<br>Kubernetes集群现在支持增加一个可选的组件——DNS服务器。这个DNS服务器使用Kubernetes的watchAPI，不间断的监测新的service的创建并为每个service新建一个DNS记录。如果DNS在整个集群范围内都可用，那么所有的Pod都能够自动解析service的域名。</li>\n</ul>\n<h3 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h3><ul>\n<li><p>多个service如何避免地址和端口冲突<br>此处设计思想是，Kubernetes通过为每个service分配一个唯一的ClusterIP，所以当使用ClusterIP：port的组合访问一个service的时候，不管port是什么，这个组合是一定不会发生重复的。另一方面，kube-proxy为每个service真正打开的是一个绝对不会重复的随机端口，用户在service描述文件中指定的访问端口会被映射到这个随机端口上。这就是为什么用户可以在创建service时随意指定访问端口。</p>\n</li>\n<li><p>目前存在的不足<br>Kubernetes使用iptables和kube-proxy解析service的入口地址，在中小规模的集群中运行良好，但是当service的数量超过一定规模时，仍然有一些小问题。首当其冲的便是service环境变量泛滥，以及service与使用service的pod两者创建时间先后的制约关系。目前来看，很多使用者在使用Kubernetes时往往会开发一套自己的Router组件来替代service，以便更好地掌控和定制这部分功能。</p>\n</li>\n</ul>\n<h2 id=\"Label\"><a href=\"#Label\" class=\"headerlink\" title=\"Label\"></a>Label</h2><p>Service就是靠Label选择器（Label Selectors）来匹配组内的Pod的，而且很多命令都可以操作Label。Label是绑定在对象上（比如Pod）的键值对，主要用来把一些相关的对象组织在一起，并且对于用户来说label是有含义的，比如：</p>\n<ul>\n<li>Production environment (production, test, dev)</li>\n<li>Application version (beta, v1.3)</li>\n<li>Type of service/server (frontend, backend, database)</li>\n</ul>\n<blockquote>\n<p>   Labels are key/value pairs that are attached to objects</p>\n</blockquote>\n<p><img src=\"http://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg\" alt=\"Label\"></p>\n<h3 id=\"Pod创建时默认创建的Label\"><a href=\"#Pod创建时默认创建的Label\" class=\"headerlink\" title=\"Pod创建时默认创建的Label\"></a>Pod创建时默认创建的Label</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn</div><div class=\"line\">Name:  \t\thelloworld-2790924137-bvfhn</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Node:  \t\tminikube/192.168.99.100</div><div class=\"line\">Start Time:    \tFri, 05 May 2017 14:03:56 +0800</div><div class=\"line\">Labels:\t\tpod-template-hash=2790924137</div><div class=\"line\">       \t\trun=helloworld</div></pre></td></tr></table></figure>\n<h3 id=\"新增一个Label\"><a href=\"#新增一个Label\" class=\"headerlink\" title=\"新增一个Label\"></a>新增一个Label</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl label pod helloworld-2790924137-bvfhn app=v1</div><div class=\"line\">pod &quot;helloworld-2790924137-bvfhn&quot; labeled</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe pod helloworld-2790924137-bvfhn</div><div class=\"line\">Name:  \t\thelloworld-2790924137-bvfhn</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Node:  \t\tminikube/192.168.99.100</div><div class=\"line\">Start Time:    \tFri, 05 May 2017 14:03:56 +0800</div><div class=\"line\">Labels:\t\tapp=v1</div><div class=\"line\">       \t\tpod-template-hash=2790924137</div><div class=\"line\">       \t\trun=helloworld</div></pre></td></tr></table></figure>\n<h3 id=\"使用Label的例子\"><a href=\"#使用Label的例子\" class=\"headerlink\" title=\"使用Label的例子\"></a>使用Label的例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   46m</div><div class=\"line\">kubernetes   10.0.0.1     &lt;none&gt;        443/TCP          17h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get service  -l run=helloworld</div><div class=\"line\">NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE</div><div class=\"line\">helloworld   10.0.0.249   &lt;nodes&gt;       8090:31240/TCP   46m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod -l app=v1</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running   0          1h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod</div><div class=\"line\">NAME                             READY     STATUS             RESTARTS   AGE</div><div class=\"line\">hello-minikube-938614450-b7xwm   0/1       ImagePullBackOff   0          38m</div><div class=\"line\">helloworld-2790924137-bvfhn      1/1       Running            0          1h</div></pre></td></tr></table></figure>\n<h2 id=\"Pet-Sets-StatefulSet\"><a href=\"#Pet-Sets-StatefulSet\" class=\"headerlink\" title=\"Pet Sets/StatefulSet\"></a>Pet Sets/StatefulSet</h2><p>K8s在1.3版本里发布了Alpha版的PetSet功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而PetSet是用来控制有状态服务，PetSet中的每个Pod的名字都是事先确定的，不能更改。PetSet中Pod的名字的作用，是用来关联与该Pod对应的状态。</p>\n<p>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别；对于PetSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂在上原来Pod的存储继续以它的状态提供服务。</p>\n<p>适合于PetSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。PetSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用PetSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。</p>\n<h2 id=\"Volume\"><a href=\"#Volume\" class=\"headerlink\" title=\"Volume\"></a>Volume</h2><p>在Docker的设计实现中，容器中的数据是临时的，即当容器被销毁时，其中的数据将会丢失。如果需要持久化数据，需要使用Docker数据卷挂载宿主机上的文件或者目录到容器中。在Kubernetes中，当Pod重建的时候，数据是会丢失的，Kubernetes也是通过数据卷挂载来提供Pod数据的持久化的。Kubernetes数据卷是对Docker数据卷的扩展，Kubernetes数据卷是Pod级别的，可以用来实现Pod中容器的文件共享。目前，Kubernetes支持的数据卷类型如下：</p>\n<ul>\n<li>本地数据卷<br>EmptyDir、HostPath这两种类型的数据卷，只能最用于本地文件系统。本地数据卷中的数据只会存在于一台机器上，所以当Pod发生迁移的时候，数据便会丢失。该类型Volume的用途是：Pod中容器间的文件共享、共享宿主机的文件系统。</li>\n<li>网络数据卷<br>Kubernetes提供了很多类型的数据卷以集成第三方的存储系统，包括一些非常流行的分布式文件系统，也有在IaaS平台上提供的存储支持，这些存储系统都是分布式的，通过网络共享文件系统，因此我们称这一类数据卷为网络数据卷。<br>网络数据卷能够满足数据的持久化需求，Pod通过配置使用网络数据卷，每次Pod创建的时候都会将存储系统的远端文件目录挂载到容器中，数据卷中的数据将被水久保存，即使Pod被删除，只是除去挂载数据卷，数据卷中的数据仍然保存在存储系统中，且当新的Pod被创建的时候，仍是挂载同样的数据卷。网络数据卷包含以下几种：NFS、iSCISI、GlusterFS、RBD（Ceph Block Device）、Flocker、AWS Elastic Block Store、GCE Persistent Disk.</li>\n<li>信息数据卷<br>Kubernetes中有一些数据卷，主要用来给容器传递配置信息，我们称之为信息数据卷，比如Secret（处理敏感配置信息，密码、Token等）、Downward API（通过环境变量的方式告诉容器Pod的信息）、Git Repo（将Git仓库下载到Pod中），都是将Pod的信息以文件形式保存，然后以数据卷方式挂载到容器中，容器通过读取文件获取相应的信息。</li>\n</ul>\n<h2 id=\"Deployment\"><a href=\"#Deployment\" class=\"headerlink\" title=\"Deployment\"></a>Deployment</h2><p>Kubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性：</p>\n<ul>\n<li>Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。</li>\n<li>事件和状态查看：可以查看Deployment的升级详细进度和状态。</li>\n<li>回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。</li>\n<li>版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。</li>\n<li>暂停和启动：对于每一次升级，都能够随时暂停和启动。</li>\n<li>多种升级方案：Recreate—-删除所有已存在的pod,重新创建新的; RollingUpdate—-滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。</li>\n</ul>\n<h3 id=\"Scale\"><a href=\"#Scale\" class=\"headerlink\" title=\"Scale\"></a>Scale</h3><p>随着流量的增加，我们可能需要增加我们应用的规模来满足用户的需求。Kubernetes的Scale功能就可以实现这个需求。</p>\n<blockquote>\n<p>   Scaling is accomplished by changing the number of replicas in a Deployment.</p>\n</blockquote>\n<p>扩大应用的规模时，Kubernetes将会在Nodes上面使用可用的资源来创建新的Pod，并运行新增加的应用，缩小规模时做相反的操作。Kubernetes也支持自动规模化Pod。当然我们也可以将应用的数量变为0，这样就会终止所有部署该应用的Pods。应用数量增加后，Service内的负载均衡就会变得非常有用了.</p>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling1.svg\" alt=\"scale\"></p>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_05_scaling2.svg\" alt=\"scale\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   1         1         1            1           2h</div></pre></td></tr></table></figure>\n<p>可以看到，现在我们只有一个Pod，</p>\n<ul>\n<li>DESIRED字段表示我们配置的replicas的个数，即实例的个数。</li>\n<li>CURRENT字段表示目前处于running状态的replicas的个数。</li>\n<li>UP-TO-DATE字段表示表示和预先配置的期望状态相符的replicas的个数。</li>\n<li>AVAILABLE字段表示目前实际对用户可用的replicas的个数。</li>\n</ul>\n<p>下面我们使用kubectl scale命令将启动4个复制品，语法规则是kubectl scale deployment-type name replicas-number：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl scale deployment/helloworld --replicas=4</div><div class=\"line\">deployment &quot;helloworld&quot; scaled</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   4         4         4            4           2h</div><div class=\"line\"></div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pod -o wide</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE       IP           NODE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Running   0          3m        172.17.0.4   minikube</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running   0          2h        172.17.0.3   minikube</div><div class=\"line\">helloworld-2790924137-jg15m   1/1       Running   0          3m        172.17.0.5   minikube</div><div class=\"line\">helloworld-2790924137-tgqr9   1/1       Running   0          3m        172.17.0.2   minikube</div></pre></td></tr></table></figure></p>\n<p>验证一下这个Service是有负载均衡的：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get deployment</div><div class=\"line\">NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class=\"line\">helloworld   4         4         4            4           2h</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-tgqr9</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-2kg70</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-bvfhn</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ curl 192.168.99.100:31240</div><div class=\"line\">Hello world !</div><div class=\"line\">hostname:helloworld-2790924137-tgqr9</div></pre></td></tr></table></figure></p>\n<h3 id=\"Rolling-Update\"><a href=\"#Rolling-Update\" class=\"headerlink\" title=\"Rolling Update\"></a>Rolling Update</h3><p>滚动更新（Rolling update）特性的好处就是我们不用停止服务就可以实现应用更新。默认更新的时候是一个Pod一个Pod更新的，所以整个过程服务不会中断。当然你也可以设置一次更新的Pod的百分比。而且更新过程中，Service只会将流量转发到可用的节点上面。更加重要的是，我们可以随时回退到旧版本。</p>\n<blockquote>\n<p>Rolling updates allow Deployments’ update to take place with zero downtime by incrementally updating Pods instances with new ones.<br>If a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.</p>\n</blockquote>\n<p><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates1.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates2.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates3.svg\" alt=\"rollingupdate\"><br><img src=\"https://kubernetes.io/docs/tutorials/kubernetes-basics/public/images/module_06_rollingupdates4.svg\" alt=\"rollingupdate\"></p>\n<h3 id=\"Set-image\"><a href=\"#Set-image\" class=\"headerlink\" title=\"Set image\"></a>Set image</h3><p>在原来程序的基础上，多输出一个v2作为新版本，使用set image命令指定新版本镜像.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl set image deployments/helloworld helloworld=registry.hnaresearch.com/public/hello-world:v2.0</div><div class=\"line\">deployment &quot;helloworld&quot; image updated</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS              RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Running             0          54m</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running             0          3h</div><div class=\"line\">helloworld-2790924137-tgqr9   1/1       Running             0          54m</div><div class=\"line\">helloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m</div><div class=\"line\">helloworld-2889228138-q68vx   0/1       ContainerCreating   0          9m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS              RESTARTS   AGE</div><div class=\"line\">helloworld-2790924137-2kg70   1/1       Terminating         0          54m</div><div class=\"line\">helloworld-2790924137-bvfhn   1/1       Running             0          3h</div><div class=\"line\">helloworld-2790924137-tgqr9   0/1       Terminating         0          54m</div><div class=\"line\">helloworld-2889228138-65lmj   0/1       ContainerCreating   0          9m</div><div class=\"line\">helloworld-2889228138-bj38m   0/1       Pending             0          9m</div><div class=\"line\">helloworld-2889228138-dv3ch   1/1       Running             0          9m</div><div class=\"line\">helloworld-2889228138-q68vx   1/1       Running             0          9m</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get pods</div><div class=\"line\">NAME                          READY     STATUS    RESTARTS   AGE</div><div class=\"line\">helloworld-2889228138-65lmj   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-bj38m   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-dv3ch   1/1       Running   0          10m</div><div class=\"line\">helloworld-2889228138-q68vx   1/1       Running   0          10m</div></pre></td></tr></table></figure></p>\n<h3 id=\"Rollout-undo\"><a href=\"#Rollout-undo\" class=\"headerlink\" title=\"Rollout undo\"></a>Rollout undo</h3><p>使用kubectl rollout undo命令回滚到之前的版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl rollout undo deployment/helloworld</div><div class=\"line\">deployment &quot;helloworld&quot; rolled back</div></pre></td></tr></table></figure></p>\n<h2 id=\"Autoscaling\"><a href=\"#Autoscaling\" class=\"headerlink\" title=\"Autoscaling\"></a>Autoscaling</h2><p>系统能够根据负载的变化对计算资源的分配进行自动的扩增或者收缩，无疑是一个非常吸引人的特征，它能够最大可能地减少费用或者其他代价（如电力损耗）。自动扩展主要分为两种，其一为水平扩展，针对于实例数目的增减；其二为垂直扩展，即单个实例可以使用的资源的增减。Horizontal Pod Autoscaler（HPA）属于前者。</p>\n<p>Horizontal Pod Autoscaler的操作对象是Replication Controller、ReplicaSet或Deployment对应的Pod，根据观察到的CPU实际使用量与用户的期望值进行比对，做出是否需要增减实例数量的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。</p>\n<h3 id=\"决策策略\"><a href=\"#决策策略\" class=\"headerlink\" title=\"决策策略\"></a>决策策略</h3><p>在HPA Controller检测到CPU的实际使用量之后，会求出当前的CPU使用率（实际使用量与pod 请求量的比率)。然后，HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近．</p>\n<p>另外，考虑到自动扩展的决策可能需要一段时间才会生效，甚至在短时间内会引入一些噪声．</p>\n<ul>\n<li>例如当pod所需要的CPU负荷过大，从而运行一个新的pod进行分流，在创建的过程中，系统的CPU使用量可能会有一个攀升的过程。所以，在每一次作出决策后的一段时间内，将不再进行扩展决策。对于ScaleUp而言，这个时间段为3分钟，Scaledown为5分钟。</li>\n<li>再者HPA Controller允许一定范围内的CPU使用量的不稳定，也就是说，只有当aVg（CurrentPodConsumption／Target低于0.9或者高于1.1时才进行实例调整，这也是出于维护系统稳定性的考虑。</li>\n</ul>\n"},{"title":"Kubernetes Secrets","date":"2017-03-10T12:46:25.000Z","_content":"\n## Secrets描述\n在Kubernetes中，Secret对象类型主要目的是保存一些私密数据，比如密码, tokens, ssh keys等信息。将这些信息放在Secret对象中比直接放在pod或docker image中更安全，也更方便使用。\n\n创建Secrets对象的方式有两种，一种是用户手动创建，另一种是集群自动创建。\n一个已经创建好的Secrets对象有两种方式被pod对象使用，其一，在container中的volume对象里以file的形式被使用，其二，在pull images时被kubelet使用。\n\n为了使用Secret对象，pod必须引用这个Secret，同样可以手动或者自动来执行引用操作。\n\n## Built-in Secrets\nKubernetes会自动创建包含证书信息的Secret，并且使用它来访问api, Kubernetes也将自动修改pod来使用这个Secret。\n\n自动创建的Secret以及所使用的api证书,可以根据需要disable或者override。如果仅仅需要安全访问apiserver，那么上述的流程是推荐的方式。\n\n## 自定义Secrets\n### 通过kubectl自定义Secrets\n数据中的字段为map类型。其中keys必须符合dns_subdomain规则，values可以为任意类型，使用base64编码。上述例子中，username和password的数据值在base64编码前的值为value-1 和 value-2。\n```\nxis-macbook-pro:~ xiningwang$ echo -n \"admin\" > ./username.txt\nxis-macbook-pro:~ xiningwang$  echo -n \"1f2d1e2e67df\" > ./password.txt\nxis-macbook-pro:~ xiningwang$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\nsecret \"db-user-pass\" created\n```\n\n查看创建的Secrets:\n```\nxis-macbook-pro:~ xiningwang$ kubectl get secrets\nNAME                  TYPE                                  DATA      AGE\ndb-user-pass          Opaque                                2         6m\ndefault-token-t5bk8   kubernetes.io/service-account-token   3         4d\nxis-macbook-pro:~ xiningwang$ kubectl describe secrets/db-user-pass\nName:  \t\tdb-user-pass\nNamespace:     \tdefault\nLabels:\t\t<none>\nAnnotations:   \t<none>\n\nType:  \tOpaque\n\nData\n====\npassword.txt:  \t12 bytes\nusername.txt:  \t5 bytes\n```\n\n### 手工创建Secrets\n```\nxis-macbook-pro:~ xiningwang$ echo -n \"admin\" | base64\nYWRtaW4=\nxis-macbook-pro:~ xiningwang$ echo -n \"1f2d1e2e67df\" | base64\nMWYyZDFlMmU2N2Rm\nxis-macbook-pro:~ xiningwang$ vi mysecrect.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MWYyZDFlMmU2N2Rm\n```\n使用yaml文件创建Secrets:\n```\nxis-macbook-pro:~ xiningwang$ kubectl create -f ./mysecrect.yaml\nsecret \"mysecret\" created\n```   \n\n## 查看Secrets\n使用如下命令获取创建的Secrets内容:\n```\nxis-macbook-pro:~ xiningwang$ kubectl get secret db-user-pass -o yaml\napiVersion: v1\ndata:\n  password.txt: MWYyZDFlMmU2N2Rm\n  username.txt: YWRtaW4=\nkind: Secret\nmetadata:\n  creationTimestamp: 2017-05-09T07:54:29Z\n  name: db-user-pass\n  namespace: default\n  resourceVersion: \"84609\"\n  selfLink: /api/v1/namespaces/default/secrets/db-user-pass\n  uid: bb23a365-348c-11e7-ab36-080027fd8883\ntype: Opaque\n```\nbase64解密:\n```\n$ echo \"MWYyZDFlMmU2N2Rm\" | base64 --decode\n1f2d1e2e67df\n```\n## 使用Secrets\n### 手动为pod绑定secret\n必须有spec.volumes才能使用secret。 如果一个pod中有多个container，每个container需要他们单独对应的volumeMounts ，但是一个secret只能对应一个spec.volumes。\n```\n{\n \"apiVersion\": \"v1\",\n \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"mypod\",\n    \"namespace\": \"myns\"\n  },\n  \"spec\": {\n    \"containers\": [{\n      \"name\": \"mypod\",\n      \"image\": \"redis\",\n      \"volumeMounts\": [{\n        \"name\": \"foo\",\n        \"mountPath\": \"/etc/foo\",\n        \"readOnly\": true\n      }]\n    }],\n    \"volumes\": [{\n      \"name\": \"foo\",\n      \"secret\": {\n        \"secretName\": \"mysecret\"\n      }\n    }]\n  }\n}\n```\n查看Secrets的值:\n```\n$ ls /etc/foo/\nusername\npassword\n$ cat /etc/foo/username\nadmin\n$ cat /etc/foo/password\n1f2d1e2e67df\n```\n\n### 使用Secrets作为环境变量\n>  定义对Secrets的引用: env[x].valueFrom.secretKeyRef.\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-env-pod\nspec:\n  containers:\n    - name: mycontainer\n      image: redis\n      env:\n        - name: SECRET_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: username\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: password\n  restartPolicy: Never\n```\n查看Secrets的值:\n```\n$ echo $SECRET_USERNAME\nadmin\n$ echo $SECRET_PASSWORD\n1f2d1e2e67df\n```\n\n## Secret与Pod的生命周期\n当通过api创建一个pod后，不会去检查所引用的secret是否存在。一旦这个pod被使用，kubelet将会尝试去获取引用的secret的值。如果这个secret不存在，或者kubelet暂时链接不上apiserver，kubelet将会定期重试，并发送一个event来解释pod没有启动的原因。如果获取到了对应的secret，kubelet将会创建对应的volume并绑定到container。\n\n一旦kubelet创建了一个pod，则container使用的相关secret volume不会在改变，即使对应的secret对象被修改。如果为了改变使用的secret，则必须删除旧的pod，并重新创建一个新的pod。\n\n##限制\n在使用之前，Secret volume 资源被验证，以确保指定的对象引用真是指向一个secret对象。因此，在pod使用它之前必须保证需要的secret被成功创建。Secret api对象从属于namespace，一个Secret对象只能被同namespace的pod所使用。\n\n单个secret限制在1Mb之内，防止过大的secret耗尽apiserver & kubelet的内存。然而，创建许多类似的secret同样也会无用的消耗掉apiserver&kubelet的内存。\n\nkubelet目前只支持pod使用来自于apiserver的secret。pods包括了被 kubectl创建的pod 或者 被replication controller间接创建的。\n","source":"_posts/kubernetes-secrets.md","raw":"---\ntitle: Kubernetes Secrets\ndate: 2017-3-10 20:46:25\ncategories:\n  - 分布式&云计算\n  - Kubernetes\ntags:\n  - 分布式\n  - Kubernetes\n  - 配置\n  - 容器\n  - Secrets\n---\n\n## Secrets描述\n在Kubernetes中，Secret对象类型主要目的是保存一些私密数据，比如密码, tokens, ssh keys等信息。将这些信息放在Secret对象中比直接放在pod或docker image中更安全，也更方便使用。\n\n创建Secrets对象的方式有两种，一种是用户手动创建，另一种是集群自动创建。\n一个已经创建好的Secrets对象有两种方式被pod对象使用，其一，在container中的volume对象里以file的形式被使用，其二，在pull images时被kubelet使用。\n\n为了使用Secret对象，pod必须引用这个Secret，同样可以手动或者自动来执行引用操作。\n\n## Built-in Secrets\nKubernetes会自动创建包含证书信息的Secret，并且使用它来访问api, Kubernetes也将自动修改pod来使用这个Secret。\n\n自动创建的Secret以及所使用的api证书,可以根据需要disable或者override。如果仅仅需要安全访问apiserver，那么上述的流程是推荐的方式。\n\n## 自定义Secrets\n### 通过kubectl自定义Secrets\n数据中的字段为map类型。其中keys必须符合dns_subdomain规则，values可以为任意类型，使用base64编码。上述例子中，username和password的数据值在base64编码前的值为value-1 和 value-2。\n```\nxis-macbook-pro:~ xiningwang$ echo -n \"admin\" > ./username.txt\nxis-macbook-pro:~ xiningwang$  echo -n \"1f2d1e2e67df\" > ./password.txt\nxis-macbook-pro:~ xiningwang$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt\nsecret \"db-user-pass\" created\n```\n\n查看创建的Secrets:\n```\nxis-macbook-pro:~ xiningwang$ kubectl get secrets\nNAME                  TYPE                                  DATA      AGE\ndb-user-pass          Opaque                                2         6m\ndefault-token-t5bk8   kubernetes.io/service-account-token   3         4d\nxis-macbook-pro:~ xiningwang$ kubectl describe secrets/db-user-pass\nName:  \t\tdb-user-pass\nNamespace:     \tdefault\nLabels:\t\t<none>\nAnnotations:   \t<none>\n\nType:  \tOpaque\n\nData\n====\npassword.txt:  \t12 bytes\nusername.txt:  \t5 bytes\n```\n\n### 手工创建Secrets\n```\nxis-macbook-pro:~ xiningwang$ echo -n \"admin\" | base64\nYWRtaW4=\nxis-macbook-pro:~ xiningwang$ echo -n \"1f2d1e2e67df\" | base64\nMWYyZDFlMmU2N2Rm\nxis-macbook-pro:~ xiningwang$ vi mysecrect.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MWYyZDFlMmU2N2Rm\n```\n使用yaml文件创建Secrets:\n```\nxis-macbook-pro:~ xiningwang$ kubectl create -f ./mysecrect.yaml\nsecret \"mysecret\" created\n```   \n\n## 查看Secrets\n使用如下命令获取创建的Secrets内容:\n```\nxis-macbook-pro:~ xiningwang$ kubectl get secret db-user-pass -o yaml\napiVersion: v1\ndata:\n  password.txt: MWYyZDFlMmU2N2Rm\n  username.txt: YWRtaW4=\nkind: Secret\nmetadata:\n  creationTimestamp: 2017-05-09T07:54:29Z\n  name: db-user-pass\n  namespace: default\n  resourceVersion: \"84609\"\n  selfLink: /api/v1/namespaces/default/secrets/db-user-pass\n  uid: bb23a365-348c-11e7-ab36-080027fd8883\ntype: Opaque\n```\nbase64解密:\n```\n$ echo \"MWYyZDFlMmU2N2Rm\" | base64 --decode\n1f2d1e2e67df\n```\n## 使用Secrets\n### 手动为pod绑定secret\n必须有spec.volumes才能使用secret。 如果一个pod中有多个container，每个container需要他们单独对应的volumeMounts ，但是一个secret只能对应一个spec.volumes。\n```\n{\n \"apiVersion\": \"v1\",\n \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"mypod\",\n    \"namespace\": \"myns\"\n  },\n  \"spec\": {\n    \"containers\": [{\n      \"name\": \"mypod\",\n      \"image\": \"redis\",\n      \"volumeMounts\": [{\n        \"name\": \"foo\",\n        \"mountPath\": \"/etc/foo\",\n        \"readOnly\": true\n      }]\n    }],\n    \"volumes\": [{\n      \"name\": \"foo\",\n      \"secret\": {\n        \"secretName\": \"mysecret\"\n      }\n    }]\n  }\n}\n```\n查看Secrets的值:\n```\n$ ls /etc/foo/\nusername\npassword\n$ cat /etc/foo/username\nadmin\n$ cat /etc/foo/password\n1f2d1e2e67df\n```\n\n### 使用Secrets作为环境变量\n>  定义对Secrets的引用: env[x].valueFrom.secretKeyRef.\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-env-pod\nspec:\n  containers:\n    - name: mycontainer\n      image: redis\n      env:\n        - name: SECRET_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: username\n        - name: SECRET_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: password\n  restartPolicy: Never\n```\n查看Secrets的值:\n```\n$ echo $SECRET_USERNAME\nadmin\n$ echo $SECRET_PASSWORD\n1f2d1e2e67df\n```\n\n## Secret与Pod的生命周期\n当通过api创建一个pod后，不会去检查所引用的secret是否存在。一旦这个pod被使用，kubelet将会尝试去获取引用的secret的值。如果这个secret不存在，或者kubelet暂时链接不上apiserver，kubelet将会定期重试，并发送一个event来解释pod没有启动的原因。如果获取到了对应的secret，kubelet将会创建对应的volume并绑定到container。\n\n一旦kubelet创建了一个pod，则container使用的相关secret volume不会在改变，即使对应的secret对象被修改。如果为了改变使用的secret，则必须删除旧的pod，并重新创建一个新的pod。\n\n##限制\n在使用之前，Secret volume 资源被验证，以确保指定的对象引用真是指向一个secret对象。因此，在pod使用它之前必须保证需要的secret被成功创建。Secret api对象从属于namespace，一个Secret对象只能被同namespace的pod所使用。\n\n单个secret限制在1Mb之内，防止过大的secret耗尽apiserver & kubelet的内存。然而，创建许多类似的secret同样也会无用的消耗掉apiserver&kubelet的内存。\n\nkubelet目前只支持pod使用来自于apiserver的secret。pods包括了被 kubectl创建的pod 或者 被replication controller间接创建的。\n","slug":"kubernetes-secrets","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bg4000li272zthqr4vt","content":"<h2 id=\"Secrets描述\"><a href=\"#Secrets描述\" class=\"headerlink\" title=\"Secrets描述\"></a>Secrets描述</h2><p>在Kubernetes中，Secret对象类型主要目的是保存一些私密数据，比如密码, tokens, ssh keys等信息。将这些信息放在Secret对象中比直接放在pod或docker image中更安全，也更方便使用。</p>\n<p>创建Secrets对象的方式有两种，一种是用户手动创建，另一种是集群自动创建。<br>一个已经创建好的Secrets对象有两种方式被pod对象使用，其一，在container中的volume对象里以file的形式被使用，其二，在pull images时被kubelet使用。</p>\n<p>为了使用Secret对象，pod必须引用这个Secret，同样可以手动或者自动来执行引用操作。</p>\n<h2 id=\"Built-in-Secrets\"><a href=\"#Built-in-Secrets\" class=\"headerlink\" title=\"Built-in Secrets\"></a>Built-in Secrets</h2><p>Kubernetes会自动创建包含证书信息的Secret，并且使用它来访问api, Kubernetes也将自动修改pod来使用这个Secret。</p>\n<p>自动创建的Secret以及所使用的api证书,可以根据需要disable或者override。如果仅仅需要安全访问apiserver，那么上述的流程是推荐的方式。</p>\n<h2 id=\"自定义Secrets\"><a href=\"#自定义Secrets\" class=\"headerlink\" title=\"自定义Secrets\"></a>自定义Secrets</h2><h3 id=\"通过kubectl自定义Secrets\"><a href=\"#通过kubectl自定义Secrets\" class=\"headerlink\" title=\"通过kubectl自定义Secrets\"></a>通过kubectl自定义Secrets</h3><p>数据中的字段为map类型。其中keys必须符合dns_subdomain规则，values可以为任意类型，使用base64编码。上述例子中，username和password的数据值在base64编码前的值为value-1 和 value-2。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;admin&quot; &gt; ./username.txt</div><div class=\"line\">xis-macbook-pro:~ xiningwang$  echo -n &quot;1f2d1e2e67df&quot; &gt; ./password.txt</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt</div><div class=\"line\">secret &quot;db-user-pass&quot; created</div></pre></td></tr></table></figure></p>\n<p>查看创建的Secrets:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get secrets</div><div class=\"line\">NAME                  TYPE                                  DATA      AGE</div><div class=\"line\">db-user-pass          Opaque                                2         6m</div><div class=\"line\">default-token-t5bk8   kubernetes.io/service-account-token   3         4d</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe secrets/db-user-pass</div><div class=\"line\">Name:  \t\tdb-user-pass</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Labels:\t\t&lt;none&gt;</div><div class=\"line\">Annotations:   \t&lt;none&gt;</div><div class=\"line\"></div><div class=\"line\">Type:  \tOpaque</div><div class=\"line\"></div><div class=\"line\">Data</div><div class=\"line\">====</div><div class=\"line\">password.txt:  \t12 bytes</div><div class=\"line\">username.txt:  \t5 bytes</div></pre></td></tr></table></figure></p>\n<h3 id=\"手工创建Secrets\"><a href=\"#手工创建Secrets\" class=\"headerlink\" title=\"手工创建Secrets\"></a>手工创建Secrets</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;admin&quot; | base64</div><div class=\"line\">YWRtaW4=</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;1f2d1e2e67df&quot; | base64</div><div class=\"line\">MWYyZDFlMmU2N2Rm</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ vi mysecrect.yaml</div><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Secret</div><div class=\"line\">metadata:</div><div class=\"line\">  name: mysecret</div><div class=\"line\">type: Opaque</div><div class=\"line\">data:</div><div class=\"line\">  username: YWRtaW4=</div><div class=\"line\">  password: MWYyZDFlMmU2N2Rm</div></pre></td></tr></table></figure>\n<p>使用yaml文件创建Secrets:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl create -f ./mysecrect.yaml</div><div class=\"line\">secret &quot;mysecret&quot; created</div><div class=\"line\">```   </div><div class=\"line\"></div><div class=\"line\">## 查看Secrets</div><div class=\"line\">使用如下命令获取创建的Secrets内容:</div></pre></td></tr></table></figure></p>\n<p>xis-macbook-pro:~ xiningwang$ kubectl get secret db-user-pass -o yaml<br>apiVersion: v1<br>data:<br>  password.txt: MWYyZDFlMmU2N2Rm<br>  username.txt: YWRtaW4=<br>kind: Secret<br>metadata:<br>  creationTimestamp: 2017-05-09T07:54:29Z<br>  name: db-user-pass<br>  namespace: default<br>  resourceVersion: “84609”<br>  selfLink: /api/v1/namespaces/default/secrets/db-user-pass<br>  uid: bb23a365-348c-11e7-ab36-080027fd8883<br>type: Opaque<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">base64解密:</div></pre></td></tr></table></figure></p>\n<p>$ echo “MWYyZDFlMmU2N2Rm” | base64 –decode<br>1f2d1e2e67df<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">## 使用Secrets</div><div class=\"line\">### 手动为pod绑定secret</div><div class=\"line\">必须有spec.volumes才能使用secret。 如果一个pod中有多个container，每个container需要他们单独对应的volumeMounts ，但是一个secret只能对应一个spec.volumes。</div></pre></td></tr></table></figure></p>\n<p>{<br> “apiVersion”: “v1”,<br> “kind”: “Pod”,<br>  “metadata”: {<br>    “name”: “mypod”,<br>    “namespace”: “myns”<br>  },<br>  “spec”: {<br>    “containers”: [{<br>      “name”: “mypod”,<br>      “image”: “redis”,<br>      “volumeMounts”: [{<br>        “name”: “foo”,<br>        “mountPath”: “/etc/foo”,<br>        “readOnly”: true<br>      }]<br>    }],<br>    “volumes”: [{<br>      “name”: “foo”,<br>      “secret”: {<br>        “secretName”: “mysecret”<br>      }<br>    }]<br>  }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看Secrets的值:</div></pre></td></tr></table></figure></p>\n<p>$ ls /etc/foo/<br>username<br>password<br>$ cat /etc/foo/username<br>admin<br>$ cat /etc/foo/password<br>1f2d1e2e67df<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">### 使用Secrets作为环境变量</div><div class=\"line\">&gt;  定义对Secrets的引用: env[x].valueFrom.secretKeyRef.</div></pre></td></tr></table></figure></p>\n<p>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secret-env-pod<br>spec:<br>  containers:</p>\n<pre><code>- name: mycontainer\n  image: redis\n  env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: mysecret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysecret\n          key: password\n</code></pre><p>  restartPolicy: Never<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看Secrets的值:</div></pre></td></tr></table></figure></p>\n<p>$ echo $SECRET_USERNAME<br>admin<br>$ echo $SECRET_PASSWORD<br>1f2d1e2e67df<br>```</p>\n<h2 id=\"Secret与Pod的生命周期\"><a href=\"#Secret与Pod的生命周期\" class=\"headerlink\" title=\"Secret与Pod的生命周期\"></a>Secret与Pod的生命周期</h2><p>当通过api创建一个pod后，不会去检查所引用的secret是否存在。一旦这个pod被使用，kubelet将会尝试去获取引用的secret的值。如果这个secret不存在，或者kubelet暂时链接不上apiserver，kubelet将会定期重试，并发送一个event来解释pod没有启动的原因。如果获取到了对应的secret，kubelet将会创建对应的volume并绑定到container。</p>\n<p>一旦kubelet创建了一个pod，则container使用的相关secret volume不会在改变，即使对应的secret对象被修改。如果为了改变使用的secret，则必须删除旧的pod，并重新创建一个新的pod。</p>\n<p>##限制<br>在使用之前，Secret volume 资源被验证，以确保指定的对象引用真是指向一个secret对象。因此，在pod使用它之前必须保证需要的secret被成功创建。Secret api对象从属于namespace，一个Secret对象只能被同namespace的pod所使用。</p>\n<p>单个secret限制在1Mb之内，防止过大的secret耗尽apiserver &amp; kubelet的内存。然而，创建许多类似的secret同样也会无用的消耗掉apiserver&amp;kubelet的内存。</p>\n<p>kubelet目前只支持pod使用来自于apiserver的secret。pods包括了被 kubectl创建的pod 或者 被replication controller间接创建的。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Secrets描述\"><a href=\"#Secrets描述\" class=\"headerlink\" title=\"Secrets描述\"></a>Secrets描述</h2><p>在Kubernetes中，Secret对象类型主要目的是保存一些私密数据，比如密码, tokens, ssh keys等信息。将这些信息放在Secret对象中比直接放在pod或docker image中更安全，也更方便使用。</p>\n<p>创建Secrets对象的方式有两种，一种是用户手动创建，另一种是集群自动创建。<br>一个已经创建好的Secrets对象有两种方式被pod对象使用，其一，在container中的volume对象里以file的形式被使用，其二，在pull images时被kubelet使用。</p>\n<p>为了使用Secret对象，pod必须引用这个Secret，同样可以手动或者自动来执行引用操作。</p>\n<h2 id=\"Built-in-Secrets\"><a href=\"#Built-in-Secrets\" class=\"headerlink\" title=\"Built-in Secrets\"></a>Built-in Secrets</h2><p>Kubernetes会自动创建包含证书信息的Secret，并且使用它来访问api, Kubernetes也将自动修改pod来使用这个Secret。</p>\n<p>自动创建的Secret以及所使用的api证书,可以根据需要disable或者override。如果仅仅需要安全访问apiserver，那么上述的流程是推荐的方式。</p>\n<h2 id=\"自定义Secrets\"><a href=\"#自定义Secrets\" class=\"headerlink\" title=\"自定义Secrets\"></a>自定义Secrets</h2><h3 id=\"通过kubectl自定义Secrets\"><a href=\"#通过kubectl自定义Secrets\" class=\"headerlink\" title=\"通过kubectl自定义Secrets\"></a>通过kubectl自定义Secrets</h3><p>数据中的字段为map类型。其中keys必须符合dns_subdomain规则，values可以为任意类型，使用base64编码。上述例子中，username和password的数据值在base64编码前的值为value-1 和 value-2。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;admin&quot; &gt; ./username.txt</div><div class=\"line\">xis-macbook-pro:~ xiningwang$  echo -n &quot;1f2d1e2e67df&quot; &gt; ./password.txt</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt</div><div class=\"line\">secret &quot;db-user-pass&quot; created</div></pre></td></tr></table></figure></p>\n<p>查看创建的Secrets:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl get secrets</div><div class=\"line\">NAME                  TYPE                                  DATA      AGE</div><div class=\"line\">db-user-pass          Opaque                                2         6m</div><div class=\"line\">default-token-t5bk8   kubernetes.io/service-account-token   3         4d</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl describe secrets/db-user-pass</div><div class=\"line\">Name:  \t\tdb-user-pass</div><div class=\"line\">Namespace:     \tdefault</div><div class=\"line\">Labels:\t\t&lt;none&gt;</div><div class=\"line\">Annotations:   \t&lt;none&gt;</div><div class=\"line\"></div><div class=\"line\">Type:  \tOpaque</div><div class=\"line\"></div><div class=\"line\">Data</div><div class=\"line\">====</div><div class=\"line\">password.txt:  \t12 bytes</div><div class=\"line\">username.txt:  \t5 bytes</div></pre></td></tr></table></figure></p>\n<h3 id=\"手工创建Secrets\"><a href=\"#手工创建Secrets\" class=\"headerlink\" title=\"手工创建Secrets\"></a>手工创建Secrets</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;admin&quot; | base64</div><div class=\"line\">YWRtaW4=</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ echo -n &quot;1f2d1e2e67df&quot; | base64</div><div class=\"line\">MWYyZDFlMmU2N2Rm</div><div class=\"line\">xis-macbook-pro:~ xiningwang$ vi mysecrect.yaml</div><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Secret</div><div class=\"line\">metadata:</div><div class=\"line\">  name: mysecret</div><div class=\"line\">type: Opaque</div><div class=\"line\">data:</div><div class=\"line\">  username: YWRtaW4=</div><div class=\"line\">  password: MWYyZDFlMmU2N2Rm</div></pre></td></tr></table></figure>\n<p>使用yaml文件创建Secrets:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">xis-macbook-pro:~ xiningwang$ kubectl create -f ./mysecrect.yaml</div><div class=\"line\">secret &quot;mysecret&quot; created</div><div class=\"line\">```   </div><div class=\"line\"></div><div class=\"line\">## 查看Secrets</div><div class=\"line\">使用如下命令获取创建的Secrets内容:</div></pre></td></tr></table></figure></p>\n<p>xis-macbook-pro:~ xiningwang$ kubectl get secret db-user-pass -o yaml<br>apiVersion: v1<br>data:<br>  password.txt: MWYyZDFlMmU2N2Rm<br>  username.txt: YWRtaW4=<br>kind: Secret<br>metadata:<br>  creationTimestamp: 2017-05-09T07:54:29Z<br>  name: db-user-pass<br>  namespace: default<br>  resourceVersion: “84609”<br>  selfLink: /api/v1/namespaces/default/secrets/db-user-pass<br>  uid: bb23a365-348c-11e7-ab36-080027fd8883<br>type: Opaque<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">base64解密:</div></pre></td></tr></table></figure></p>\n<p>$ echo “MWYyZDFlMmU2N2Rm” | base64 –decode<br>1f2d1e2e67df<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">## 使用Secrets</div><div class=\"line\">### 手动为pod绑定secret</div><div class=\"line\">必须有spec.volumes才能使用secret。 如果一个pod中有多个container，每个container需要他们单独对应的volumeMounts ，但是一个secret只能对应一个spec.volumes。</div></pre></td></tr></table></figure></p>\n<p>{<br> “apiVersion”: “v1”,<br> “kind”: “Pod”,<br>  “metadata”: {<br>    “name”: “mypod”,<br>    “namespace”: “myns”<br>  },<br>  “spec”: {<br>    “containers”: [{<br>      “name”: “mypod”,<br>      “image”: “redis”,<br>      “volumeMounts”: [{<br>        “name”: “foo”,<br>        “mountPath”: “/etc/foo”,<br>        “readOnly”: true<br>      }]<br>    }],<br>    “volumes”: [{<br>      “name”: “foo”,<br>      “secret”: {<br>        “secretName”: “mysecret”<br>      }<br>    }]<br>  }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看Secrets的值:</div></pre></td></tr></table></figure></p>\n<p>$ ls /etc/foo/<br>username<br>password<br>$ cat /etc/foo/username<br>admin<br>$ cat /etc/foo/password<br>1f2d1e2e67df<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">### 使用Secrets作为环境变量</div><div class=\"line\">&gt;  定义对Secrets的引用: env[x].valueFrom.secretKeyRef.</div></pre></td></tr></table></figure></p>\n<p>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: secret-env-pod<br>spec:<br>  containers:</p>\n<pre><code>- name: mycontainer\n  image: redis\n  env:\n    - name: SECRET_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: mysecret\n          key: username\n    - name: SECRET_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysecret\n          key: password\n</code></pre><p>  restartPolicy: Never<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看Secrets的值:</div></pre></td></tr></table></figure></p>\n<p>$ echo $SECRET_USERNAME<br>admin<br>$ echo $SECRET_PASSWORD<br>1f2d1e2e67df<br>```</p>\n<h2 id=\"Secret与Pod的生命周期\"><a href=\"#Secret与Pod的生命周期\" class=\"headerlink\" title=\"Secret与Pod的生命周期\"></a>Secret与Pod的生命周期</h2><p>当通过api创建一个pod后，不会去检查所引用的secret是否存在。一旦这个pod被使用，kubelet将会尝试去获取引用的secret的值。如果这个secret不存在，或者kubelet暂时链接不上apiserver，kubelet将会定期重试，并发送一个event来解释pod没有启动的原因。如果获取到了对应的secret，kubelet将会创建对应的volume并绑定到container。</p>\n<p>一旦kubelet创建了一个pod，则container使用的相关secret volume不会在改变，即使对应的secret对象被修改。如果为了改变使用的secret，则必须删除旧的pod，并重新创建一个新的pod。</p>\n<p>##限制<br>在使用之前，Secret volume 资源被验证，以确保指定的对象引用真是指向一个secret对象。因此，在pod使用它之前必须保证需要的secret被成功创建。Secret api对象从属于namespace，一个Secret对象只能被同namespace的pod所使用。</p>\n<p>单个secret限制在1Mb之内，防止过大的secret耗尽apiserver &amp; kubelet的内存。然而，创建许多类似的secret同样也会无用的消耗掉apiserver&amp;kubelet的内存。</p>\n<p>kubelet目前只支持pod使用来自于apiserver的secret。pods包括了被 kubectl创建的pod 或者 被replication controller间接创建的。</p>\n"},{"title":"Kubernetes ConfigMap","date":"2017-03-09T12:46:25.000Z","_content":"\n## 相关术语\n很多应用程序的配置需要通过配置文件，命令行参数和环境变量的组合配置来完成。这些配置应该从image内容中解耦，以此来保持容器化应用程序的便携性。ConfigMap API资源提供了将配置数据注入容器的方式，同时保持容器是不知道Kubernetes的。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。\n\n>kubernetes通过ConfigMap来实现对容器中应用的配置管理。\n\n从数据角度来看，ConfigMap的类型只是键值组。应用可以从不同角度来配置，所以关于给用户如何存储和使用配置数据，我们需要给他们一些弹性。在一个pod里面使用ConfigMap大致有三种方式：\n- 环境变量\n- 命令行参数\n- 数据卷文件\n\n## 创建ConfigMap\n创建ConfigMap的方式有两种，一种是通过yaml文件来创建，另一种是通过kubectl直接在命令行下创建。\n\n### yaml\n在yaml文件中，配置文件以key-value键值对的形式保存，当然也可以直接放一个完整的配置文件，在下面的示例中，cache_hst、cache_port、cache_prefix即是key-value键值对，而app.properties和my.cnf都是配置文件：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-cfg\n  namespace: default\ndata:\n  cache_host: memcached-gcxt\n  cache_port: \"11211\"\n  cache_prefix: gcxt\n  my.cnf: |\n    [mysqld]\n    log-bin = mysql-bin\n  app.properties: |\n    property.1 = value-1\n property.2 = value-2\n property.3 = value-3\n ```\n\n创建ConfigMap：\n```\nkubectl create -f test-cfg.yml\n```\n\n### kubectl\n直接将一个目录下的所有配置文件创建为一个ConfigMap：\n```\nkubectl create configmap test-config --from-file=./configs\n```\n\n直接将一个配置文件创建为一个ConfigMap：\n```\nkubectl create configmap test-config2 --from-file=./configs/db.conf --from-file=./configs/cache.conf\n```\n\n在使用kubectl创建的时候，通过在命令行直接传递键值对创建：\n```\nkubectl create configmap test-config3 --from-literal=db.host=10.5.10.116 --from-listeral=db.port='3306'\n```\n可以通过如下方式查看创建的ConfigMap：\n```\nkubectl get configmaps\nkubectl get configmap test-config -o yaml\nkubectl describe configmap test-config\n```\n\n## 使用ConfigMap\n### 环境变量\n通过环境变量的方式，直接传递pod.\nConfigMap文件：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n第一个pod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n第二个pod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: CACHE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: test-cfg\n              key: cache_host\n              optional: true\n  restartPolicy: Never\n```\n\n### 命令行参数\n在命令行下引用时，需要先设置为环境变量，之后 可以通过$(VAR_NAME)设置容器启动命令的启动参数.\nConfigMap文件示例：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\nPod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n### 数据卷文件\n使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容.\nConfigMap示例：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n第一个pod示例，简单的将上面创建的ConfigMap直接挂载至pod的/etc/config目录下：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"cat /etc/config/special.how\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n  restartPolicy: Never\n```\n\n第二个pod示例，只将ConfigMap的special.how这个key挂载到/etc/config目录下的一个相对路径path/to/special-key，如果存在同名文件，直接覆盖。其他的key不挂载：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\",\"-c\",\"cat /etc/config/path/to/special-key\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n        items:\n        - key: special.how\n          path: path/to/special-key\n  restartPolicy: Never\n```\n\n## Note\n- ConfigMap必须在Pod之前创建\n- 只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。\n- 很多生产环境中的应用程序配置较为复杂，可能需要多个config文件、命令行参数和环境变量的组合。并且，这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。社区引入ConfigMap这个API资源来满足这一需求。\n- ConfigMap包含了一系列的键值对，用于存储被Pod或者系统组件（如controller）访问的信息。这与secret的设计理念有异曲同工之妙，它们的主要区别在于ConfigMap通常不用于存储敏感信息，而只存储简单的文本信息。\n","source":"_posts/kubernetes-configmap.md","raw":"---\ntitle: Kubernetes ConfigMap\ndate: 2017-3-9 20:46:25\ncategories:\n  - 分布式&云计算\n  - Kubernetes\ntags:\n  - 分布式\n  - Kubernetes\n  - 路由\n  - 容器\n  - ConfigMap\n---\n\n## 相关术语\n很多应用程序的配置需要通过配置文件，命令行参数和环境变量的组合配置来完成。这些配置应该从image内容中解耦，以此来保持容器化应用程序的便携性。ConfigMap API资源提供了将配置数据注入容器的方式，同时保持容器是不知道Kubernetes的。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。\n\n>kubernetes通过ConfigMap来实现对容器中应用的配置管理。\n\n从数据角度来看，ConfigMap的类型只是键值组。应用可以从不同角度来配置，所以关于给用户如何存储和使用配置数据，我们需要给他们一些弹性。在一个pod里面使用ConfigMap大致有三种方式：\n- 环境变量\n- 命令行参数\n- 数据卷文件\n\n## 创建ConfigMap\n创建ConfigMap的方式有两种，一种是通过yaml文件来创建，另一种是通过kubectl直接在命令行下创建。\n\n### yaml\n在yaml文件中，配置文件以key-value键值对的形式保存，当然也可以直接放一个完整的配置文件，在下面的示例中，cache_hst、cache_port、cache_prefix即是key-value键值对，而app.properties和my.cnf都是配置文件：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: test-cfg\n  namespace: default\ndata:\n  cache_host: memcached-gcxt\n  cache_port: \"11211\"\n  cache_prefix: gcxt\n  my.cnf: |\n    [mysqld]\n    log-bin = mysql-bin\n  app.properties: |\n    property.1 = value-1\n property.2 = value-2\n property.3 = value-3\n ```\n\n创建ConfigMap：\n```\nkubectl create -f test-cfg.yml\n```\n\n### kubectl\n直接将一个目录下的所有配置文件创建为一个ConfigMap：\n```\nkubectl create configmap test-config --from-file=./configs\n```\n\n直接将一个配置文件创建为一个ConfigMap：\n```\nkubectl create configmap test-config2 --from-file=./configs/db.conf --from-file=./configs/cache.conf\n```\n\n在使用kubectl创建的时候，通过在命令行直接传递键值对创建：\n```\nkubectl create configmap test-config3 --from-literal=db.host=10.5.10.116 --from-listeral=db.port='3306'\n```\n可以通过如下方式查看创建的ConfigMap：\n```\nkubectl get configmaps\nkubectl get configmap test-config -o yaml\nkubectl describe configmap test-config\n```\n\n## 使用ConfigMap\n### 环境变量\n通过环境变量的方式，直接传递pod.\nConfigMap文件：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n第一个pod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n第二个pod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      env:\n        - name: CACHE_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: test-cfg\n              key: cache_host\n              optional: true\n  restartPolicy: Never\n```\n\n### 命令行参数\n在命令行下引用时，需要先设置为环境变量，之后 可以通过$(VAR_NAME)设置容器启动命令的启动参数.\nConfigMap文件示例：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\nPod示例：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ]\n      env:\n        - name: SPECIAL_LEVEL_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.how\n        - name: SPECIAL_TYPE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: special-config\n              key: special.type\n  restartPolicy: Never\n```\n\n### 数据卷文件\n使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容.\nConfigMap示例：\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: special-config\n  namespace: default\ndata:\n  special.how: very\n  special.type: charm\n```\n\n第一个pod示例，简单的将上面创建的ConfigMap直接挂载至pod的/etc/config目录下：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\", \"-c\", \"cat /etc/config/special.how\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n  restartPolicy: Never\n```\n\n第二个pod示例，只将ConfigMap的special.how这个key挂载到/etc/config目录下的一个相对路径path/to/special-key，如果存在同名文件，直接覆盖。其他的key不挂载：\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dapi-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: gcr.io/google_containers/busybox\n      command: [ \"/bin/sh\",\"-c\",\"cat /etc/config/path/to/special-key\" ]\n      volumeMounts:\n      - name: config-volume\n        mountPath: /etc/config\n  volumes:\n    - name: config-volume\n      configMap:\n        name: special-config\n        items:\n        - key: special.how\n          path: path/to/special-key\n  restartPolicy: Never\n```\n\n## Note\n- ConfigMap必须在Pod之前创建\n- 只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。\n- 很多生产环境中的应用程序配置较为复杂，可能需要多个config文件、命令行参数和环境变量的组合。并且，这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。社区引入ConfigMap这个API资源来满足这一需求。\n- ConfigMap包含了一系列的键值对，用于存储被Pod或者系统组件（如controller）访问的信息。这与secret的设计理念有异曲同工之妙，它们的主要区别在于ConfigMap通常不用于存储敏感信息，而只存储简单的文本信息。\n","slug":"kubernetes-configmap","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bg7000oi2723g1myplz","content":"<h2 id=\"相关术语\"><a href=\"#相关术语\" class=\"headerlink\" title=\"相关术语\"></a>相关术语</h2><p>很多应用程序的配置需要通过配置文件，命令行参数和环境变量的组合配置来完成。这些配置应该从image内容中解耦，以此来保持容器化应用程序的便携性。ConfigMap API资源提供了将配置数据注入容器的方式，同时保持容器是不知道Kubernetes的。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。</p>\n<blockquote>\n<p>kubernetes通过ConfigMap来实现对容器中应用的配置管理。</p>\n</blockquote>\n<p>从数据角度来看，ConfigMap的类型只是键值组。应用可以从不同角度来配置，所以关于给用户如何存储和使用配置数据，我们需要给他们一些弹性。在一个pod里面使用ConfigMap大致有三种方式：</p>\n<ul>\n<li>环境变量</li>\n<li>命令行参数</li>\n<li>数据卷文件</li>\n</ul>\n<h2 id=\"创建ConfigMap\"><a href=\"#创建ConfigMap\" class=\"headerlink\" title=\"创建ConfigMap\"></a>创建ConfigMap</h2><p>创建ConfigMap的方式有两种，一种是通过yaml文件来创建，另一种是通过kubectl直接在命令行下创建。</p>\n<h3 id=\"yaml\"><a href=\"#yaml\" class=\"headerlink\" title=\"yaml\"></a>yaml</h3><p>在yaml文件中，配置文件以key-value键值对的形式保存，当然也可以直接放一个完整的配置文件，在下面的示例中，cache_hst、cache_port、cache_prefix即是key-value键值对，而app.properties和my.cnf都是配置文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-cfg</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  cache_host: memcached-gcxt</div><div class=\"line\">  cache_port: &quot;11211&quot;</div><div class=\"line\">  cache_prefix: gcxt</div><div class=\"line\">  my.cnf: |</div><div class=\"line\">    [mysqld]</div><div class=\"line\">    log-bin = mysql-bin</div><div class=\"line\">  app.properties: |</div><div class=\"line\">    property.1 = value-1</div><div class=\"line\"> property.2 = value-2</div><div class=\"line\"> property.3 = value-3</div></pre></td></tr></table></figure></p>\n<p>创建ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create -f test-cfg.yml</div></pre></td></tr></table></figure></p>\n<h3 id=\"kubectl\"><a href=\"#kubectl\" class=\"headerlink\" title=\"kubectl\"></a>kubectl</h3><p>直接将一个目录下的所有配置文件创建为一个ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config --from-file=./configs</div></pre></td></tr></table></figure></p>\n<p>直接将一个配置文件创建为一个ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config2 --from-file=./configs/db.conf --from-file=./configs/cache.conf</div></pre></td></tr></table></figure></p>\n<p>在使用kubectl创建的时候，通过在命令行直接传递键值对创建：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config3 --from-literal=db.host=10.5.10.116 --from-listeral=db.port=&apos;3306&apos;</div></pre></td></tr></table></figure></p>\n<p>可以通过如下方式查看创建的ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl get configmaps</div><div class=\"line\">kubectl get configmap test-config -o yaml</div><div class=\"line\">kubectl describe configmap test-config</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用ConfigMap\"><a href=\"#使用ConfigMap\" class=\"headerlink\" title=\"使用ConfigMap\"></a>使用ConfigMap</h2><h3 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h3><p>通过环境变量的方式，直接传递pod.<br>ConfigMap文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>第一个pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: SPECIAL_LEVEL_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.how</div><div class=\"line\">        - name: SPECIAL_TYPE_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.type</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<p>第二个pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: CACHE_HOST</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: test-cfg</div><div class=\"line\">              key: cache_host</div><div class=\"line\">              optional: true</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h3 id=\"命令行参数\"><a href=\"#命令行参数\" class=\"headerlink\" title=\"命令行参数\"></a>命令行参数</h3><p>在命令行下引用时，需要先设置为环境变量，之后 可以通过$(VAR_NAME)设置容器启动命令的启动参数.<br>ConfigMap文件示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>Pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: SPECIAL_LEVEL_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.how</div><div class=\"line\">        - name: SPECIAL_TYPE_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.type</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h3 id=\"数据卷文件\"><a href=\"#数据卷文件\" class=\"headerlink\" title=\"数据卷文件\"></a>数据卷文件</h3><p>使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容.<br>ConfigMap示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>第一个pod示例，简单的将上面创建的ConfigMap直接挂载至pod的/etc/config目录下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot; ]</div><div class=\"line\">      volumeMounts:</div><div class=\"line\">      - name: config-volume</div><div class=\"line\">        mountPath: /etc/config</div><div class=\"line\">  volumes:</div><div class=\"line\">    - name: config-volume</div><div class=\"line\">      configMap:</div><div class=\"line\">        name: special-config</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<p>第二个pod示例，只将ConfigMap的special.how这个key挂载到/etc/config目录下的一个相对路径path/to/special-key，如果存在同名文件，直接覆盖。其他的key不挂载：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/path/to/special-key&quot; ]</div><div class=\"line\">      volumeMounts:</div><div class=\"line\">      - name: config-volume</div><div class=\"line\">        mountPath: /etc/config</div><div class=\"line\">  volumes:</div><div class=\"line\">    - name: config-volume</div><div class=\"line\">      configMap:</div><div class=\"line\">        name: special-config</div><div class=\"line\">        items:</div><div class=\"line\">        - key: special.how</div><div class=\"line\">          path: path/to/special-key</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h2 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h2><ul>\n<li>ConfigMap必须在Pod之前创建</li>\n<li>只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。</li>\n<li>很多生产环境中的应用程序配置较为复杂，可能需要多个config文件、命令行参数和环境变量的组合。并且，这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。社区引入ConfigMap这个API资源来满足这一需求。</li>\n<li>ConfigMap包含了一系列的键值对，用于存储被Pod或者系统组件（如controller）访问的信息。这与secret的设计理念有异曲同工之妙，它们的主要区别在于ConfigMap通常不用于存储敏感信息，而只存储简单的文本信息。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"相关术语\"><a href=\"#相关术语\" class=\"headerlink\" title=\"相关术语\"></a>相关术语</h2><p>很多应用程序的配置需要通过配置文件，命令行参数和环境变量的组合配置来完成。这些配置应该从image内容中解耦，以此来保持容器化应用程序的便携性。ConfigMap API资源提供了将配置数据注入容器的方式，同时保持容器是不知道Kubernetes的。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象。</p>\n<blockquote>\n<p>kubernetes通过ConfigMap来实现对容器中应用的配置管理。</p>\n</blockquote>\n<p>从数据角度来看，ConfigMap的类型只是键值组。应用可以从不同角度来配置，所以关于给用户如何存储和使用配置数据，我们需要给他们一些弹性。在一个pod里面使用ConfigMap大致有三种方式：</p>\n<ul>\n<li>环境变量</li>\n<li>命令行参数</li>\n<li>数据卷文件</li>\n</ul>\n<h2 id=\"创建ConfigMap\"><a href=\"#创建ConfigMap\" class=\"headerlink\" title=\"创建ConfigMap\"></a>创建ConfigMap</h2><p>创建ConfigMap的方式有两种，一种是通过yaml文件来创建，另一种是通过kubectl直接在命令行下创建。</p>\n<h3 id=\"yaml\"><a href=\"#yaml\" class=\"headerlink\" title=\"yaml\"></a>yaml</h3><p>在yaml文件中，配置文件以key-value键值对的形式保存，当然也可以直接放一个完整的配置文件，在下面的示例中，cache_hst、cache_port、cache_prefix即是key-value键值对，而app.properties和my.cnf都是配置文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: test-cfg</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  cache_host: memcached-gcxt</div><div class=\"line\">  cache_port: &quot;11211&quot;</div><div class=\"line\">  cache_prefix: gcxt</div><div class=\"line\">  my.cnf: |</div><div class=\"line\">    [mysqld]</div><div class=\"line\">    log-bin = mysql-bin</div><div class=\"line\">  app.properties: |</div><div class=\"line\">    property.1 = value-1</div><div class=\"line\"> property.2 = value-2</div><div class=\"line\"> property.3 = value-3</div></pre></td></tr></table></figure></p>\n<p>创建ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create -f test-cfg.yml</div></pre></td></tr></table></figure></p>\n<h3 id=\"kubectl\"><a href=\"#kubectl\" class=\"headerlink\" title=\"kubectl\"></a>kubectl</h3><p>直接将一个目录下的所有配置文件创建为一个ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config --from-file=./configs</div></pre></td></tr></table></figure></p>\n<p>直接将一个配置文件创建为一个ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config2 --from-file=./configs/db.conf --from-file=./configs/cache.conf</div></pre></td></tr></table></figure></p>\n<p>在使用kubectl创建的时候，通过在命令行直接传递键值对创建：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl create configmap test-config3 --from-literal=db.host=10.5.10.116 --from-listeral=db.port=&apos;3306&apos;</div></pre></td></tr></table></figure></p>\n<p>可以通过如下方式查看创建的ConfigMap：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">kubectl get configmaps</div><div class=\"line\">kubectl get configmap test-config -o yaml</div><div class=\"line\">kubectl describe configmap test-config</div></pre></td></tr></table></figure></p>\n<h2 id=\"使用ConfigMap\"><a href=\"#使用ConfigMap\" class=\"headerlink\" title=\"使用ConfigMap\"></a>使用ConfigMap</h2><h3 id=\"环境变量\"><a href=\"#环境变量\" class=\"headerlink\" title=\"环境变量\"></a>环境变量</h3><p>通过环境变量的方式，直接传递pod.<br>ConfigMap文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>第一个pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: SPECIAL_LEVEL_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.how</div><div class=\"line\">        - name: SPECIAL_TYPE_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.type</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<p>第二个pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: CACHE_HOST</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: test-cfg</div><div class=\"line\">              key: cache_host</div><div class=\"line\">              optional: true</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h3 id=\"命令行参数\"><a href=\"#命令行参数\" class=\"headerlink\" title=\"命令行参数\"></a>命令行参数</h3><p>在命令行下引用时，需要先设置为环境变量，之后 可以通过$(VAR_NAME)设置容器启动命令的启动参数.<br>ConfigMap文件示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>Pod示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)&quot; ]</div><div class=\"line\">      env:</div><div class=\"line\">        - name: SPECIAL_LEVEL_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.how</div><div class=\"line\">        - name: SPECIAL_TYPE_KEY</div><div class=\"line\">          valueFrom:</div><div class=\"line\">            configMapKeyRef:</div><div class=\"line\">              name: special-config</div><div class=\"line\">              key: special.type</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h3 id=\"数据卷文件\"><a href=\"#数据卷文件\" class=\"headerlink\" title=\"数据卷文件\"></a>数据卷文件</h3><p>使用volume将ConfigMap作为文件或目录直接挂载，其中每一个key-value键值对都会生成一个文件，key为文件名，value为内容.<br>ConfigMap示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: ConfigMap</div><div class=\"line\">metadata:</div><div class=\"line\">  name: special-config</div><div class=\"line\">  namespace: default</div><div class=\"line\">data:</div><div class=\"line\">  special.how: very</div><div class=\"line\">  special.type: charm</div></pre></td></tr></table></figure></p>\n<p>第一个pod示例，简单的将上面创建的ConfigMap直接挂载至pod的/etc/config目录下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/special.how&quot; ]</div><div class=\"line\">      volumeMounts:</div><div class=\"line\">      - name: config-volume</div><div class=\"line\">        mountPath: /etc/config</div><div class=\"line\">  volumes:</div><div class=\"line\">    - name: config-volume</div><div class=\"line\">      configMap:</div><div class=\"line\">        name: special-config</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<p>第二个pod示例，只将ConfigMap的special.how这个key挂载到/etc/config目录下的一个相对路径path/to/special-key，如果存在同名文件，直接覆盖。其他的key不挂载：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">apiVersion: v1</div><div class=\"line\">kind: Pod</div><div class=\"line\">metadata:</div><div class=\"line\">  name: dapi-test-pod</div><div class=\"line\">spec:</div><div class=\"line\">  containers:</div><div class=\"line\">    - name: test-container</div><div class=\"line\">      image: gcr.io/google_containers/busybox</div><div class=\"line\">      command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/path/to/special-key&quot; ]</div><div class=\"line\">      volumeMounts:</div><div class=\"line\">      - name: config-volume</div><div class=\"line\">        mountPath: /etc/config</div><div class=\"line\">  volumes:</div><div class=\"line\">    - name: config-volume</div><div class=\"line\">      configMap:</div><div class=\"line\">        name: special-config</div><div class=\"line\">        items:</div><div class=\"line\">        - key: special.how</div><div class=\"line\">          path: path/to/special-key</div><div class=\"line\">  restartPolicy: Never</div></pre></td></tr></table></figure></p>\n<h2 id=\"Note\"><a href=\"#Note\" class=\"headerlink\" title=\"Note\"></a>Note</h2><ul>\n<li>ConfigMap必须在Pod之前创建</li>\n<li>只有与当前ConfigMap在同一个namespace内的pod才能使用这个ConfigMap，换句话说，ConfigMap不能跨命名空间调用。</li>\n<li>很多生产环境中的应用程序配置较为复杂，可能需要多个config文件、命令行参数和环境变量的组合。并且，这些配置信息应该从应用程序镜像中解耦出来，以保证镜像的可移植性以及配置信息不被泄露。社区引入ConfigMap这个API资源来满足这一需求。</li>\n<li>ConfigMap包含了一系列的键值对，用于存储被Pod或者系统组件（如controller）访问的信息。这与secret的设计理念有异曲同工之妙，它们的主要区别在于ConfigMap通常不用于存储敏感信息，而只存储简单的文本信息。</li>\n</ul>\n"},{"title":"Mongo","date":"2017-01-09T12:46:25.000Z","_content":"### 1. mongo docker image\nStart the Database\n```\ndocker run --name mymongo -d mongo:<label> --auth\n```\n\nAdd the Initial Admin User\n```\n$ docker exec -it mymongo mongo admin\nconnecting to: admin\n> db.createUser({ user: 'jsmith', pwd: 'some-initial-password', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] });\nSuccessfully added user: {\n    \"user\" : \"jsmith\",\n    \"roles\" : [\n        {\n            \"role\" : \"userAdminAnyDatabase\",\n            \"db\" : \"admin\"\n        }\n    ]\n}\n```\n### 2. mongo backup and restore\n","source":"_posts/mongo.md","raw":"---\ntitle: Mongo\ndate: 2017-1-9 20:46:25\n---\n### 1. mongo docker image\nStart the Database\n```\ndocker run --name mymongo -d mongo:<label> --auth\n```\n\nAdd the Initial Admin User\n```\n$ docker exec -it mymongo mongo admin\nconnecting to: admin\n> db.createUser({ user: 'jsmith', pwd: 'some-initial-password', roles: [ { role: \"userAdminAnyDatabase\", db: \"admin\" } ] });\nSuccessfully added user: {\n    \"user\" : \"jsmith\",\n    \"roles\" : [\n        {\n            \"role\" : \"userAdminAnyDatabase\",\n            \"db\" : \"admin\"\n        }\n    ]\n}\n```\n### 2. mongo backup and restore\n","slug":"mongo","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bg9000pi27288mwko2f","content":"<h3 id=\"1-mongo-docker-image\"><a href=\"#1-mongo-docker-image\" class=\"headerlink\" title=\"1. mongo docker image\"></a>1. mongo docker image</h3><p>Start the Database<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run --name mymongo -d mongo:&lt;label&gt; --auth</div></pre></td></tr></table></figure></p>\n<p>Add the Initial Admin User<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ docker exec -it mymongo mongo admin</div><div class=\"line\">connecting to: admin</div><div class=\"line\">&gt; db.createUser(&#123; user: &apos;jsmith&apos;, pwd: &apos;some-initial-password&apos;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</div><div class=\"line\">Successfully added user: &#123;</div><div class=\"line\">    &quot;user&quot; : &quot;jsmith&quot;,</div><div class=\"line\">    &quot;roles&quot; : [</div><div class=\"line\">        &#123;</div><div class=\"line\">            &quot;role&quot; : &quot;userAdminAnyDatabase&quot;,</div><div class=\"line\">            &quot;db&quot; : &quot;admin&quot;</div><div class=\"line\">        &#125;</div><div class=\"line\">    ]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"2-mongo-backup-and-restore\"><a href=\"#2-mongo-backup-and-restore\" class=\"headerlink\" title=\"2. mongo backup and restore\"></a>2. mongo backup and restore</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-mongo-docker-image\"><a href=\"#1-mongo-docker-image\" class=\"headerlink\" title=\"1. mongo docker image\"></a>1. mongo docker image</h3><p>Start the Database<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">docker run --name mymongo -d mongo:&lt;label&gt; --auth</div></pre></td></tr></table></figure></p>\n<p>Add the Initial Admin User<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ docker exec -it mymongo mongo admin</div><div class=\"line\">connecting to: admin</div><div class=\"line\">&gt; db.createUser(&#123; user: &apos;jsmith&apos;, pwd: &apos;some-initial-password&apos;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</div><div class=\"line\">Successfully added user: &#123;</div><div class=\"line\">    &quot;user&quot; : &quot;jsmith&quot;,</div><div class=\"line\">    &quot;roles&quot; : [</div><div class=\"line\">        &#123;</div><div class=\"line\">            &quot;role&quot; : &quot;userAdminAnyDatabase&quot;,</div><div class=\"line\">            &quot;db&quot; : &quot;admin&quot;</div><div class=\"line\">        &#125;</div><div class=\"line\">    ]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"2-mongo-backup-and-restore\"><a href=\"#2-mongo-backup-and-restore\" class=\"headerlink\" title=\"2. mongo backup and restore\"></a>2. mongo backup and restore</h3>"},{"title":"Redis","date":"2017-03-02T12:46:25.000Z","_content":"\ndocker run --name myredis -v /Users/xiningwang/nosql/redis/redis-3.2.8/data:/data -p 6379:6379 -d redis redis-server --appendonly yes\n","source":"_posts/redis.md","raw":"---\ntitle: Redis\ndate: 2017-3-2 20:46:25\n---\n\ndocker run --name myredis -v /Users/xiningwang/nosql/redis/redis-3.2.8/data:/data -p 6379:6379 -d redis redis-server --appendonly yes\n","slug":"redis","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bga000si27258bq6gzp","content":"<p>docker run –name myredis -v /Users/xiningwang/nosql/redis/redis-3.2.8/data:/data -p 6379:6379 -d redis redis-server –appendonly yes</p>\n","site":{"data":{}},"excerpt":"","more":"<p>docker run –name myredis -v /Users/xiningwang/nosql/redis/redis-3.2.8/data:/data -p 6379:6379 -d redis redis-server –appendonly yes</p>\n"},{"title":"Running Spark on YARN","date":"2016-03-09T12:46:25.000Z","_content":"### To launch a Spark application in cluster/yarn mode:\n```\n$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\n```\n\nFor example:\n```\n$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --driver-memory 4g \\\n    --executor-memory 2g \\\n    --executor-cores 1 \\\n    --queue thequeue \\\n    lib/spark-examples*.jar \\\n    10\n```\n\nSee the detail from:\nhttp://spark.apache.org/docs/latest/running-on-yarn.html\n","source":"_posts/running-spark-on-yarn.md","raw":"---\ntitle: Running Spark on YARN\ndate: 2016-3-9 20:46:25\n---\n### To launch a Spark application in cluster/yarn mode:\n```\n$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\n```\n\nFor example:\n```\n$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --driver-memory 4g \\\n    --executor-memory 2g \\\n    --executor-cores 1 \\\n    --queue thequeue \\\n    lib/spark-examples*.jar \\\n    10\n```\n\nSee the detail from:\nhttp://spark.apache.org/docs/latest/running-on-yarn.html\n","slug":"running-spark-on-yarn","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bgc000ui272sfkwmz0g","content":"<h3 id=\"To-launch-a-Spark-application-in-cluster-yarn-mode\"><a href=\"#To-launch-a-Spark-application-in-cluster-yarn-mode\" class=\"headerlink\" title=\"To launch a Spark application in cluster/yarn mode:\"></a>To launch a Spark application in cluster/yarn mode:</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</div></pre></td></tr></table></figure>\n<p>For example:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\</div><div class=\"line\">    --master yarn \\</div><div class=\"line\">    --deploy-mode cluster \\</div><div class=\"line\">    --driver-memory 4g \\</div><div class=\"line\">    --executor-memory 2g \\</div><div class=\"line\">    --executor-cores 1 \\</div><div class=\"line\">    --queue thequeue \\</div><div class=\"line\">    lib/spark-examples*.jar \\</div><div class=\"line\">    10</div></pre></td></tr></table></figure></p>\n<p>See the detail from:<br><a href=\"http://spark.apache.org/docs/latest/running-on-yarn.html\" target=\"_blank\" rel=\"external\">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"To-launch-a-Spark-application-in-cluster-yarn-mode\"><a href=\"#To-launch-a-Spark-application-in-cluster-yarn-mode\" class=\"headerlink\" title=\"To launch a Spark application in cluster/yarn mode:\"></a>To launch a Spark application in cluster/yarn mode:</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</div></pre></td></tr></table></figure>\n<p>For example:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \\</div><div class=\"line\">    --master yarn \\</div><div class=\"line\">    --deploy-mode cluster \\</div><div class=\"line\">    --driver-memory 4g \\</div><div class=\"line\">    --executor-memory 2g \\</div><div class=\"line\">    --executor-cores 1 \\</div><div class=\"line\">    --queue thequeue \\</div><div class=\"line\">    lib/spark-examples*.jar \\</div><div class=\"line\">    10</div></pre></td></tr></table></figure></p>\n<p>See the detail from:<br><a href=\"http://spark.apache.org/docs/latest/running-on-yarn.html\" target=\"_blank\" rel=\"external\">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p>\n"},{"title":"StatsD - Graphite - Grafana","date":"2017-04-09T12:46:25.000Z","_content":"#1. StatsD - metrics data collecting\n\n  - In your application code, use the StatsD client (e.g. Java client library) to collect and send the statistics and aggregation data to StatsD server;\n  - Not need to pre-define the metrics in anywhere, just place them into your application code;\n  - By default, stats data are aggregated and sent to Graphite server by every 10 seconds, so think this near-realtime;\n\n#2. Graphite - metrics data graphing and storage\n  - Store numeric time-series data: The metric data would be stored into Graphite server (include a Whisper database);\n  - Render the graph for metrics data per the metrics demand;\n  - The pre-built UI dashboard is not powerful shown as below, but can easily use it to view the metrics data;\n  -  For production environment, Graphite server should be running as cluster instead of stand-alone server;\n\n\n\n#3. Grafana - powerful dashboard for visualizing the metrics data\n  - easy to integrate with Graphite to visualize the metrics data;\n  - input the Graphite HTTP URL to link to Graphite as data source;\n  - powerful pre-built reporting charts and dashboard;\n\n#4.  Sample Code:\n - easy to feed the data to StatsD -> Graphite -> Grafana\n```\n private static final StatsDClient statsd = new NonBlockingStatsDClient(\"app.api-analytics.sample\", \"127.0.0.1\",\n      8125);\n statsd.count(\"get.request\", (long)(Math.random()* 10)); // Request count of this API\n```\n","source":"_posts/sgg.md","raw":"---\ntitle: StatsD - Graphite - Grafana\ndate: 2017-4-9 20:46:25\n---\n#1. StatsD - metrics data collecting\n\n  - In your application code, use the StatsD client (e.g. Java client library) to collect and send the statistics and aggregation data to StatsD server;\n  - Not need to pre-define the metrics in anywhere, just place them into your application code;\n  - By default, stats data are aggregated and sent to Graphite server by every 10 seconds, so think this near-realtime;\n\n#2. Graphite - metrics data graphing and storage\n  - Store numeric time-series data: The metric data would be stored into Graphite server (include a Whisper database);\n  - Render the graph for metrics data per the metrics demand;\n  - The pre-built UI dashboard is not powerful shown as below, but can easily use it to view the metrics data;\n  -  For production environment, Graphite server should be running as cluster instead of stand-alone server;\n\n\n\n#3. Grafana - powerful dashboard for visualizing the metrics data\n  - easy to integrate with Graphite to visualize the metrics data;\n  - input the Graphite HTTP URL to link to Graphite as data source;\n  - powerful pre-built reporting charts and dashboard;\n\n#4.  Sample Code:\n - easy to feed the data to StatsD -> Graphite -> Grafana\n```\n private static final StatsDClient statsd = new NonBlockingStatsDClient(\"app.api-analytics.sample\", \"127.0.0.1\",\n      8125);\n statsd.count(\"get.request\", (long)(Math.random()* 10)); // Request count of this API\n```\n","slug":"sgg","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bgd000xi272mlp5l90s","content":"<p>#1. StatsD - metrics data collecting</p>\n<ul>\n<li>In your application code, use the StatsD client (e.g. Java client library) to collect and send the statistics and aggregation data to StatsD server;</li>\n<li>Not need to pre-define the metrics in anywhere, just place them into your application code;</li>\n<li>By default, stats data are aggregated and sent to Graphite server by every 10 seconds, so think this near-realtime;</li>\n</ul>\n<p>#2. Graphite - metrics data graphing and storage</p>\n<ul>\n<li>Store numeric time-series data: The metric data would be stored into Graphite server (include a Whisper database);</li>\n<li>Render the graph for metrics data per the metrics demand;</li>\n<li>The pre-built UI dashboard is not powerful shown as below, but can easily use it to view the metrics data;</li>\n<li>For production environment, Graphite server should be running as cluster instead of stand-alone server;</li>\n</ul>\n<p>#3. Grafana - powerful dashboard for visualizing the metrics data</p>\n<ul>\n<li>easy to integrate with Graphite to visualize the metrics data;</li>\n<li>input the Graphite HTTP URL to link to Graphite as data source;</li>\n<li>powerful pre-built reporting charts and dashboard;</li>\n</ul>\n<p>#4.  Sample Code:</p>\n<ul>\n<li>easy to feed the data to StatsD -&gt; Graphite -&gt; Grafana<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">private static final StatsDClient statsd = new NonBlockingStatsDClient(&quot;app.api-analytics.sample&quot;, &quot;127.0.0.1&quot;,</div><div class=\"line\">     8125);</div><div class=\"line\">statsd.count(&quot;get.request&quot;, (long)(Math.random()* 10)); // Request count of this API</div></pre></td></tr></table></figure>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>#1. StatsD - metrics data collecting</p>\n<ul>\n<li>In your application code, use the StatsD client (e.g. Java client library) to collect and send the statistics and aggregation data to StatsD server;</li>\n<li>Not need to pre-define the metrics in anywhere, just place them into your application code;</li>\n<li>By default, stats data are aggregated and sent to Graphite server by every 10 seconds, so think this near-realtime;</li>\n</ul>\n<p>#2. Graphite - metrics data graphing and storage</p>\n<ul>\n<li>Store numeric time-series data: The metric data would be stored into Graphite server (include a Whisper database);</li>\n<li>Render the graph for metrics data per the metrics demand;</li>\n<li>The pre-built UI dashboard is not powerful shown as below, but can easily use it to view the metrics data;</li>\n<li>For production environment, Graphite server should be running as cluster instead of stand-alone server;</li>\n</ul>\n<p>#3. Grafana - powerful dashboard for visualizing the metrics data</p>\n<ul>\n<li>easy to integrate with Graphite to visualize the metrics data;</li>\n<li>input the Graphite HTTP URL to link to Graphite as data source;</li>\n<li>powerful pre-built reporting charts and dashboard;</li>\n</ul>\n<p>#4.  Sample Code:</p>\n<ul>\n<li>easy to feed the data to StatsD -&gt; Graphite -&gt; Grafana<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">private static final StatsDClient statsd = new NonBlockingStatsDClient(&quot;app.api-analytics.sample&quot;, &quot;127.0.0.1&quot;,</div><div class=\"line\">     8125);</div><div class=\"line\">statsd.count(&quot;get.request&quot;, (long)(Math.random()* 10)); // Request count of this API</div></pre></td></tr></table></figure>\n</li>\n</ul>\n"},{"title":"Spark","date":"2017-04-09T12:46:25.000Z","_content":"","source":"_posts/spark.md","raw":"---\ntitle: Spark\ndate: 2017-4-9 20:46:25\n---\n","slug":"spark","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bge000zi27214abthll","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Writing YARN Applications","date":"2016-04-09T12:46:25.000Z","_content":"\n# Hadoop: Writing YARN Applications\n- 原生开发YARN应用\n  - 参考： http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\n  - 在YARN上编写一个应用程序，你需要开发Client和ApplicationMaster两个模块，并了解涉及到的几个协议的若干API和参数列表，其中ApplicationMaster还要负责资源申请，任务调度、容错等，总之，整个过程非常复杂。\n- 基于Twill开发\n  - 参考： http://twill.apache.org/GettingStarted.html\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；文档也太少；\n- 基于Slider开发\n  - 参考： http://slider.incubator.apache.org\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；本身的框架也很复杂，\n- 基于Spring Hadoop开发\n  - 参考： https://spring.io/guides/gs/yarn-basic/\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；\n\n## 开发Client和ApplicationMaster\n当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是启动ApplicationMaster； 第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。\n### 1.开发Client启动AM\nClient部分是用于将应用提交到YARN, 从而可以启动application master.\n客户端通常只需与ResourceManager交互，期间涉及到多个数据结构和一个RPC协议，具体如下：\n\n![](images/yarn-dev1.png)\n- 客户端通过RPC协议ApplicationClientProtocol向ResourceManager(也称之为ApplicationsManager、ASM)发送应用程序提交请求GetNewApplicationRequest，ResourceManager为其返回应答GetNewApplicationResponse，该数据结构中包含多种信息，包括ApplicationId、可资源使用上限和下限等。初始化并启动一个yarnClient:\n```\nYarnClient yarnClient = YarnClient.createYarnClient();\nyarnClient.init(conf);\nyarnClient.start();\nYarnClientApplication app = yarnClient.createApplication();\nGetNewApplicationResponse appResponse = app.getNewApplicationResponse();\n```\n- Client部分最关键的是构建一个ApplicationSubmissionContext。启动ApplicationMaster所需的所有信息打包到数据结构ApplicationSubmissionContext中，主要包括以下几种信息：\n  - (1) application id\n  - (2) application 名称\n  - (3) application优先级\n  - (4) application 所属队列\n  - (5) application 启动用户名\n  - (6)  ApplicationMaster对应的Container信息ContainerLaunchContext，包括：启动ApplicationMaster所需各种文件资源、jar包、环境变量、启动命令、运行ApplicationMaster所需的资源（主要指内存）等。\n\n  ```\n  // set the application name\n  ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();\n  ApplicationId appId = appContext.getApplicationId();\n\n  appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);\n  appContext.setApplicationName(appName);\n  // Set up the container launch context for the application master\n  ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(\n    localResources, env, commands, null, null, null);\n\n  // Set up resource type requirements\n  // For now, both memory and vcores are supported, so we set memory and\n  // vcores requirements\n  Resource capability = Resource.newInstance(amMemory, amVCores);\n  appContext.setResource(capability);\n  ```\n\n- 客户端调用ClientRMProtocol#submitApplication(ApplicationSubmissionContext)将ApplicationMaster提交到ResourceManager上。ResourceManager收到请求后，会为ApplicationMaster寻找合适的节点，并在该节点上启动它。\n  ```\n  LOG.info(\"Submitting application to ASM\");\n  yarnClient.submitApplication(appContext);\n  ```\n\n- 客户端可通过多种方式查询应用程序的运行状态，其中一种是调用RPC函数ClientRMProtocol#getApplicationReport获取一个应用程序当前运行状况报告，该报告内容包括应用程序名称、所属用户、所在队列、ApplicationMaster所在节点、一些诊断信息、启动时间等。\n  ```\n  // Get application report for the appId we are interested in\n  ApplicationReport report = yarnClient.getApplicationReport(appId);\n\n  LOG.info(\"Got application report from ASM for\"\n      + \", appId=\" + appId.getId()\n      + \", clientToAMToken=\" + report.getClientToAMToken()\n      + \", appDiagnostics=\" + report.getDiagnostics()\n      + \", appMasterHost=\" + report.getHost()\n      + \", appQueue=\" + report.getQueue()\n      + \", appMasterRpcPort=\" + report.getRpcPort()\n      + \", appStartTime=\" + report.getStartTime()\n      + \", yarnAppState=\" + report.getYarnApplicationState().toString()\n      + \", distributedFinalState=\" + report.getFinalApplicationStatus().toString()\n      + \", appTrackingUrl=\" + report.getTrackingUrl()\n      + \", appUser=\" + report.getUser());\n\n  YarnApplicationState state = report.getYarnApplicationState();\n  FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();\n  ```\n- 如果有异常或者其他情况，可以通过yarnClient.killApplication(appId);来kill掉应用；\n\n### 2.开发ApplicationMaster\nApplicationMaster需要与ResoureManager和NodeManager交互，以申请资源和启动Container，期间涉及到多个数据结构和两个RPC协议。具体步骤如下：\n- ApplicationMaster首先需通过RPC协议AMRMProtocol向ResourceManager发送注册请求RegisterApplicationMasterRequest，该数据结构中包含ApplicationMaster所在节点的host、RPC port和TrackingUrl等信息，而ResourceManager将返回RegisterApplicationMasterResponse，该数据结构中包含多种信息，包括该应用程序的ACL列表、可资源使用上限和下限等。\n\n![](images/yarn-dev2.png)\n\n- ApplicationMaster与RM之间的心跳：整个运行过程中，ApplicationMaster需通过心跳与ResourceManager保持联系，这是因为，如果一段时间内（默认是10min），ResourceManager未收到ApplicationMaster信息，则认为它死掉了，会重新调度或者让其失败。通常而言，ApplicationMaster周期性调用RPC函数ApplicationMasterProtocol.allocate向其发送空的AllocateRequest请求即可。\n\n- 构造Container：根据每个任务的资源需求，ApplicationMaster可向ResourceManager申请一系列用于运行任务的Container，ApplicationMaster使用ResourceRequest类描述每个Container（一个container只能运行一个任务）：\n  - 1）Hostname：期望Container所在的节点，如果是*，表示可以为任意节点。\n  - 2）Resource capability：运行该任务所需的资源量，如(memory/disk/cpu)。\n  - 3）Priority：任务优先级。一个应用程序中的任务可能有多种优先级，ResourceManager会优先为高优先级的任务分配资源。\n  - 4）numContainers：符合以上条件的container数目。\n\n\n- 申请资源分配Container：一旦为任务构造了Container后，ApplicationMaster会使用RPC函数AMRMProtocol#allocate向ResourceManager发送一个AllocateRequest对象，以请求分配这些Container，AllocateRequest中包含以下信息：\n  - 1）Requested containers：所需的Container列表\n  - 2）Released containers：有些情况下，比如有些任务在某些节点上失败过，则ApplicationMaster不想再在这些节点上运行任务，此时可要求释放这些节点上的Container。\n  - 3）Progress update information：应用程序执行进度\n  - 4）ResponseId：RPC响应ID，每次调用RPC，该值会加1。\n- ResourceManager会为ApplicationMaster返回一个AllocateResponse对象，该对象中主要信息包含在AMResponse中：\n  - 1）reboot：ApplicationMaster是否需要重新初始化.当ResourceManager端出现不一致状态时，会要求对应的ApplicationMaster重新初始化。\n  - 2）Allocated Containers：新分配的container列表。\n  - 3）Completed Containers：已运行完成的container列表，该列表中包含运行成功和未成功的Container，ApplicationMaster可能需要重新运行那些未运行成功的Container。\n- ApplicationMaster会不断追踪已经获取的container，且只有当需求发生变化时，才允许重新为Container申请资源。\n\n![](images/yarn-dev3.png)\n\n- 启动Container：当ApplicationMaster（从ResourceManager端）收到新分配的Container列表后，会使用ContainerManagementProtocol#startContainer向对应的NodeManager发送ContainerLaunchContext以启动Container，ContainerLaunchContext包含以下内容：\n  - 1）ContainerId：Container id\n  - 2）Resource：该Container可使用的资源量（当前仅支持内存）\n  - 3）User：Container所属用户\n  - 4）Security tokens：安全令牌，只有持有该令牌才可启动container\n  - 5）LocalResource：运行Container所需的本地资源，比如jar包、二进制文件、其他外部文件等。\n  - 6）ServiceData：应用程序可能使用其他外部服务，这些服务相关的数据通过该参数指定。\n  - 7）Environment：启动container所需的环境变量\n  - 8）command：启动container的命令\n\n\n- 监控Container：ApplicationMaster可以通过2种途径监控启动的Container：\n  - 使用ApplicationMasterProtocol.allocate向ResourceManager发送查询请求；\n  - 使用ContainerManagementProtocol查询指定的ContainerId对应的Container的状态；\n\n\n- ApplicationMaster会不断重复前面的步骤，直到所有任务运行成功，此时，它会发送FinishApplicationMasterRequest，以告诉ResourceManage自己运行结束。\n\n## 基于Twill开发\nApache Twill这个项目则是为简化YARN上应用程序开发而成立的项目，该项目把与YARN相关的重复性的工作封装成库，使得用户可以专注于自己的应用程序逻辑。\n\n下面代码示例是使用Apache Twill开发一个运行在YARN上的helloworld程序：\n```\npublic class HelloWorld {\n static Logger LOG = LoggerFactory.getLogger(HelloWorld.class);\n static class HelloWorldRunnable extends AbstractTwillRunnable {\n\n public void run() {\n   OG.info(\"Hello World\");\n   }\n }\n\npublic static void main(String[] args) throws Exception {\n YarnConfiguration conf = new YarnConfiguration();\n TwillRunnerService runner = new YarnTwillRunnerService(conf, \"localhost:2181\");\n runner.startAndWait();\n\n HelloWorldRunnable helloworldRunner = new HelloWorldRunnable();\n TwillController controller = runner.prepare(helloworldRunner).start();\n Services.getCompletionFuture(controller).get();\n}\n```\n#### 编译Twill\n```\nmvn clean install -DskipTests=true\n```\n\n#### 启动zookeeper\nStart a Zookeeper server instance\n```\n$ docker run --name zk1 --restart always -d -P zookeeper\n```\n\nThis image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively), so standard container linking will make it automatically available to the linked containers. Since the Zookeeper \"fails fast\" it's better to always restart it.\n\n#### Run Twill sample\n```\n$ export CP=twill-examples-yarn-0.11.0-SNAPSHOT.jar:`/Users/xiningwang/hadoop/hadoop-2.7.3/bin/hadoop classpath`\n\n$ java -cp $CP org.apache.twill.example.yarn.HelloWorld {zookeeper_host:port}\n```\n#### BundledJarExample Application\n\nThe BundledJarExample application demonstrates the Twill functionality that allows you to run any Java application in Twill without worrying about library version conflicts between your application and Hadoop. The example calls the main class in a sample application Echo, which simply logs the command line argument(s) passed to it. The Echo application uses a different version of Guava from Twill and Hadoop distributions. BundledJarExample looks for the dependency in a lib folder packaged at the root of the Echo jar.\n\nYou can run the BundleJarExample application from any node of the Hadoop cluster using the below command (be sure to add your ZooKeeper Host and Port):\n```\n$ export CP=twill-examples-yarn-0.10.0.jar:`hadoop classpath`\n\n$ java -cp $CP org.apache.twill.example.yarn.BundledJarExample {zookeeper_host:port} \\\n    twill-examples-echo-0.10.0.jar echo.EchoMain arg1\n```\n\n## Slider\n由SliderAM负责给cluster申请资源，并负责容错（component挂掉之后，SliderAM重新找RM申请资源，并进行相应的分配），每个component的实例运行在YARN container中，一个cluster在YARN中的运行流程大致如下：\n\n![Slider](images/slider.png)\n\n## Spring Hadoop\n\n![spring-hadoop](https://spring.io/guides/gs/yarn-basic/images/rm-ui.png)\n","source":"_posts/yarn-appdev.md","raw":"---\ntitle: Writing YARN Applications\ndate: 2016-4-9 20:46:25\n---\n\n# Hadoop: Writing YARN Applications\n- 原生开发YARN应用\n  - 参考： http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\n  - 在YARN上编写一个应用程序，你需要开发Client和ApplicationMaster两个模块，并了解涉及到的几个协议的若干API和参数列表，其中ApplicationMaster还要负责资源申请，任务调度、容错等，总之，整个过程非常复杂。\n- 基于Twill开发\n  - 参考： http://twill.apache.org/GettingStarted.html\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；文档也太少；\n- 基于Slider开发\n  - 参考： http://slider.incubator.apache.org\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；本身的框架也很复杂，\n- 基于Spring Hadoop开发\n  - 参考： https://spring.io/guides/gs/yarn-basic/\n  - 优点： 简化了YARN开发的复杂性；\n  - 缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；\n\n## 开发Client和ApplicationMaster\n当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是启动ApplicationMaster； 第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。\n### 1.开发Client启动AM\nClient部分是用于将应用提交到YARN, 从而可以启动application master.\n客户端通常只需与ResourceManager交互，期间涉及到多个数据结构和一个RPC协议，具体如下：\n\n![](images/yarn-dev1.png)\n- 客户端通过RPC协议ApplicationClientProtocol向ResourceManager(也称之为ApplicationsManager、ASM)发送应用程序提交请求GetNewApplicationRequest，ResourceManager为其返回应答GetNewApplicationResponse，该数据结构中包含多种信息，包括ApplicationId、可资源使用上限和下限等。初始化并启动一个yarnClient:\n```\nYarnClient yarnClient = YarnClient.createYarnClient();\nyarnClient.init(conf);\nyarnClient.start();\nYarnClientApplication app = yarnClient.createApplication();\nGetNewApplicationResponse appResponse = app.getNewApplicationResponse();\n```\n- Client部分最关键的是构建一个ApplicationSubmissionContext。启动ApplicationMaster所需的所有信息打包到数据结构ApplicationSubmissionContext中，主要包括以下几种信息：\n  - (1) application id\n  - (2) application 名称\n  - (3) application优先级\n  - (4) application 所属队列\n  - (5) application 启动用户名\n  - (6)  ApplicationMaster对应的Container信息ContainerLaunchContext，包括：启动ApplicationMaster所需各种文件资源、jar包、环境变量、启动命令、运行ApplicationMaster所需的资源（主要指内存）等。\n\n  ```\n  // set the application name\n  ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();\n  ApplicationId appId = appContext.getApplicationId();\n\n  appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);\n  appContext.setApplicationName(appName);\n  // Set up the container launch context for the application master\n  ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(\n    localResources, env, commands, null, null, null);\n\n  // Set up resource type requirements\n  // For now, both memory and vcores are supported, so we set memory and\n  // vcores requirements\n  Resource capability = Resource.newInstance(amMemory, amVCores);\n  appContext.setResource(capability);\n  ```\n\n- 客户端调用ClientRMProtocol#submitApplication(ApplicationSubmissionContext)将ApplicationMaster提交到ResourceManager上。ResourceManager收到请求后，会为ApplicationMaster寻找合适的节点，并在该节点上启动它。\n  ```\n  LOG.info(\"Submitting application to ASM\");\n  yarnClient.submitApplication(appContext);\n  ```\n\n- 客户端可通过多种方式查询应用程序的运行状态，其中一种是调用RPC函数ClientRMProtocol#getApplicationReport获取一个应用程序当前运行状况报告，该报告内容包括应用程序名称、所属用户、所在队列、ApplicationMaster所在节点、一些诊断信息、启动时间等。\n  ```\n  // Get application report for the appId we are interested in\n  ApplicationReport report = yarnClient.getApplicationReport(appId);\n\n  LOG.info(\"Got application report from ASM for\"\n      + \", appId=\" + appId.getId()\n      + \", clientToAMToken=\" + report.getClientToAMToken()\n      + \", appDiagnostics=\" + report.getDiagnostics()\n      + \", appMasterHost=\" + report.getHost()\n      + \", appQueue=\" + report.getQueue()\n      + \", appMasterRpcPort=\" + report.getRpcPort()\n      + \", appStartTime=\" + report.getStartTime()\n      + \", yarnAppState=\" + report.getYarnApplicationState().toString()\n      + \", distributedFinalState=\" + report.getFinalApplicationStatus().toString()\n      + \", appTrackingUrl=\" + report.getTrackingUrl()\n      + \", appUser=\" + report.getUser());\n\n  YarnApplicationState state = report.getYarnApplicationState();\n  FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();\n  ```\n- 如果有异常或者其他情况，可以通过yarnClient.killApplication(appId);来kill掉应用；\n\n### 2.开发ApplicationMaster\nApplicationMaster需要与ResoureManager和NodeManager交互，以申请资源和启动Container，期间涉及到多个数据结构和两个RPC协议。具体步骤如下：\n- ApplicationMaster首先需通过RPC协议AMRMProtocol向ResourceManager发送注册请求RegisterApplicationMasterRequest，该数据结构中包含ApplicationMaster所在节点的host、RPC port和TrackingUrl等信息，而ResourceManager将返回RegisterApplicationMasterResponse，该数据结构中包含多种信息，包括该应用程序的ACL列表、可资源使用上限和下限等。\n\n![](images/yarn-dev2.png)\n\n- ApplicationMaster与RM之间的心跳：整个运行过程中，ApplicationMaster需通过心跳与ResourceManager保持联系，这是因为，如果一段时间内（默认是10min），ResourceManager未收到ApplicationMaster信息，则认为它死掉了，会重新调度或者让其失败。通常而言，ApplicationMaster周期性调用RPC函数ApplicationMasterProtocol.allocate向其发送空的AllocateRequest请求即可。\n\n- 构造Container：根据每个任务的资源需求，ApplicationMaster可向ResourceManager申请一系列用于运行任务的Container，ApplicationMaster使用ResourceRequest类描述每个Container（一个container只能运行一个任务）：\n  - 1）Hostname：期望Container所在的节点，如果是*，表示可以为任意节点。\n  - 2）Resource capability：运行该任务所需的资源量，如(memory/disk/cpu)。\n  - 3）Priority：任务优先级。一个应用程序中的任务可能有多种优先级，ResourceManager会优先为高优先级的任务分配资源。\n  - 4）numContainers：符合以上条件的container数目。\n\n\n- 申请资源分配Container：一旦为任务构造了Container后，ApplicationMaster会使用RPC函数AMRMProtocol#allocate向ResourceManager发送一个AllocateRequest对象，以请求分配这些Container，AllocateRequest中包含以下信息：\n  - 1）Requested containers：所需的Container列表\n  - 2）Released containers：有些情况下，比如有些任务在某些节点上失败过，则ApplicationMaster不想再在这些节点上运行任务，此时可要求释放这些节点上的Container。\n  - 3）Progress update information：应用程序执行进度\n  - 4）ResponseId：RPC响应ID，每次调用RPC，该值会加1。\n- ResourceManager会为ApplicationMaster返回一个AllocateResponse对象，该对象中主要信息包含在AMResponse中：\n  - 1）reboot：ApplicationMaster是否需要重新初始化.当ResourceManager端出现不一致状态时，会要求对应的ApplicationMaster重新初始化。\n  - 2）Allocated Containers：新分配的container列表。\n  - 3）Completed Containers：已运行完成的container列表，该列表中包含运行成功和未成功的Container，ApplicationMaster可能需要重新运行那些未运行成功的Container。\n- ApplicationMaster会不断追踪已经获取的container，且只有当需求发生变化时，才允许重新为Container申请资源。\n\n![](images/yarn-dev3.png)\n\n- 启动Container：当ApplicationMaster（从ResourceManager端）收到新分配的Container列表后，会使用ContainerManagementProtocol#startContainer向对应的NodeManager发送ContainerLaunchContext以启动Container，ContainerLaunchContext包含以下内容：\n  - 1）ContainerId：Container id\n  - 2）Resource：该Container可使用的资源量（当前仅支持内存）\n  - 3）User：Container所属用户\n  - 4）Security tokens：安全令牌，只有持有该令牌才可启动container\n  - 5）LocalResource：运行Container所需的本地资源，比如jar包、二进制文件、其他外部文件等。\n  - 6）ServiceData：应用程序可能使用其他外部服务，这些服务相关的数据通过该参数指定。\n  - 7）Environment：启动container所需的环境变量\n  - 8）command：启动container的命令\n\n\n- 监控Container：ApplicationMaster可以通过2种途径监控启动的Container：\n  - 使用ApplicationMasterProtocol.allocate向ResourceManager发送查询请求；\n  - 使用ContainerManagementProtocol查询指定的ContainerId对应的Container的状态；\n\n\n- ApplicationMaster会不断重复前面的步骤，直到所有任务运行成功，此时，它会发送FinishApplicationMasterRequest，以告诉ResourceManage自己运行结束。\n\n## 基于Twill开发\nApache Twill这个项目则是为简化YARN上应用程序开发而成立的项目，该项目把与YARN相关的重复性的工作封装成库，使得用户可以专注于自己的应用程序逻辑。\n\n下面代码示例是使用Apache Twill开发一个运行在YARN上的helloworld程序：\n```\npublic class HelloWorld {\n static Logger LOG = LoggerFactory.getLogger(HelloWorld.class);\n static class HelloWorldRunnable extends AbstractTwillRunnable {\n\n public void run() {\n   OG.info(\"Hello World\");\n   }\n }\n\npublic static void main(String[] args) throws Exception {\n YarnConfiguration conf = new YarnConfiguration();\n TwillRunnerService runner = new YarnTwillRunnerService(conf, \"localhost:2181\");\n runner.startAndWait();\n\n HelloWorldRunnable helloworldRunner = new HelloWorldRunnable();\n TwillController controller = runner.prepare(helloworldRunner).start();\n Services.getCompletionFuture(controller).get();\n}\n```\n#### 编译Twill\n```\nmvn clean install -DskipTests=true\n```\n\n#### 启动zookeeper\nStart a Zookeeper server instance\n```\n$ docker run --name zk1 --restart always -d -P zookeeper\n```\n\nThis image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively), so standard container linking will make it automatically available to the linked containers. Since the Zookeeper \"fails fast\" it's better to always restart it.\n\n#### Run Twill sample\n```\n$ export CP=twill-examples-yarn-0.11.0-SNAPSHOT.jar:`/Users/xiningwang/hadoop/hadoop-2.7.3/bin/hadoop classpath`\n\n$ java -cp $CP org.apache.twill.example.yarn.HelloWorld {zookeeper_host:port}\n```\n#### BundledJarExample Application\n\nThe BundledJarExample application demonstrates the Twill functionality that allows you to run any Java application in Twill without worrying about library version conflicts between your application and Hadoop. The example calls the main class in a sample application Echo, which simply logs the command line argument(s) passed to it. The Echo application uses a different version of Guava from Twill and Hadoop distributions. BundledJarExample looks for the dependency in a lib folder packaged at the root of the Echo jar.\n\nYou can run the BundleJarExample application from any node of the Hadoop cluster using the below command (be sure to add your ZooKeeper Host and Port):\n```\n$ export CP=twill-examples-yarn-0.10.0.jar:`hadoop classpath`\n\n$ java -cp $CP org.apache.twill.example.yarn.BundledJarExample {zookeeper_host:port} \\\n    twill-examples-echo-0.10.0.jar echo.EchoMain arg1\n```\n\n## Slider\n由SliderAM负责给cluster申请资源，并负责容错（component挂掉之后，SliderAM重新找RM申请资源，并进行相应的分配），每个component的实例运行在YARN container中，一个cluster在YARN中的运行流程大致如下：\n\n![Slider](images/slider.png)\n\n## Spring Hadoop\n\n![spring-hadoop](https://spring.io/guides/gs/yarn-basic/images/rm-ui.png)\n","slug":"yarn-appdev","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bgg0012i272ppc9xbml","content":"<h1 id=\"Hadoop-Writing-YARN-Applications\"><a href=\"#Hadoop-Writing-YARN-Applications\" class=\"headerlink\" title=\"Hadoop: Writing YARN Applications\"></a>Hadoop: Writing YARN Applications</h1><ul>\n<li>原生开发YARN应用<ul>\n<li>参考： <a href=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html</a></li>\n<li>在YARN上编写一个应用程序，你需要开发Client和ApplicationMaster两个模块，并了解涉及到的几个协议的若干API和参数列表，其中ApplicationMaster还要负责资源申请，任务调度、容错等，总之，整个过程非常复杂。</li>\n</ul>\n</li>\n<li>基于Twill开发<ul>\n<li>参考： <a href=\"http://twill.apache.org/GettingStarted.html\" target=\"_blank\" rel=\"external\">http://twill.apache.org/GettingStarted.html</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；文档也太少；</li>\n</ul>\n</li>\n<li>基于Slider开发<ul>\n<li>参考： <a href=\"http://slider.incubator.apache.org\" target=\"_blank\" rel=\"external\">http://slider.incubator.apache.org</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；本身的框架也很复杂，</li>\n</ul>\n</li>\n<li>基于Spring Hadoop开发<ul>\n<li>参考： <a href=\"https://spring.io/guides/gs/yarn-basic/\" target=\"_blank\" rel=\"external\">https://spring.io/guides/gs/yarn-basic/</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"开发Client和ApplicationMaster\"><a href=\"#开发Client和ApplicationMaster\" class=\"headerlink\" title=\"开发Client和ApplicationMaster\"></a>开发Client和ApplicationMaster</h2><p>当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是启动ApplicationMaster； 第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。</p>\n<h3 id=\"1-开发Client启动AM\"><a href=\"#1-开发Client启动AM\" class=\"headerlink\" title=\"1.开发Client启动AM\"></a>1.开发Client启动AM</h3><p>Client部分是用于将应用提交到YARN, 从而可以启动application master.<br>客户端通常只需与ResourceManager交互，期间涉及到多个数据结构和一个RPC协议，具体如下：</p>\n<p><img src=\"images/yarn-dev1.png\" alt=\"\"></p>\n<ul>\n<li><p>客户端通过RPC协议ApplicationClientProtocol向ResourceManager(也称之为ApplicationsManager、ASM)发送应用程序提交请求GetNewApplicationRequest，ResourceManager为其返回应答GetNewApplicationResponse，该数据结构中包含多种信息，包括ApplicationId、可资源使用上限和下限等。初始化并启动一个yarnClient:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">YarnClient yarnClient = YarnClient.createYarnClient();</div><div class=\"line\">yarnClient.init(conf);</div><div class=\"line\">yarnClient.start();</div><div class=\"line\">YarnClientApplication app = yarnClient.createApplication();</div><div class=\"line\">GetNewApplicationResponse appResponse = app.getNewApplicationResponse();</div></pre></td></tr></table></figure>\n</li>\n<li><p>Client部分最关键的是构建一个ApplicationSubmissionContext。启动ApplicationMaster所需的所有信息打包到数据结构ApplicationSubmissionContext中，主要包括以下几种信息：</p>\n<ul>\n<li>(1) application id</li>\n<li>(2) application 名称</li>\n<li>(3) application优先级</li>\n<li>(4) application 所属队列</li>\n<li>(5) application 启动用户名</li>\n<li>(6)  ApplicationMaster对应的Container信息ContainerLaunchContext，包括：启动ApplicationMaster所需各种文件资源、jar包、环境变量、启动命令、运行ApplicationMaster所需的资源（主要指内存）等。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">// set the application name</div><div class=\"line\">ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();</div><div class=\"line\">ApplicationId appId = appContext.getApplicationId();</div><div class=\"line\"></div><div class=\"line\">appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);</div><div class=\"line\">appContext.setApplicationName(appName);</div><div class=\"line\">// Set up the container launch context for the application master</div><div class=\"line\">ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(</div><div class=\"line\">  localResources, env, commands, null, null, null);</div><div class=\"line\"></div><div class=\"line\">// Set up resource type requirements</div><div class=\"line\">// For now, both memory and vcores are supported, so we set memory and</div><div class=\"line\">// vcores requirements</div><div class=\"line\">Resource capability = Resource.newInstance(amMemory, amVCores);</div><div class=\"line\">appContext.setResource(capability);</div></pre></td></tr></table></figure>\n</li>\n<li><p>客户端调用ClientRMProtocol#submitApplication(ApplicationSubmissionContext)将ApplicationMaster提交到ResourceManager上。ResourceManager收到请求后，会为ApplicationMaster寻找合适的节点，并在该节点上启动它。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">LOG.info(&quot;Submitting application to ASM&quot;);</div><div class=\"line\">yarnClient.submitApplication(appContext);</div></pre></td></tr></table></figure>\n</li>\n<li><p>客户端可通过多种方式查询应用程序的运行状态，其中一种是调用RPC函数ClientRMProtocol#getApplicationReport获取一个应用程序当前运行状况报告，该报告内容包括应用程序名称、所属用户、所在队列、ApplicationMaster所在节点、一些诊断信息、启动时间等。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Get application report for the appId we are interested in</div><div class=\"line\">ApplicationReport report = yarnClient.getApplicationReport(appId);</div><div class=\"line\"></div><div class=\"line\">LOG.info(&quot;Got application report from ASM for&quot;</div><div class=\"line\">    + &quot;, appId=&quot; + appId.getId()</div><div class=\"line\">    + &quot;, clientToAMToken=&quot; + report.getClientToAMToken()</div><div class=\"line\">    + &quot;, appDiagnostics=&quot; + report.getDiagnostics()</div><div class=\"line\">    + &quot;, appMasterHost=&quot; + report.getHost()</div><div class=\"line\">    + &quot;, appQueue=&quot; + report.getQueue()</div><div class=\"line\">    + &quot;, appMasterRpcPort=&quot; + report.getRpcPort()</div><div class=\"line\">    + &quot;, appStartTime=&quot; + report.getStartTime()</div><div class=\"line\">    + &quot;, yarnAppState=&quot; + report.getYarnApplicationState().toString()</div><div class=\"line\">    + &quot;, distributedFinalState=&quot; + report.getFinalApplicationStatus().toString()</div><div class=\"line\">    + &quot;, appTrackingUrl=&quot; + report.getTrackingUrl()</div><div class=\"line\">    + &quot;, appUser=&quot; + report.getUser());</div><div class=\"line\"></div><div class=\"line\">YarnApplicationState state = report.getYarnApplicationState();</div><div class=\"line\">FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果有异常或者其他情况，可以通过yarnClient.killApplication(appId);来kill掉应用；</p>\n</li>\n</ul>\n<h3 id=\"2-开发ApplicationMaster\"><a href=\"#2-开发ApplicationMaster\" class=\"headerlink\" title=\"2.开发ApplicationMaster\"></a>2.开发ApplicationMaster</h3><p>ApplicationMaster需要与ResoureManager和NodeManager交互，以申请资源和启动Container，期间涉及到多个数据结构和两个RPC协议。具体步骤如下：</p>\n<ul>\n<li>ApplicationMaster首先需通过RPC协议AMRMProtocol向ResourceManager发送注册请求RegisterApplicationMasterRequest，该数据结构中包含ApplicationMaster所在节点的host、RPC port和TrackingUrl等信息，而ResourceManager将返回RegisterApplicationMasterResponse，该数据结构中包含多种信息，包括该应用程序的ACL列表、可资源使用上限和下限等。</li>\n</ul>\n<p><img src=\"images/yarn-dev2.png\" alt=\"\"></p>\n<ul>\n<li><p>ApplicationMaster与RM之间的心跳：整个运行过程中，ApplicationMaster需通过心跳与ResourceManager保持联系，这是因为，如果一段时间内（默认是10min），ResourceManager未收到ApplicationMaster信息，则认为它死掉了，会重新调度或者让其失败。通常而言，ApplicationMaster周期性调用RPC函数ApplicationMasterProtocol.allocate向其发送空的AllocateRequest请求即可。</p>\n</li>\n<li><p>构造Container：根据每个任务的资源需求，ApplicationMaster可向ResourceManager申请一系列用于运行任务的Container，ApplicationMaster使用ResourceRequest类描述每个Container（一个container只能运行一个任务）：</p>\n<ul>\n<li>1）Hostname：期望Container所在的节点，如果是*，表示可以为任意节点。</li>\n<li>2）Resource capability：运行该任务所需的资源量，如(memory/disk/cpu)。</li>\n<li>3）Priority：任务优先级。一个应用程序中的任务可能有多种优先级，ResourceManager会优先为高优先级的任务分配资源。</li>\n<li>4）numContainers：符合以上条件的container数目。</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>申请资源分配Container：一旦为任务构造了Container后，ApplicationMaster会使用RPC函数AMRMProtocol#allocate向ResourceManager发送一个AllocateRequest对象，以请求分配这些Container，AllocateRequest中包含以下信息：<ul>\n<li>1）Requested containers：所需的Container列表</li>\n<li>2）Released containers：有些情况下，比如有些任务在某些节点上失败过，则ApplicationMaster不想再在这些节点上运行任务，此时可要求释放这些节点上的Container。</li>\n<li>3）Progress update information：应用程序执行进度</li>\n<li>4）ResponseId：RPC响应ID，每次调用RPC，该值会加1。</li>\n</ul>\n</li>\n<li>ResourceManager会为ApplicationMaster返回一个AllocateResponse对象，该对象中主要信息包含在AMResponse中：<ul>\n<li>1）reboot：ApplicationMaster是否需要重新初始化.当ResourceManager端出现不一致状态时，会要求对应的ApplicationMaster重新初始化。</li>\n<li>2）Allocated Containers：新分配的container列表。</li>\n<li>3）Completed Containers：已运行完成的container列表，该列表中包含运行成功和未成功的Container，ApplicationMaster可能需要重新运行那些未运行成功的Container。</li>\n</ul>\n</li>\n<li>ApplicationMaster会不断追踪已经获取的container，且只有当需求发生变化时，才允许重新为Container申请资源。</li>\n</ul>\n<p><img src=\"images/yarn-dev3.png\" alt=\"\"></p>\n<ul>\n<li>启动Container：当ApplicationMaster（从ResourceManager端）收到新分配的Container列表后，会使用ContainerManagementProtocol#startContainer向对应的NodeManager发送ContainerLaunchContext以启动Container，ContainerLaunchContext包含以下内容：<ul>\n<li>1）ContainerId：Container id</li>\n<li>2）Resource：该Container可使用的资源量（当前仅支持内存）</li>\n<li>3）User：Container所属用户</li>\n<li>4）Security tokens：安全令牌，只有持有该令牌才可启动container</li>\n<li>5）LocalResource：运行Container所需的本地资源，比如jar包、二进制文件、其他外部文件等。</li>\n<li>6）ServiceData：应用程序可能使用其他外部服务，这些服务相关的数据通过该参数指定。</li>\n<li>7）Environment：启动container所需的环境变量</li>\n<li>8）command：启动container的命令</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>监控Container：ApplicationMaster可以通过2种途径监控启动的Container：<ul>\n<li>使用ApplicationMasterProtocol.allocate向ResourceManager发送查询请求；</li>\n<li>使用ContainerManagementProtocol查询指定的ContainerId对应的Container的状态；</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ApplicationMaster会不断重复前面的步骤，直到所有任务运行成功，此时，它会发送FinishApplicationMasterRequest，以告诉ResourceManage自己运行结束。</li>\n</ul>\n<h2 id=\"基于Twill开发\"><a href=\"#基于Twill开发\" class=\"headerlink\" title=\"基于Twill开发\"></a>基于Twill开发</h2><p>Apache Twill这个项目则是为简化YARN上应用程序开发而成立的项目，该项目把与YARN相关的重复性的工作封装成库，使得用户可以专注于自己的应用程序逻辑。</p>\n<p>下面代码示例是使用Apache Twill开发一个运行在YARN上的helloworld程序：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class HelloWorld &#123;</div><div class=\"line\"> static Logger LOG = LoggerFactory.getLogger(HelloWorld.class);</div><div class=\"line\"> static class HelloWorldRunnable extends AbstractTwillRunnable &#123;</div><div class=\"line\"></div><div class=\"line\"> public void run() &#123;</div><div class=\"line\">   OG.info(&quot;Hello World&quot;);</div><div class=\"line\">   &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"></div><div class=\"line\">public static void main(String[] args) throws Exception &#123;</div><div class=\"line\"> YarnConfiguration conf = new YarnConfiguration();</div><div class=\"line\"> TwillRunnerService runner = new YarnTwillRunnerService(conf, &quot;localhost:2181&quot;);</div><div class=\"line\"> runner.startAndWait();</div><div class=\"line\"></div><div class=\"line\"> HelloWorldRunnable helloworldRunner = new HelloWorldRunnable();</div><div class=\"line\"> TwillController controller = runner.prepare(helloworldRunner).start();</div><div class=\"line\"> Services.getCompletionFuture(controller).get();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"编译Twill\"><a href=\"#编译Twill\" class=\"headerlink\" title=\"编译Twill\"></a>编译Twill</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mvn clean install -DskipTests=true</div></pre></td></tr></table></figure>\n<h4 id=\"启动zookeeper\"><a href=\"#启动zookeeper\" class=\"headerlink\" title=\"启动zookeeper\"></a>启动zookeeper</h4><p>Start a Zookeeper server instance<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ docker run --name zk1 --restart always -d -P zookeeper</div></pre></td></tr></table></figure></p>\n<p>This image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively), so standard container linking will make it automatically available to the linked containers. Since the Zookeeper “fails fast” it’s better to always restart it.</p>\n<h4 id=\"Run-Twill-sample\"><a href=\"#Run-Twill-sample\" class=\"headerlink\" title=\"Run Twill sample\"></a>Run Twill sample</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ export CP=twill-examples-yarn-0.11.0-SNAPSHOT.jar:`/Users/xiningwang/hadoop/hadoop-2.7.3/bin/hadoop classpath`</div><div class=\"line\"></div><div class=\"line\">$ java -cp $CP org.apache.twill.example.yarn.HelloWorld &#123;zookeeper_host:port&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"BundledJarExample-Application\"><a href=\"#BundledJarExample-Application\" class=\"headerlink\" title=\"BundledJarExample Application\"></a>BundledJarExample Application</h4><p>The BundledJarExample application demonstrates the Twill functionality that allows you to run any Java application in Twill without worrying about library version conflicts between your application and Hadoop. The example calls the main class in a sample application Echo, which simply logs the command line argument(s) passed to it. The Echo application uses a different version of Guava from Twill and Hadoop distributions. BundledJarExample looks for the dependency in a lib folder packaged at the root of the Echo jar.</p>\n<p>You can run the BundleJarExample application from any node of the Hadoop cluster using the below command (be sure to add your ZooKeeper Host and Port):<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ export CP=twill-examples-yarn-0.10.0.jar:`hadoop classpath`</div><div class=\"line\"></div><div class=\"line\">$ java -cp $CP org.apache.twill.example.yarn.BundledJarExample &#123;zookeeper_host:port&#125; \\</div><div class=\"line\">    twill-examples-echo-0.10.0.jar echo.EchoMain arg1</div></pre></td></tr></table></figure></p>\n<h2 id=\"Slider\"><a href=\"#Slider\" class=\"headerlink\" title=\"Slider\"></a>Slider</h2><p>由SliderAM负责给cluster申请资源，并负责容错（component挂掉之后，SliderAM重新找RM申请资源，并进行相应的分配），每个component的实例运行在YARN container中，一个cluster在YARN中的运行流程大致如下：</p>\n<p><img src=\"images/slider.png\" alt=\"Slider\"></p>\n<h2 id=\"Spring-Hadoop\"><a href=\"#Spring-Hadoop\" class=\"headerlink\" title=\"Spring Hadoop\"></a>Spring Hadoop</h2><p><img src=\"https://spring.io/guides/gs/yarn-basic/images/rm-ui.png\" alt=\"spring-hadoop\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hadoop-Writing-YARN-Applications\"><a href=\"#Hadoop-Writing-YARN-Applications\" class=\"headerlink\" title=\"Hadoop: Writing YARN Applications\"></a>Hadoop: Writing YARN Applications</h1><ul>\n<li>原生开发YARN应用<ul>\n<li>参考： <a href=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html</a></li>\n<li>在YARN上编写一个应用程序，你需要开发Client和ApplicationMaster两个模块，并了解涉及到的几个协议的若干API和参数列表，其中ApplicationMaster还要负责资源申请，任务调度、容错等，总之，整个过程非常复杂。</li>\n</ul>\n</li>\n<li>基于Twill开发<ul>\n<li>参考： <a href=\"http://twill.apache.org/GettingStarted.html\" target=\"_blank\" rel=\"external\">http://twill.apache.org/GettingStarted.html</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；文档也太少；</li>\n</ul>\n</li>\n<li>基于Slider开发<ul>\n<li>参考： <a href=\"http://slider.incubator.apache.org\" target=\"_blank\" rel=\"external\">http://slider.incubator.apache.org</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；本身的框架也很复杂，</li>\n</ul>\n</li>\n<li>基于Spring Hadoop开发<ul>\n<li>参考： <a href=\"https://spring.io/guides/gs/yarn-basic/\" target=\"_blank\" rel=\"external\">https://spring.io/guides/gs/yarn-basic/</a></li>\n<li>优点： 简化了YARN开发的复杂性；</li>\n<li>缺点： 不容易trouble shooting,封装部分不易detect问题，另外也不能支持最新的Hadoop cluster；</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"开发Client和ApplicationMaster\"><a href=\"#开发Client和ApplicationMaster\" class=\"headerlink\" title=\"开发Client和ApplicationMaster\"></a>开发Client和ApplicationMaster</h2><p>当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是启动ApplicationMaster； 第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。</p>\n<h3 id=\"1-开发Client启动AM\"><a href=\"#1-开发Client启动AM\" class=\"headerlink\" title=\"1.开发Client启动AM\"></a>1.开发Client启动AM</h3><p>Client部分是用于将应用提交到YARN, 从而可以启动application master.<br>客户端通常只需与ResourceManager交互，期间涉及到多个数据结构和一个RPC协议，具体如下：</p>\n<p><img src=\"images/yarn-dev1.png\" alt=\"\"></p>\n<ul>\n<li><p>客户端通过RPC协议ApplicationClientProtocol向ResourceManager(也称之为ApplicationsManager、ASM)发送应用程序提交请求GetNewApplicationRequest，ResourceManager为其返回应答GetNewApplicationResponse，该数据结构中包含多种信息，包括ApplicationId、可资源使用上限和下限等。初始化并启动一个yarnClient:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">YarnClient yarnClient = YarnClient.createYarnClient();</div><div class=\"line\">yarnClient.init(conf);</div><div class=\"line\">yarnClient.start();</div><div class=\"line\">YarnClientApplication app = yarnClient.createApplication();</div><div class=\"line\">GetNewApplicationResponse appResponse = app.getNewApplicationResponse();</div></pre></td></tr></table></figure>\n</li>\n<li><p>Client部分最关键的是构建一个ApplicationSubmissionContext。启动ApplicationMaster所需的所有信息打包到数据结构ApplicationSubmissionContext中，主要包括以下几种信息：</p>\n<ul>\n<li>(1) application id</li>\n<li>(2) application 名称</li>\n<li>(3) application优先级</li>\n<li>(4) application 所属队列</li>\n<li>(5) application 启动用户名</li>\n<li>(6)  ApplicationMaster对应的Container信息ContainerLaunchContext，包括：启动ApplicationMaster所需各种文件资源、jar包、环境变量、启动命令、运行ApplicationMaster所需的资源（主要指内存）等。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">// set the application name</div><div class=\"line\">ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();</div><div class=\"line\">ApplicationId appId = appContext.getApplicationId();</div><div class=\"line\"></div><div class=\"line\">appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);</div><div class=\"line\">appContext.setApplicationName(appName);</div><div class=\"line\">// Set up the container launch context for the application master</div><div class=\"line\">ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(</div><div class=\"line\">  localResources, env, commands, null, null, null);</div><div class=\"line\"></div><div class=\"line\">// Set up resource type requirements</div><div class=\"line\">// For now, both memory and vcores are supported, so we set memory and</div><div class=\"line\">// vcores requirements</div><div class=\"line\">Resource capability = Resource.newInstance(amMemory, amVCores);</div><div class=\"line\">appContext.setResource(capability);</div></pre></td></tr></table></figure>\n</li>\n<li><p>客户端调用ClientRMProtocol#submitApplication(ApplicationSubmissionContext)将ApplicationMaster提交到ResourceManager上。ResourceManager收到请求后，会为ApplicationMaster寻找合适的节点，并在该节点上启动它。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">LOG.info(&quot;Submitting application to ASM&quot;);</div><div class=\"line\">yarnClient.submitApplication(appContext);</div></pre></td></tr></table></figure>\n</li>\n<li><p>客户端可通过多种方式查询应用程序的运行状态，其中一种是调用RPC函数ClientRMProtocol#getApplicationReport获取一个应用程序当前运行状况报告，该报告内容包括应用程序名称、所属用户、所在队列、ApplicationMaster所在节点、一些诊断信息、启动时间等。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">// Get application report for the appId we are interested in</div><div class=\"line\">ApplicationReport report = yarnClient.getApplicationReport(appId);</div><div class=\"line\"></div><div class=\"line\">LOG.info(&quot;Got application report from ASM for&quot;</div><div class=\"line\">    + &quot;, appId=&quot; + appId.getId()</div><div class=\"line\">    + &quot;, clientToAMToken=&quot; + report.getClientToAMToken()</div><div class=\"line\">    + &quot;, appDiagnostics=&quot; + report.getDiagnostics()</div><div class=\"line\">    + &quot;, appMasterHost=&quot; + report.getHost()</div><div class=\"line\">    + &quot;, appQueue=&quot; + report.getQueue()</div><div class=\"line\">    + &quot;, appMasterRpcPort=&quot; + report.getRpcPort()</div><div class=\"line\">    + &quot;, appStartTime=&quot; + report.getStartTime()</div><div class=\"line\">    + &quot;, yarnAppState=&quot; + report.getYarnApplicationState().toString()</div><div class=\"line\">    + &quot;, distributedFinalState=&quot; + report.getFinalApplicationStatus().toString()</div><div class=\"line\">    + &quot;, appTrackingUrl=&quot; + report.getTrackingUrl()</div><div class=\"line\">    + &quot;, appUser=&quot; + report.getUser());</div><div class=\"line\"></div><div class=\"line\">YarnApplicationState state = report.getYarnApplicationState();</div><div class=\"line\">FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();</div></pre></td></tr></table></figure>\n</li>\n<li><p>如果有异常或者其他情况，可以通过yarnClient.killApplication(appId);来kill掉应用；</p>\n</li>\n</ul>\n<h3 id=\"2-开发ApplicationMaster\"><a href=\"#2-开发ApplicationMaster\" class=\"headerlink\" title=\"2.开发ApplicationMaster\"></a>2.开发ApplicationMaster</h3><p>ApplicationMaster需要与ResoureManager和NodeManager交互，以申请资源和启动Container，期间涉及到多个数据结构和两个RPC协议。具体步骤如下：</p>\n<ul>\n<li>ApplicationMaster首先需通过RPC协议AMRMProtocol向ResourceManager发送注册请求RegisterApplicationMasterRequest，该数据结构中包含ApplicationMaster所在节点的host、RPC port和TrackingUrl等信息，而ResourceManager将返回RegisterApplicationMasterResponse，该数据结构中包含多种信息，包括该应用程序的ACL列表、可资源使用上限和下限等。</li>\n</ul>\n<p><img src=\"images/yarn-dev2.png\" alt=\"\"></p>\n<ul>\n<li><p>ApplicationMaster与RM之间的心跳：整个运行过程中，ApplicationMaster需通过心跳与ResourceManager保持联系，这是因为，如果一段时间内（默认是10min），ResourceManager未收到ApplicationMaster信息，则认为它死掉了，会重新调度或者让其失败。通常而言，ApplicationMaster周期性调用RPC函数ApplicationMasterProtocol.allocate向其发送空的AllocateRequest请求即可。</p>\n</li>\n<li><p>构造Container：根据每个任务的资源需求，ApplicationMaster可向ResourceManager申请一系列用于运行任务的Container，ApplicationMaster使用ResourceRequest类描述每个Container（一个container只能运行一个任务）：</p>\n<ul>\n<li>1）Hostname：期望Container所在的节点，如果是*，表示可以为任意节点。</li>\n<li>2）Resource capability：运行该任务所需的资源量，如(memory/disk/cpu)。</li>\n<li>3）Priority：任务优先级。一个应用程序中的任务可能有多种优先级，ResourceManager会优先为高优先级的任务分配资源。</li>\n<li>4）numContainers：符合以上条件的container数目。</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>申请资源分配Container：一旦为任务构造了Container后，ApplicationMaster会使用RPC函数AMRMProtocol#allocate向ResourceManager发送一个AllocateRequest对象，以请求分配这些Container，AllocateRequest中包含以下信息：<ul>\n<li>1）Requested containers：所需的Container列表</li>\n<li>2）Released containers：有些情况下，比如有些任务在某些节点上失败过，则ApplicationMaster不想再在这些节点上运行任务，此时可要求释放这些节点上的Container。</li>\n<li>3）Progress update information：应用程序执行进度</li>\n<li>4）ResponseId：RPC响应ID，每次调用RPC，该值会加1。</li>\n</ul>\n</li>\n<li>ResourceManager会为ApplicationMaster返回一个AllocateResponse对象，该对象中主要信息包含在AMResponse中：<ul>\n<li>1）reboot：ApplicationMaster是否需要重新初始化.当ResourceManager端出现不一致状态时，会要求对应的ApplicationMaster重新初始化。</li>\n<li>2）Allocated Containers：新分配的container列表。</li>\n<li>3）Completed Containers：已运行完成的container列表，该列表中包含运行成功和未成功的Container，ApplicationMaster可能需要重新运行那些未运行成功的Container。</li>\n</ul>\n</li>\n<li>ApplicationMaster会不断追踪已经获取的container，且只有当需求发生变化时，才允许重新为Container申请资源。</li>\n</ul>\n<p><img src=\"images/yarn-dev3.png\" alt=\"\"></p>\n<ul>\n<li>启动Container：当ApplicationMaster（从ResourceManager端）收到新分配的Container列表后，会使用ContainerManagementProtocol#startContainer向对应的NodeManager发送ContainerLaunchContext以启动Container，ContainerLaunchContext包含以下内容：<ul>\n<li>1）ContainerId：Container id</li>\n<li>2）Resource：该Container可使用的资源量（当前仅支持内存）</li>\n<li>3）User：Container所属用户</li>\n<li>4）Security tokens：安全令牌，只有持有该令牌才可启动container</li>\n<li>5）LocalResource：运行Container所需的本地资源，比如jar包、二进制文件、其他外部文件等。</li>\n<li>6）ServiceData：应用程序可能使用其他外部服务，这些服务相关的数据通过该参数指定。</li>\n<li>7）Environment：启动container所需的环境变量</li>\n<li>8）command：启动container的命令</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>监控Container：ApplicationMaster可以通过2种途径监控启动的Container：<ul>\n<li>使用ApplicationMasterProtocol.allocate向ResourceManager发送查询请求；</li>\n<li>使用ContainerManagementProtocol查询指定的ContainerId对应的Container的状态；</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>ApplicationMaster会不断重复前面的步骤，直到所有任务运行成功，此时，它会发送FinishApplicationMasterRequest，以告诉ResourceManage自己运行结束。</li>\n</ul>\n<h2 id=\"基于Twill开发\"><a href=\"#基于Twill开发\" class=\"headerlink\" title=\"基于Twill开发\"></a>基于Twill开发</h2><p>Apache Twill这个项目则是为简化YARN上应用程序开发而成立的项目，该项目把与YARN相关的重复性的工作封装成库，使得用户可以专注于自己的应用程序逻辑。</p>\n<p>下面代码示例是使用Apache Twill开发一个运行在YARN上的helloworld程序：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class HelloWorld &#123;</div><div class=\"line\"> static Logger LOG = LoggerFactory.getLogger(HelloWorld.class);</div><div class=\"line\"> static class HelloWorldRunnable extends AbstractTwillRunnable &#123;</div><div class=\"line\"></div><div class=\"line\"> public void run() &#123;</div><div class=\"line\">   OG.info(&quot;Hello World&quot;);</div><div class=\"line\">   &#125;</div><div class=\"line\"> &#125;</div><div class=\"line\"></div><div class=\"line\">public static void main(String[] args) throws Exception &#123;</div><div class=\"line\"> YarnConfiguration conf = new YarnConfiguration();</div><div class=\"line\"> TwillRunnerService runner = new YarnTwillRunnerService(conf, &quot;localhost:2181&quot;);</div><div class=\"line\"> runner.startAndWait();</div><div class=\"line\"></div><div class=\"line\"> HelloWorldRunnable helloworldRunner = new HelloWorldRunnable();</div><div class=\"line\"> TwillController controller = runner.prepare(helloworldRunner).start();</div><div class=\"line\"> Services.getCompletionFuture(controller).get();</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"编译Twill\"><a href=\"#编译Twill\" class=\"headerlink\" title=\"编译Twill\"></a>编译Twill</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mvn clean install -DskipTests=true</div></pre></td></tr></table></figure>\n<h4 id=\"启动zookeeper\"><a href=\"#启动zookeeper\" class=\"headerlink\" title=\"启动zookeeper\"></a>启动zookeeper</h4><p>Start a Zookeeper server instance<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ docker run --name zk1 --restart always -d -P zookeeper</div></pre></td></tr></table></figure></p>\n<p>This image includes EXPOSE 2181 2888 3888 (the zookeeper client port, follower port, election port respectively), so standard container linking will make it automatically available to the linked containers. Since the Zookeeper “fails fast” it’s better to always restart it.</p>\n<h4 id=\"Run-Twill-sample\"><a href=\"#Run-Twill-sample\" class=\"headerlink\" title=\"Run Twill sample\"></a>Run Twill sample</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ export CP=twill-examples-yarn-0.11.0-SNAPSHOT.jar:`/Users/xiningwang/hadoop/hadoop-2.7.3/bin/hadoop classpath`</div><div class=\"line\"></div><div class=\"line\">$ java -cp $CP org.apache.twill.example.yarn.HelloWorld &#123;zookeeper_host:port&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"BundledJarExample-Application\"><a href=\"#BundledJarExample-Application\" class=\"headerlink\" title=\"BundledJarExample Application\"></a>BundledJarExample Application</h4><p>The BundledJarExample application demonstrates the Twill functionality that allows you to run any Java application in Twill without worrying about library version conflicts between your application and Hadoop. The example calls the main class in a sample application Echo, which simply logs the command line argument(s) passed to it. The Echo application uses a different version of Guava from Twill and Hadoop distributions. BundledJarExample looks for the dependency in a lib folder packaged at the root of the Echo jar.</p>\n<p>You can run the BundleJarExample application from any node of the Hadoop cluster using the below command (be sure to add your ZooKeeper Host and Port):<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ export CP=twill-examples-yarn-0.10.0.jar:`hadoop classpath`</div><div class=\"line\"></div><div class=\"line\">$ java -cp $CP org.apache.twill.example.yarn.BundledJarExample &#123;zookeeper_host:port&#125; \\</div><div class=\"line\">    twill-examples-echo-0.10.0.jar echo.EchoMain arg1</div></pre></td></tr></table></figure></p>\n<h2 id=\"Slider\"><a href=\"#Slider\" class=\"headerlink\" title=\"Slider\"></a>Slider</h2><p>由SliderAM负责给cluster申请资源，并负责容错（component挂掉之后，SliderAM重新找RM申请资源，并进行相应的分配），每个component的实例运行在YARN container中，一个cluster在YARN中的运行流程大致如下：</p>\n<p><img src=\"images/slider.png\" alt=\"Slider\"></p>\n<h2 id=\"Spring-Hadoop\"><a href=\"#Spring-Hadoop\" class=\"headerlink\" title=\"Spring Hadoop\"></a>Spring Hadoop</h2><p><img src=\"https://spring.io/guides/gs/yarn-basic/images/rm-ui.png\" alt=\"spring-hadoop\"></p>\n"},{"title":"分布式跟踪系统","date":"2016-05-29T12:46:25.000Z","_content":"\n### Architecture Overview\n\n![zipkin](http://zipkin.io/public/img/web-screenshot.png)\n\nTracers live in your applications and record timing and metadata about operations that took place. They often instrument libraries, so that their use is transparent to users. For example, an instrumented web server records when it received a request and when it sent a response. The trace data collected is called a Span.\n\nInstrumentation is written to be safe in production and have little overhead. For this reason, they only propagate IDs in-band, to tell the receiver there’s a trace in progress. Completed spans are reported to Zipkin out-of-band, similar to how applications report metrics asynchronously.\n\nFor example, when an operation is being traced and it needs to make an outgoing http request, a few headers are added to propagate IDs. Headers are not used to send details such as the operation name.\n\nThe component in an instrumented app that sends data to Zipkin is called a Reporter. Reporters send trace data via one of several transports to Zipkin collectors, which persist trace data to storage. Later, storage is queried by the API to provide data to the UI.\n\nHere’s a diagram describing this flow:\n\n![](http://zipkin.io/public/img/architecture-1.png)\n\n### Zipkin architecture\n\nTo see if an instrumentation library already exists for your platform, see the list of existing instrumentations.\n\n### Example flow\nAs mentioned in the overview, identifiers are sent in-band and details are sent out-of-band to Zipkin. In both cases, trace instrumentation is responsible for creating valid traces and rendering them properly. For example, a tracer ensures parity between the data it sends in-band (downstream) and out-of-band (async to Zipkin).\n\nHere’s an example sequence of http tracing where user code calls the resource /foo. This results in a single span, sent asynchronously to Zipkin after user code receives the http response.\n\n┌─────────────┐ ┌───────────────────────┐  ┌─────────────┐  ┌──────────────────┐\n│ User Code   │ │ Trace Instrumentation │  │ Http Client │  │ Zipkin Collector │\n└─────────────┘ └───────────────────────┘  └─────────────┘  └──────────────────┘\n       │                 │                         │                 │\n           ┌─────────┐\n       │ ──┤GET /foo ├─▶ │ ────┐                   │                 │\n           └─────────┘         │ record tags\n       │                 │ ◀───┘                   │                 │\n                           ────┐\n       │                 │     │ add trace headers │                 │\n                           ◀───┘\n       │                 │ ────┐                   │                 │\n                               │ record timestamp\n       │                 │ ◀───┘                   │                 │\n                             ┌─────────────────┐\n       │                 │ ──┤GET /foo         ├─▶ │                 │\n                             │X-B3-TraceId: aa │     ────┐\n       │                 │   │X-B3-SpanId: 6b  │   │     │           │\n                             └─────────────────┘         │ invoke\n       │                 │                         │     │ request   │\n                                                         │\n       │                 │                         │     │           │\n                                 ┌────────┐          ◀───┘\n       │                 │ ◀─────┤200 OK  ├─────── │                 │\n                           ────┐ └────────┘\n       │                 │     │ record duration   │                 │\n            ┌────────┐     ◀───┘\n       │ ◀──┤200 OK  ├── │                         │                 │\n            └────────┘       ┌────────────────────────────────┐\n       │                 │ ──┤ asynchronously report span     ├────▶ │\n                             │                                │\n                             │{                               │\n                             │  \"traceId\": \"aa\",              │\n                             │  \"id\": \"6b\",                   │\n                             │  \"name\": \"get\",                │\n                             │  \"timestamp\": 1483945573944000,│\n                             │  \"duration\": 386000,           │\n                             │  \"annotations\": [              │\n                             │--snip--                        │\n                             └────────────────────────────────┘\nTrace instrumentation report spans asynchronously to prevent delays or failures relating to the tracing system from delaying or breaking user code.\n\n### Transport\nSpans sent by the instrumented library must be transported from the services being traced to Zipkin collectors. There are three primary transports: HTTP, Kafka and Scribe. See Span Receivers for more information.\n\nThere are 4 components that make up Zipkin:\n\n - collector\n - storage\n - search\n - web UI\n\n### Zipkin Collector\nOnce the trace data arrives at the Zipkin collector daemon, it is validated, stored, and indexed for lookups by the Zipkin collector.\n\n### Storage\nZipkin was initially built to store data on Cassandra since Cassandra is scalable, has a flexible schema, and is heavily used within Twitter. However, we made this component pluggable. In addition to Cassandra, we natively support ElasticSearch and MySQL. Other back-ends might be offered as third party extensions.\n\n### Zipkin Query Service\nOnce the data is stored and indexed, we need a way to extract it. The query daemon provides a simple JSON API for finding and retrieving traces. The primary consumer of this API is the Web UI.\n\n### Web UI\nWe created a GUI that presents a nice interface for viewing traces. The web UI provides a method for viewing traces based on service, time, and annotations. Note: there is no built-in authentication in the UI!\n\n### 例子\n\n```\n<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-sleuth</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency>\n\t\t<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-zipkin -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-sleuth-zipkin</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency>\n```\n\n属性文件：\n```\nspring.application.name=ServiceRegistryUsingConsulAndDistributedTrace\nlogging.level.org.springframework.web.servlet.DispatcherServlet=INFO\nspring.zipkin.baseUrl=http://localhost:9411/\nspring.sleuth.sampler.percentage=1.0\nsample.zipkin.enabled=true\n```\n\n```\n//Use this for debugging (or if there is no Zipkin server running on port 9411)\n  @Bean\n  @ConditionalOnProperty(value = \"sample.zipkin.enabled\", havingValue = \"false\")\n  public ZipkinSpanReporter spanCollector() {\n      return new ZipkinSpanReporter() {\n          @Override\n          public void report(zipkin.Span span) {\n//              log.info(String.format(\"Reporting span [%s]\", span));\n          }\n      };\n  }\n```\n","source":"_posts/zipkin.md","raw":"---\ntitle: 分布式跟踪系统\ndate: 2016-5-29 20:46:25\n---\n\n### Architecture Overview\n\n![zipkin](http://zipkin.io/public/img/web-screenshot.png)\n\nTracers live in your applications and record timing and metadata about operations that took place. They often instrument libraries, so that their use is transparent to users. For example, an instrumented web server records when it received a request and when it sent a response. The trace data collected is called a Span.\n\nInstrumentation is written to be safe in production and have little overhead. For this reason, they only propagate IDs in-band, to tell the receiver there’s a trace in progress. Completed spans are reported to Zipkin out-of-band, similar to how applications report metrics asynchronously.\n\nFor example, when an operation is being traced and it needs to make an outgoing http request, a few headers are added to propagate IDs. Headers are not used to send details such as the operation name.\n\nThe component in an instrumented app that sends data to Zipkin is called a Reporter. Reporters send trace data via one of several transports to Zipkin collectors, which persist trace data to storage. Later, storage is queried by the API to provide data to the UI.\n\nHere’s a diagram describing this flow:\n\n![](http://zipkin.io/public/img/architecture-1.png)\n\n### Zipkin architecture\n\nTo see if an instrumentation library already exists for your platform, see the list of existing instrumentations.\n\n### Example flow\nAs mentioned in the overview, identifiers are sent in-band and details are sent out-of-band to Zipkin. In both cases, trace instrumentation is responsible for creating valid traces and rendering them properly. For example, a tracer ensures parity between the data it sends in-band (downstream) and out-of-band (async to Zipkin).\n\nHere’s an example sequence of http tracing where user code calls the resource /foo. This results in a single span, sent asynchronously to Zipkin after user code receives the http response.\n\n┌─────────────┐ ┌───────────────────────┐  ┌─────────────┐  ┌──────────────────┐\n│ User Code   │ │ Trace Instrumentation │  │ Http Client │  │ Zipkin Collector │\n└─────────────┘ └───────────────────────┘  └─────────────┘  └──────────────────┘\n       │                 │                         │                 │\n           ┌─────────┐\n       │ ──┤GET /foo ├─▶ │ ────┐                   │                 │\n           └─────────┘         │ record tags\n       │                 │ ◀───┘                   │                 │\n                           ────┐\n       │                 │     │ add trace headers │                 │\n                           ◀───┘\n       │                 │ ────┐                   │                 │\n                               │ record timestamp\n       │                 │ ◀───┘                   │                 │\n                             ┌─────────────────┐\n       │                 │ ──┤GET /foo         ├─▶ │                 │\n                             │X-B3-TraceId: aa │     ────┐\n       │                 │   │X-B3-SpanId: 6b  │   │     │           │\n                             └─────────────────┘         │ invoke\n       │                 │                         │     │ request   │\n                                                         │\n       │                 │                         │     │           │\n                                 ┌────────┐          ◀───┘\n       │                 │ ◀─────┤200 OK  ├─────── │                 │\n                           ────┐ └────────┘\n       │                 │     │ record duration   │                 │\n            ┌────────┐     ◀───┘\n       │ ◀──┤200 OK  ├── │                         │                 │\n            └────────┘       ┌────────────────────────────────┐\n       │                 │ ──┤ asynchronously report span     ├────▶ │\n                             │                                │\n                             │{                               │\n                             │  \"traceId\": \"aa\",              │\n                             │  \"id\": \"6b\",                   │\n                             │  \"name\": \"get\",                │\n                             │  \"timestamp\": 1483945573944000,│\n                             │  \"duration\": 386000,           │\n                             │  \"annotations\": [              │\n                             │--snip--                        │\n                             └────────────────────────────────┘\nTrace instrumentation report spans asynchronously to prevent delays or failures relating to the tracing system from delaying or breaking user code.\n\n### Transport\nSpans sent by the instrumented library must be transported from the services being traced to Zipkin collectors. There are three primary transports: HTTP, Kafka and Scribe. See Span Receivers for more information.\n\nThere are 4 components that make up Zipkin:\n\n - collector\n - storage\n - search\n - web UI\n\n### Zipkin Collector\nOnce the trace data arrives at the Zipkin collector daemon, it is validated, stored, and indexed for lookups by the Zipkin collector.\n\n### Storage\nZipkin was initially built to store data on Cassandra since Cassandra is scalable, has a flexible schema, and is heavily used within Twitter. However, we made this component pluggable. In addition to Cassandra, we natively support ElasticSearch and MySQL. Other back-ends might be offered as third party extensions.\n\n### Zipkin Query Service\nOnce the data is stored and indexed, we need a way to extract it. The query daemon provides a simple JSON API for finding and retrieving traces. The primary consumer of this API is the Web UI.\n\n### Web UI\nWe created a GUI that presents a nice interface for viewing traces. The web UI provides a method for viewing traces based on service, time, and annotations. Note: there is no built-in authentication in the UI!\n\n### 例子\n\n```\n<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-starter-sleuth</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency>\n\t\t<!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-zipkin -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.cloud</groupId>\n\t\t\t<artifactId>spring-cloud-sleuth-zipkin</artifactId>\n\t\t\t<version>1.1.2.RELEASE</version>\n\t\t</dependency>\n```\n\n属性文件：\n```\nspring.application.name=ServiceRegistryUsingConsulAndDistributedTrace\nlogging.level.org.springframework.web.servlet.DispatcherServlet=INFO\nspring.zipkin.baseUrl=http://localhost:9411/\nspring.sleuth.sampler.percentage=1.0\nsample.zipkin.enabled=true\n```\n\n```\n//Use this for debugging (or if there is no Zipkin server running on port 9411)\n  @Bean\n  @ConditionalOnProperty(value = \"sample.zipkin.enabled\", havingValue = \"false\")\n  public ZipkinSpanReporter spanCollector() {\n      return new ZipkinSpanReporter() {\n          @Override\n          public void report(zipkin.Span span) {\n//              log.info(String.format(\"Reporting span [%s]\", span));\n          }\n      };\n  }\n```\n","slug":"zipkin","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bgi0015i272e3veyadc","content":"<h3 id=\"Architecture-Overview\"><a href=\"#Architecture-Overview\" class=\"headerlink\" title=\"Architecture Overview\"></a>Architecture Overview</h3><p><img src=\"http://zipkin.io/public/img/web-screenshot.png\" alt=\"zipkin\"></p>\n<p>Tracers live in your applications and record timing and metadata about operations that took place. They often instrument libraries, so that their use is transparent to users. For example, an instrumented web server records when it received a request and when it sent a response. The trace data collected is called a Span.</p>\n<p>Instrumentation is written to be safe in production and have little overhead. For this reason, they only propagate IDs in-band, to tell the receiver there’s a trace in progress. Completed spans are reported to Zipkin out-of-band, similar to how applications report metrics asynchronously.</p>\n<p>For example, when an operation is being traced and it needs to make an outgoing http request, a few headers are added to propagate IDs. Headers are not used to send details such as the operation name.</p>\n<p>The component in an instrumented app that sends data to Zipkin is called a Reporter. Reporters send trace data via one of several transports to Zipkin collectors, which persist trace data to storage. Later, storage is queried by the API to provide data to the UI.</p>\n<p>Here’s a diagram describing this flow:</p>\n<p><img src=\"http://zipkin.io/public/img/architecture-1.png\" alt=\"\"></p>\n<h3 id=\"Zipkin-architecture\"><a href=\"#Zipkin-architecture\" class=\"headerlink\" title=\"Zipkin architecture\"></a>Zipkin architecture</h3><p>To see if an instrumentation library already exists for your platform, see the list of existing instrumentations.</p>\n<h3 id=\"Example-flow\"><a href=\"#Example-flow\" class=\"headerlink\" title=\"Example flow\"></a>Example flow</h3><p>As mentioned in the overview, identifiers are sent in-band and details are sent out-of-band to Zipkin. In both cases, trace instrumentation is responsible for creating valid traces and rendering them properly. For example, a tracer ensures parity between the data it sends in-band (downstream) and out-of-band (async to Zipkin).</p>\n<p>Here’s an example sequence of http tracing where user code calls the resource /foo. This results in a single span, sent asynchronously to Zipkin after user code receives the http response.</p>\n<p>┌─────────────┐ ┌───────────────────────┐  ┌─────────────┐  ┌──────────────────┐<br>│ User Code   │ │ Trace Instrumentation │  │ Http Client │  │ Zipkin Collector │<br>└─────────────┘ └───────────────────────┘  └─────────────┘  └──────────────────┘<br>       │                 │                         │                 │<br>           ┌─────────┐<br>       │ ──┤GET /foo ├─▶ │ ────┐                   │                 │<br>           └─────────┘         │ record tags<br>       │                 │ ◀───┘                   │                 │<br>                           ────┐<br>       │                 │     │ add trace headers │                 │<br>                           ◀───┘<br>       │                 │ ────┐                   │                 │<br>                               │ record timestamp<br>       │                 │ ◀───┘                   │                 │<br>                             ┌─────────────────┐<br>       │                 │ ──┤GET /foo         ├─▶ │                 │<br>                             │X-B3-TraceId: aa │     ────┐<br>       │                 │   │X-B3-SpanId: 6b  │   │     │           │<br>                             └─────────────────┘         │ invoke<br>       │                 │                         │     │ request   │<br>                                                         │<br>       │                 │                         │     │           │<br>                                 ┌────────┐          ◀───┘<br>       │                 │ ◀─────┤200 OK  ├─────── │                 │<br>                           ────┐ └────────┘<br>       │                 │     │ record duration   │                 │<br>            ┌────────┐     ◀───┘<br>       │ ◀──┤200 OK  ├── │                         │                 │<br>            └────────┘       ┌────────────────────────────────┐<br>       │                 │ ──┤ asynchronously report span     ├────▶ │<br>                             │                                │<br>                             │{                               │<br>                             │  “traceId”: “aa”,              │<br>                             │  “id”: “6b”,                   │<br>                             │  “name”: “get”,                │<br>                             │  “timestamp”: 1483945573944000,│<br>                             │  “duration”: 386000,           │<br>                             │  “annotations”: [              │<br>                             │–snip–                        │<br>                             └────────────────────────────────┘<br>Trace instrumentation report spans asynchronously to prevent delays or failures relating to the tracing system from delaying or breaking user code.</p>\n<h3 id=\"Transport\"><a href=\"#Transport\" class=\"headerlink\" title=\"Transport\"></a>Transport</h3><p>Spans sent by the instrumented library must be transported from the services being traced to Zipkin collectors. There are three primary transports: HTTP, Kafka and Scribe. See Span Receivers for more information.</p>\n<p>There are 4 components that make up Zipkin:</p>\n<ul>\n<li>collector</li>\n<li>storage</li>\n<li>search</li>\n<li>web UI</li>\n</ul>\n<h3 id=\"Zipkin-Collector\"><a href=\"#Zipkin-Collector\" class=\"headerlink\" title=\"Zipkin Collector\"></a>Zipkin Collector</h3><p>Once the trace data arrives at the Zipkin collector daemon, it is validated, stored, and indexed for lookups by the Zipkin collector.</p>\n<h3 id=\"Storage\"><a href=\"#Storage\" class=\"headerlink\" title=\"Storage\"></a>Storage</h3><p>Zipkin was initially built to store data on Cassandra since Cassandra is scalable, has a flexible schema, and is heavily used within Twitter. However, we made this component pluggable. In addition to Cassandra, we natively support ElasticSearch and MySQL. Other back-ends might be offered as third party extensions.</p>\n<h3 id=\"Zipkin-Query-Service\"><a href=\"#Zipkin-Query-Service\" class=\"headerlink\" title=\"Zipkin Query Service\"></a>Zipkin Query Service</h3><p>Once the data is stored and indexed, we need a way to extract it. The query daemon provides a simple JSON API for finding and retrieving traces. The primary consumer of this API is the Web UI.</p>\n<h3 id=\"Web-UI\"><a href=\"#Web-UI\" class=\"headerlink\" title=\"Web UI\"></a>Web UI</h3><p>We created a GUI that presents a nice interface for viewing traces. The web UI provides a method for viewing traces based on service, time, and annotations. Note: there is no built-in authentication in the UI!</p>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency&gt;</div><div class=\"line\">\t\t&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-zipkin --&gt;</div><div class=\"line\">\t\t&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency&gt;</div></pre></td></tr></table></figure>\n<p>属性文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring.application.name=ServiceRegistryUsingConsulAndDistributedTrace</div><div class=\"line\">logging.level.org.springframework.web.servlet.DispatcherServlet=INFO</div><div class=\"line\">spring.zipkin.baseUrl=http://localhost:9411/</div><div class=\"line\">spring.sleuth.sampler.percentage=1.0</div><div class=\"line\">sample.zipkin.enabled=true</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">//Use this for debugging (or if there is no Zipkin server running on port 9411)</div><div class=\"line\">  @Bean</div><div class=\"line\">  @ConditionalOnProperty(value = &quot;sample.zipkin.enabled&quot;, havingValue = &quot;false&quot;)</div><div class=\"line\">  public ZipkinSpanReporter spanCollector() &#123;</div><div class=\"line\">      return new ZipkinSpanReporter() &#123;</div><div class=\"line\">          @Override</div><div class=\"line\">          public void report(zipkin.Span span) &#123;</div><div class=\"line\">//              log.info(String.format(&quot;Reporting span [%s]&quot;, span));</div><div class=\"line\">          &#125;</div><div class=\"line\">      &#125;;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Architecture-Overview\"><a href=\"#Architecture-Overview\" class=\"headerlink\" title=\"Architecture Overview\"></a>Architecture Overview</h3><p><img src=\"http://zipkin.io/public/img/web-screenshot.png\" alt=\"zipkin\"></p>\n<p>Tracers live in your applications and record timing and metadata about operations that took place. They often instrument libraries, so that their use is transparent to users. For example, an instrumented web server records when it received a request and when it sent a response. The trace data collected is called a Span.</p>\n<p>Instrumentation is written to be safe in production and have little overhead. For this reason, they only propagate IDs in-band, to tell the receiver there’s a trace in progress. Completed spans are reported to Zipkin out-of-band, similar to how applications report metrics asynchronously.</p>\n<p>For example, when an operation is being traced and it needs to make an outgoing http request, a few headers are added to propagate IDs. Headers are not used to send details such as the operation name.</p>\n<p>The component in an instrumented app that sends data to Zipkin is called a Reporter. Reporters send trace data via one of several transports to Zipkin collectors, which persist trace data to storage. Later, storage is queried by the API to provide data to the UI.</p>\n<p>Here’s a diagram describing this flow:</p>\n<p><img src=\"http://zipkin.io/public/img/architecture-1.png\" alt=\"\"></p>\n<h3 id=\"Zipkin-architecture\"><a href=\"#Zipkin-architecture\" class=\"headerlink\" title=\"Zipkin architecture\"></a>Zipkin architecture</h3><p>To see if an instrumentation library already exists for your platform, see the list of existing instrumentations.</p>\n<h3 id=\"Example-flow\"><a href=\"#Example-flow\" class=\"headerlink\" title=\"Example flow\"></a>Example flow</h3><p>As mentioned in the overview, identifiers are sent in-band and details are sent out-of-band to Zipkin. In both cases, trace instrumentation is responsible for creating valid traces and rendering them properly. For example, a tracer ensures parity between the data it sends in-band (downstream) and out-of-band (async to Zipkin).</p>\n<p>Here’s an example sequence of http tracing where user code calls the resource /foo. This results in a single span, sent asynchronously to Zipkin after user code receives the http response.</p>\n<p>┌─────────────┐ ┌───────────────────────┐  ┌─────────────┐  ┌──────────────────┐<br>│ User Code   │ │ Trace Instrumentation │  │ Http Client │  │ Zipkin Collector │<br>└─────────────┘ └───────────────────────┘  └─────────────┘  └──────────────────┘<br>       │                 │                         │                 │<br>           ┌─────────┐<br>       │ ──┤GET /foo ├─▶ │ ────┐                   │                 │<br>           └─────────┘         │ record tags<br>       │                 │ ◀───┘                   │                 │<br>                           ────┐<br>       │                 │     │ add trace headers │                 │<br>                           ◀───┘<br>       │                 │ ────┐                   │                 │<br>                               │ record timestamp<br>       │                 │ ◀───┘                   │                 │<br>                             ┌─────────────────┐<br>       │                 │ ──┤GET /foo         ├─▶ │                 │<br>                             │X-B3-TraceId: aa │     ────┐<br>       │                 │   │X-B3-SpanId: 6b  │   │     │           │<br>                             └─────────────────┘         │ invoke<br>       │                 │                         │     │ request   │<br>                                                         │<br>       │                 │                         │     │           │<br>                                 ┌────────┐          ◀───┘<br>       │                 │ ◀─────┤200 OK  ├─────── │                 │<br>                           ────┐ └────────┘<br>       │                 │     │ record duration   │                 │<br>            ┌────────┐     ◀───┘<br>       │ ◀──┤200 OK  ├── │                         │                 │<br>            └────────┘       ┌────────────────────────────────┐<br>       │                 │ ──┤ asynchronously report span     ├────▶ │<br>                             │                                │<br>                             │{                               │<br>                             │  “traceId”: “aa”,              │<br>                             │  “id”: “6b”,                   │<br>                             │  “name”: “get”,                │<br>                             │  “timestamp”: 1483945573944000,│<br>                             │  “duration”: 386000,           │<br>                             │  “annotations”: [              │<br>                             │–snip–                        │<br>                             └────────────────────────────────┘<br>Trace instrumentation report spans asynchronously to prevent delays or failures relating to the tracing system from delaying or breaking user code.</p>\n<h3 id=\"Transport\"><a href=\"#Transport\" class=\"headerlink\" title=\"Transport\"></a>Transport</h3><p>Spans sent by the instrumented library must be transported from the services being traced to Zipkin collectors. There are three primary transports: HTTP, Kafka and Scribe. See Span Receivers for more information.</p>\n<p>There are 4 components that make up Zipkin:</p>\n<ul>\n<li>collector</li>\n<li>storage</li>\n<li>search</li>\n<li>web UI</li>\n</ul>\n<h3 id=\"Zipkin-Collector\"><a href=\"#Zipkin-Collector\" class=\"headerlink\" title=\"Zipkin Collector\"></a>Zipkin Collector</h3><p>Once the trace data arrives at the Zipkin collector daemon, it is validated, stored, and indexed for lookups by the Zipkin collector.</p>\n<h3 id=\"Storage\"><a href=\"#Storage\" class=\"headerlink\" title=\"Storage\"></a>Storage</h3><p>Zipkin was initially built to store data on Cassandra since Cassandra is scalable, has a flexible schema, and is heavily used within Twitter. However, we made this component pluggable. In addition to Cassandra, we natively support ElasticSearch and MySQL. Other back-ends might be offered as third party extensions.</p>\n<h3 id=\"Zipkin-Query-Service\"><a href=\"#Zipkin-Query-Service\" class=\"headerlink\" title=\"Zipkin Query Service\"></a>Zipkin Query Service</h3><p>Once the data is stored and indexed, we need a way to extract it. The query daemon provides a simple JSON API for finding and retrieving traces. The primary consumer of this API is the Web UI.</p>\n<h3 id=\"Web-UI\"><a href=\"#Web-UI\" class=\"headerlink\" title=\"Web UI\"></a>Web UI</h3><p>We created a GUI that presents a nice interface for viewing traces. The web UI provides a method for viewing traces based on service, time, and annotations. Note: there is no built-in authentication in the UI!</p>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency&gt;</div><div class=\"line\">\t\t&lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-sleuth-zipkin --&gt;</div><div class=\"line\">\t\t&lt;dependency&gt;</div><div class=\"line\">\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;</div><div class=\"line\">\t\t\t&lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;</div><div class=\"line\">\t\t\t&lt;version&gt;1.1.2.RELEASE&lt;/version&gt;</div><div class=\"line\">\t\t&lt;/dependency&gt;</div></pre></td></tr></table></figure>\n<p>属性文件：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">spring.application.name=ServiceRegistryUsingConsulAndDistributedTrace</div><div class=\"line\">logging.level.org.springframework.web.servlet.DispatcherServlet=INFO</div><div class=\"line\">spring.zipkin.baseUrl=http://localhost:9411/</div><div class=\"line\">spring.sleuth.sampler.percentage=1.0</div><div class=\"line\">sample.zipkin.enabled=true</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">//Use this for debugging (or if there is no Zipkin server running on port 9411)</div><div class=\"line\">  @Bean</div><div class=\"line\">  @ConditionalOnProperty(value = &quot;sample.zipkin.enabled&quot;, havingValue = &quot;false&quot;)</div><div class=\"line\">  public ZipkinSpanReporter spanCollector() &#123;</div><div class=\"line\">      return new ZipkinSpanReporter() &#123;</div><div class=\"line\">          @Override</div><div class=\"line\">          public void report(zipkin.Span span) &#123;</div><div class=\"line\">//              log.info(String.format(&quot;Reporting span [%s]&quot;, span));</div><div class=\"line\">          &#125;</div><div class=\"line\">      &#125;;</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n"},{"title":"YARN基本架构","date":"2016-03-29T12:46:25.000Z","_content":"\n###  YARN基本架构\nYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：\n  - 一个全局的资源管理器ResourceManager\n  - 每个应用程序特有的ApplicationMaster。\n\n  其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。\n\nYARN 总体上仍然是Master/Slave结构，在整个资源管理框架中，ResourceManager为Master，NodeManager为 Slave，ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以 跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一 定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。\n![yarn_architecture](http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)\n\n#### 1.ResourceManager(RM)\nRM是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n(1)：调度器\n\n调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执 行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分 配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、 CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN 提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。\n\n（2）:应用程序管理器\n\n应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。\n\n#### 2.ApplicationMaster(AM)\n用户提交的每个应用程序均包含1个AM，主要功能包括：\n\n与RM调度器协商以获取资源（用Container表示）；\n\n将得到的任务进一步分配给内部的任务；\n\n与NM通信以启动/停止任务；\n\n监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。\n\n#### 3.NodeManager(NM)\nNM是每个节点上的资源和任务管理器，一方面，它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自AM的Container启动/停止等各种请求。\n\n#### 4.Container\nContainer 是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用 Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。\n目前，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。\n\n### YARN工作流程\n当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：\n第一个阶段是启动ApplicationMaster；\n第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。\n\nYARN的工作流程分为以下几个步骤：\n- 步骤1：　用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。\n- 步骤2：　ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。\n- 步骤3：　ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。\n- 步骤4：　ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。\n- 步骤5：　一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。\n- 步骤6：　NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。\n- 步骤7：　各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。\n- 步骤8：　应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。\n\n### Hadoop: Writing YARN Applications\nsee http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\n\n#### 1. 文件格式化与启动namenode&DataNode\n```\n$ bin/hdfs namenode -format\n\n$ sbin/start-dfs.sh\n\n```\n#### 2. 启动RM&NM\n```\n$ sbin/start-yarn.sh\n\nResourceManager - http://localhost:8088/\n```\n\n#### 3. 例子：\n包含了实现一个application的三个要求:\n- 客户端和RM （Client.Java）\n  - 客户端提交application\n- AM和RM （ApplicationMaster.java）\n  - 注册AM，申请分配container\n- AM和NM （ApplicationMaster.java）\n  - 启动container\n\n执行命令：\n```\nhadoop jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar -shell_command '/bin/date'\n```\n启动10个container，每个都执行`date`命令\n执行代码流程:\n1. 客户端通过org.apache.hadoop.yarn.applications.distributedshell.Client提交application到RM，需提供ApplicationSubmissionContext\n2. org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster提交containers请求，执行用户提交的命令ContainerLaunchContext.commands\n\n客户端(Client.java):\n1. YarnClient.getNewApplication\n2. 填充ApplicationSubmissionContext,ContainerLaunchContext（启动AM的Container）​\n3. YarnClient.submitApplication​\n4. 每隔一段时间调用YarnClient.getApplicationReport获得Application Status\n\n```\n  // 创建AM的上下文信息  \n  ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);  \n  // 设置本地资源，AppMaster.jar包，log4j.properties  \n  amContainer.setLocalResources(localResources);  \n  // 环境变量,shell脚本在hdfs的地址, CLASSPATH  \n  amContainer.setEnvironment(env);  \n  // 设置启动AM的命令和参数  \n  Vector<CharSequence> vargs = new Vector<CharSequence>(30);  \n  vargs.add(\"${JAVA_HOME}\" + \"/bin/java\");  \n  vargs.add(\"-Xmx\" + amMemory + \"m\");  \n  // AM主类  \n  vargs.add(\"org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster?\");  \n  vargs.add(\"--container_memory \" + String.valueOf(containerMemory));  \n  vargs.add(\"--num_containers \" + String.valueOf(numContainers));  \n  vargs.add(\"--priority \" + String.valueOf(shellCmdPriority));  \n  if (!shellCommand.isEmpty()) {  \n  vargs.add(\"--shell_command \" + shellCommand + \"\");  \n  }  \n  if (!shellArgs.isEmpty()) {  \n  vargs.add(\"--shell_args \" + shellArgs + \"\");  \n  }  \n  for (Map.Entry<String, String> entry : shellEnv.entrySet()) {  \n  vargs.add(\"--shell_env \" + entry.getKey() + \"=\" + entry.getValue());  \n  }  \n  vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stdout\");  \n  vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stderr\");  \n\n  amContainer.setCommands(commands);  \n  // 设置Resource需求，目前只设置memory  \n  capability.setMemory(amMemory);  \n  amContainer.setResource(capability);  \n  appContext.setAMContainerSpec(amContainer);  \n  // 提交application到RM  \n  super.submitApplication(appContext);\n```\nApplicationMaster(ApplicationMaster.java​)\n1. AMRMClient.registerApplicationMaster​​\n2. 提供ContainerRequest到AMRMClient.addContainerRequest​\n3. 通过AMRMClient.allocate获得container\n4. container放入新建的LaunchContainerRunnable线程内执行\n5. 创建ContainerLaunchContext​，设置localResource，shellcommand, shellArgs等​​container启动信息\n6. ContainerManager.startContainer(startReq)​​\n7. 下次RPC call后得到的Response信息，AMResponse.getCompletedContainersStatuses​​\n8. AMRMClient.unregisterApplicationMaster​​\n\n```\n  // 新建AMRMClient，2.1beta版本实现了异步AMRMClient，这里还是同步的方式  \n  resourceManager = new AMRMClientImpl(appAttemptID);  \n  resourceManager.init(conf);  \n  resourceManager.start();  \n  // 向RM注册自己  \n  RegisterApplicationMasterResponse response = resourceManager  \n    .registerApplicationMaster(appMasterHostname, appMasterRpcPort,  \n        appMasterTrackingUrl);  \n  while (numCompletedContainers.get() < numTotalContainers && !appDone) {  \n  // 封装Container请求，设置Resource需求，这边只设置了memory  \n  ContainerRequest containerAsk = setupContainerAskForRM(askCount);  \n  resourceManager.addContainerRequest(containerAsk);  \n\n  // Send the request to RM  \n  LOG.info(\"Asking RM for containers\" + \", askCount=\" + askCount);  \n  AMResponse amResp = sendContainerAskToRM();  \n\n  // Retrieve list of allocated containers from the response  \n  List<Container> allocatedContainers = amResp.getAllocatedContainers();  \n  for (Container allocatedContainer : allocatedContainers) {  \n      //新建一个线程来提交container启动请求，这样主线程就不会被block住了  \n      LaunchContainerRunnable runnableLaunchContainer = new LaunchContainerRunnable(  \n        allocatedContainer);  \n      Thread launchThread = new Thread(runnableLaunchContainer);  \n      launchThreads.add(launchThread);  \n      launchThread.start();  \n  }  \n  List<ContainerStatus> completedContainers = amResp.getCompletedContainersStatuses();  \n  }  \n  // 向RM注销自己  \n  resourceManager.unregisterApplicationMaster(appStatus, appMessage, null);  \n```\n","source":"_posts/yarn.md","raw":"---\ntitle: YARN基本架构\ndate: 2016-3-29 20:46:25\n---\n\n###  YARN基本架构\nYARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：\n  - 一个全局的资源管理器ResourceManager\n  - 每个应用程序特有的ApplicationMaster。\n\n  其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。\n\nYARN 总体上仍然是Master/Slave结构，在整个资源管理框架中，ResourceManager为Master，NodeManager为 Slave，ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以 跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一 定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。\n![yarn_architecture](http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)\n\n#### 1.ResourceManager(RM)\nRM是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。\n\n(1)：调度器\n\n调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执 行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分 配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、 CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN 提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。\n\n（2）:应用程序管理器\n\n应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。\n\n#### 2.ApplicationMaster(AM)\n用户提交的每个应用程序均包含1个AM，主要功能包括：\n\n与RM调度器协商以获取资源（用Container表示）；\n\n将得到的任务进一步分配给内部的任务；\n\n与NM通信以启动/停止任务；\n\n监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。\n\n#### 3.NodeManager(NM)\nNM是每个节点上的资源和任务管理器，一方面，它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自AM的Container启动/停止等各种请求。\n\n#### 4.Container\nContainer 是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用 Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。\n目前，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。\n\n### YARN工作流程\n当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：\n第一个阶段是启动ApplicationMaster；\n第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。\n\nYARN的工作流程分为以下几个步骤：\n- 步骤1：　用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。\n- 步骤2：　ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。\n- 步骤3：　ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。\n- 步骤4：　ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。\n- 步骤5：　一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。\n- 步骤6：　NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。\n- 步骤7：　各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。\n- 步骤8：　应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。\n\n### Hadoop: Writing YARN Applications\nsee http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\n\n#### 1. 文件格式化与启动namenode&DataNode\n```\n$ bin/hdfs namenode -format\n\n$ sbin/start-dfs.sh\n\n```\n#### 2. 启动RM&NM\n```\n$ sbin/start-yarn.sh\n\nResourceManager - http://localhost:8088/\n```\n\n#### 3. 例子：\n包含了实现一个application的三个要求:\n- 客户端和RM （Client.Java）\n  - 客户端提交application\n- AM和RM （ApplicationMaster.java）\n  - 注册AM，申请分配container\n- AM和NM （ApplicationMaster.java）\n  - 启动container\n\n执行命令：\n```\nhadoop jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar -shell_command '/bin/date'\n```\n启动10个container，每个都执行`date`命令\n执行代码流程:\n1. 客户端通过org.apache.hadoop.yarn.applications.distributedshell.Client提交application到RM，需提供ApplicationSubmissionContext\n2. org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster提交containers请求，执行用户提交的命令ContainerLaunchContext.commands\n\n客户端(Client.java):\n1. YarnClient.getNewApplication\n2. 填充ApplicationSubmissionContext,ContainerLaunchContext（启动AM的Container）​\n3. YarnClient.submitApplication​\n4. 每隔一段时间调用YarnClient.getApplicationReport获得Application Status\n\n```\n  // 创建AM的上下文信息  \n  ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);  \n  // 设置本地资源，AppMaster.jar包，log4j.properties  \n  amContainer.setLocalResources(localResources);  \n  // 环境变量,shell脚本在hdfs的地址, CLASSPATH  \n  amContainer.setEnvironment(env);  \n  // 设置启动AM的命令和参数  \n  Vector<CharSequence> vargs = new Vector<CharSequence>(30);  \n  vargs.add(\"${JAVA_HOME}\" + \"/bin/java\");  \n  vargs.add(\"-Xmx\" + amMemory + \"m\");  \n  // AM主类  \n  vargs.add(\"org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster?\");  \n  vargs.add(\"--container_memory \" + String.valueOf(containerMemory));  \n  vargs.add(\"--num_containers \" + String.valueOf(numContainers));  \n  vargs.add(\"--priority \" + String.valueOf(shellCmdPriority));  \n  if (!shellCommand.isEmpty()) {  \n  vargs.add(\"--shell_command \" + shellCommand + \"\");  \n  }  \n  if (!shellArgs.isEmpty()) {  \n  vargs.add(\"--shell_args \" + shellArgs + \"\");  \n  }  \n  for (Map.Entry<String, String> entry : shellEnv.entrySet()) {  \n  vargs.add(\"--shell_env \" + entry.getKey() + \"=\" + entry.getValue());  \n  }  \n  vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stdout\");  \n  vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stderr\");  \n\n  amContainer.setCommands(commands);  \n  // 设置Resource需求，目前只设置memory  \n  capability.setMemory(amMemory);  \n  amContainer.setResource(capability);  \n  appContext.setAMContainerSpec(amContainer);  \n  // 提交application到RM  \n  super.submitApplication(appContext);\n```\nApplicationMaster(ApplicationMaster.java​)\n1. AMRMClient.registerApplicationMaster​​\n2. 提供ContainerRequest到AMRMClient.addContainerRequest​\n3. 通过AMRMClient.allocate获得container\n4. container放入新建的LaunchContainerRunnable线程内执行\n5. 创建ContainerLaunchContext​，设置localResource，shellcommand, shellArgs等​​container启动信息\n6. ContainerManager.startContainer(startReq)​​\n7. 下次RPC call后得到的Response信息，AMResponse.getCompletedContainersStatuses​​\n8. AMRMClient.unregisterApplicationMaster​​\n\n```\n  // 新建AMRMClient，2.1beta版本实现了异步AMRMClient，这里还是同步的方式  \n  resourceManager = new AMRMClientImpl(appAttemptID);  \n  resourceManager.init(conf);  \n  resourceManager.start();  \n  // 向RM注册自己  \n  RegisterApplicationMasterResponse response = resourceManager  \n    .registerApplicationMaster(appMasterHostname, appMasterRpcPort,  \n        appMasterTrackingUrl);  \n  while (numCompletedContainers.get() < numTotalContainers && !appDone) {  \n  // 封装Container请求，设置Resource需求，这边只设置了memory  \n  ContainerRequest containerAsk = setupContainerAskForRM(askCount);  \n  resourceManager.addContainerRequest(containerAsk);  \n\n  // Send the request to RM  \n  LOG.info(\"Asking RM for containers\" + \", askCount=\" + askCount);  \n  AMResponse amResp = sendContainerAskToRM();  \n\n  // Retrieve list of allocated containers from the response  \n  List<Container> allocatedContainers = amResp.getAllocatedContainers();  \n  for (Container allocatedContainer : allocatedContainers) {  \n      //新建一个线程来提交container启动请求，这样主线程就不会被block住了  \n      LaunchContainerRunnable runnableLaunchContainer = new LaunchContainerRunnable(  \n        allocatedContainer);  \n      Thread launchThread = new Thread(runnableLaunchContainer);  \n      launchThreads.add(launchThread);  \n      launchThread.start();  \n  }  \n  List<ContainerStatus> completedContainers = amResp.getCompletedContainersStatuses();  \n  }  \n  // 向RM注销自己  \n  resourceManager.unregisterApplicationMaster(appStatus, appMessage, null);  \n```\n","slug":"yarn","published":1,"updated":"2017-05-09T15:25:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj2in1bgj0019i272ydkrzpq9","content":"<h3 id=\"YARN基本架构\"><a href=\"#YARN基本架构\" class=\"headerlink\" title=\"YARN基本架构\"></a>YARN基本架构</h3><p>YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：</p>\n<ul>\n<li>一个全局的资源管理器ResourceManager</li>\n<li><p>每个应用程序特有的ApplicationMaster。</p>\n<p>其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。</p>\n</li>\n</ul>\n<p>YARN 总体上仍然是Master/Slave结构，在整个资源管理框架中，ResourceManager为Master，NodeManager为 Slave，ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以 跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一 定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。<br><img src=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\" alt=\"yarn_architecture\"></p>\n<h4 id=\"1-ResourceManager-RM\"><a href=\"#1-ResourceManager-RM\" class=\"headerlink\" title=\"1.ResourceManager(RM)\"></a>1.ResourceManager(RM)</h4><p>RM是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>(1)：调度器</p>\n<p>调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执 行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分 配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、 CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN 提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。</p>\n<p>（2）:应用程序管理器</p>\n<p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。</p>\n<h4 id=\"2-ApplicationMaster-AM\"><a href=\"#2-ApplicationMaster-AM\" class=\"headerlink\" title=\"2.ApplicationMaster(AM)\"></a>2.ApplicationMaster(AM)</h4><p>用户提交的每个应用程序均包含1个AM，主要功能包括：</p>\n<p>与RM调度器协商以获取资源（用Container表示）；</p>\n<p>将得到的任务进一步分配给内部的任务；</p>\n<p>与NM通信以启动/停止任务；</p>\n<p>监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p>\n<h4 id=\"3-NodeManager-NM\"><a href=\"#3-NodeManager-NM\" class=\"headerlink\" title=\"3.NodeManager(NM)\"></a>3.NodeManager(NM)</h4><p>NM是每个节点上的资源和任务管理器，一方面，它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自AM的Container启动/停止等各种请求。</p>\n<h4 id=\"4-Container\"><a href=\"#4-Container\" class=\"headerlink\" title=\"4.Container\"></a>4.Container</h4><p>Container 是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用 Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。<br>目前，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。</p>\n<h3 id=\"YARN工作流程\"><a href=\"#YARN工作流程\" class=\"headerlink\" title=\"YARN工作流程\"></a>YARN工作流程</h3><p>当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：<br>第一个阶段是启动ApplicationMaster；<br>第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。</p>\n<p>YARN的工作流程分为以下几个步骤：</p>\n<ul>\n<li>步骤1：　用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。</li>\n<li>步骤2：　ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。</li>\n<li>步骤3：　ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。</li>\n<li>步骤4：　ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。</li>\n<li>步骤5：　一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。</li>\n<li>步骤6：　NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li>\n<li>步骤7：　各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。</li>\n<li>步骤8：　应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</li>\n</ul>\n<h3 id=\"Hadoop-Writing-YARN-Applications\"><a href=\"#Hadoop-Writing-YARN-Applications\" class=\"headerlink\" title=\"Hadoop: Writing YARN Applications\"></a>Hadoop: Writing YARN Applications</h3><p>see <a href=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html</a></p>\n<h4 id=\"1-文件格式化与启动namenode-amp-DataNode\"><a href=\"#1-文件格式化与启动namenode-amp-DataNode\" class=\"headerlink\" title=\"1. 文件格式化与启动namenode&amp;DataNode\"></a>1. 文件格式化与启动namenode&amp;DataNode</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ bin/hdfs namenode -format</div><div class=\"line\"></div><div class=\"line\">$ sbin/start-dfs.sh</div></pre></td></tr></table></figure>\n<h4 id=\"2-启动RM-amp-NM\"><a href=\"#2-启动RM-amp-NM\" class=\"headerlink\" title=\"2. 启动RM&amp;NM\"></a>2. 启动RM&amp;NM</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sbin/start-yarn.sh</div><div class=\"line\"></div><div class=\"line\">ResourceManager - http://localhost:8088/</div></pre></td></tr></table></figure>\n<h4 id=\"3-例子：\"><a href=\"#3-例子：\" class=\"headerlink\" title=\"3. 例子：\"></a>3. 例子：</h4><p>包含了实现一个application的三个要求:</p>\n<ul>\n<li>客户端和RM （Client.Java）<ul>\n<li>客户端提交application</li>\n</ul>\n</li>\n<li>AM和RM （ApplicationMaster.java）<ul>\n<li>注册AM，申请分配container</li>\n</ul>\n</li>\n<li>AM和NM （ApplicationMaster.java）<ul>\n<li>启动container</li>\n</ul>\n</li>\n</ul>\n<p>执行命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar -shell_command &apos;/bin/date&apos;</div></pre></td></tr></table></figure></p>\n<p>启动10个container，每个都执行<code>date</code>命令<br>执行代码流程:</p>\n<ol>\n<li>客户端通过org.apache.hadoop.yarn.applications.distributedshell.Client提交application到RM，需提供ApplicationSubmissionContext</li>\n<li>org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster提交containers请求，执行用户提交的命令ContainerLaunchContext.commands</li>\n</ol>\n<p>客户端(Client.java):</p>\n<ol>\n<li>YarnClient.getNewApplication</li>\n<li>填充ApplicationSubmissionContext,ContainerLaunchContext（启动AM的Container）​</li>\n<li>YarnClient.submitApplication​</li>\n<li>每隔一段时间调用YarnClient.getApplicationReport获得Application Status</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 创建AM的上下文信息  </div><div class=\"line\">ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);  </div><div class=\"line\">// 设置本地资源，AppMaster.jar包，log4j.properties  </div><div class=\"line\">amContainer.setLocalResources(localResources);  </div><div class=\"line\">// 环境变量,shell脚本在hdfs的地址, CLASSPATH  </div><div class=\"line\">amContainer.setEnvironment(env);  </div><div class=\"line\">// 设置启动AM的命令和参数  </div><div class=\"line\">Vector&lt;CharSequence&gt; vargs = new Vector&lt;CharSequence&gt;(30);  </div><div class=\"line\">vargs.add(&quot;$&#123;JAVA_HOME&#125;&quot; + &quot;/bin/java&quot;);  </div><div class=\"line\">vargs.add(&quot;-Xmx&quot; + amMemory + &quot;m&quot;);  </div><div class=\"line\">// AM主类  </div><div class=\"line\">vargs.add(&quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster?&quot;);  </div><div class=\"line\">vargs.add(&quot;--container_memory &quot; + String.valueOf(containerMemory));  </div><div class=\"line\">vargs.add(&quot;--num_containers &quot; + String.valueOf(numContainers));  </div><div class=\"line\">vargs.add(&quot;--priority &quot; + String.valueOf(shellCmdPriority));  </div><div class=\"line\">if (!shellCommand.isEmpty()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_command &quot; + shellCommand + &quot;&quot;);  </div><div class=\"line\">&#125;  </div><div class=\"line\">if (!shellArgs.isEmpty()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_args &quot; + shellArgs + &quot;&quot;);  </div><div class=\"line\">&#125;  </div><div class=\"line\">for (Map.Entry&lt;String, String&gt; entry : shellEnv.entrySet()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_env &quot; + entry.getKey() + &quot;=&quot; + entry.getValue());  </div><div class=\"line\">&#125;  </div><div class=\"line\">vargs.add(&quot;1&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/AppMaster.stdout&quot;);  </div><div class=\"line\">vargs.add(&quot;2&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/AppMaster.stderr&quot;);  </div><div class=\"line\"></div><div class=\"line\">amContainer.setCommands(commands);  </div><div class=\"line\">// 设置Resource需求，目前只设置memory  </div><div class=\"line\">capability.setMemory(amMemory);  </div><div class=\"line\">amContainer.setResource(capability);  </div><div class=\"line\">appContext.setAMContainerSpec(amContainer);  </div><div class=\"line\">// 提交application到RM  </div><div class=\"line\">super.submitApplication(appContext);</div></pre></td></tr></table></figure>\n<p>ApplicationMaster(ApplicationMaster.java​)</p>\n<ol>\n<li>AMRMClient.registerApplicationMaster​​</li>\n<li>提供ContainerRequest到AMRMClient.addContainerRequest​</li>\n<li>通过AMRMClient.allocate获得container</li>\n<li>container放入新建的LaunchContainerRunnable线程内执行</li>\n<li>创建ContainerLaunchContext​，设置localResource，shellcommand, shellArgs等​​container启动信息</li>\n<li>ContainerManager.startContainer(startReq)​​</li>\n<li>下次RPC call后得到的Response信息，AMResponse.getCompletedContainersStatuses​​</li>\n<li>AMRMClient.unregisterApplicationMaster​​</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 新建AMRMClient，2.1beta版本实现了异步AMRMClient，这里还是同步的方式  </div><div class=\"line\">resourceManager = new AMRMClientImpl(appAttemptID);  </div><div class=\"line\">resourceManager.init(conf);  </div><div class=\"line\">resourceManager.start();  </div><div class=\"line\">// 向RM注册自己  </div><div class=\"line\">RegisterApplicationMasterResponse response = resourceManager  </div><div class=\"line\">  .registerApplicationMaster(appMasterHostname, appMasterRpcPort,  </div><div class=\"line\">      appMasterTrackingUrl);  </div><div class=\"line\">while (numCompletedContainers.get() &lt; numTotalContainers &amp;&amp; !appDone) &#123;  </div><div class=\"line\">// 封装Container请求，设置Resource需求，这边只设置了memory  </div><div class=\"line\">ContainerRequest containerAsk = setupContainerAskForRM(askCount);  </div><div class=\"line\">resourceManager.addContainerRequest(containerAsk);  </div><div class=\"line\"></div><div class=\"line\">// Send the request to RM  </div><div class=\"line\">LOG.info(&quot;Asking RM for containers&quot; + &quot;, askCount=&quot; + askCount);  </div><div class=\"line\">AMResponse amResp = sendContainerAskToRM();  </div><div class=\"line\"></div><div class=\"line\">// Retrieve list of allocated containers from the response  </div><div class=\"line\">List&lt;Container&gt; allocatedContainers = amResp.getAllocatedContainers();  </div><div class=\"line\">for (Container allocatedContainer : allocatedContainers) &#123;  </div><div class=\"line\">    //新建一个线程来提交container启动请求，这样主线程就不会被block住了  </div><div class=\"line\">    LaunchContainerRunnable runnableLaunchContainer = new LaunchContainerRunnable(  </div><div class=\"line\">      allocatedContainer);  </div><div class=\"line\">    Thread launchThread = new Thread(runnableLaunchContainer);  </div><div class=\"line\">    launchThreads.add(launchThread);  </div><div class=\"line\">    launchThread.start();  </div><div class=\"line\">&#125;  </div><div class=\"line\">List&lt;ContainerStatus&gt; completedContainers = amResp.getCompletedContainersStatuses();  </div><div class=\"line\">&#125;  </div><div class=\"line\">// 向RM注销自己  </div><div class=\"line\">resourceManager.unregisterApplicationMaster(appStatus, appMessage, null);</div></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"YARN基本架构\"><a href=\"#YARN基本架构\" class=\"headerlink\" title=\"YARN基本架构\"></a>YARN基本架构</h3><p>YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：</p>\n<ul>\n<li>一个全局的资源管理器ResourceManager</li>\n<li><p>每个应用程序特有的ApplicationMaster。</p>\n<p>其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。</p>\n</li>\n</ul>\n<p>YARN 总体上仍然是Master/Slave结构，在整个资源管理框架中，ResourceManager为Master，NodeManager为 Slave，ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以 跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一 定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。<br><img src=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\" alt=\"yarn_architecture\"></p>\n<h4 id=\"1-ResourceManager-RM\"><a href=\"#1-ResourceManager-RM\" class=\"headerlink\" title=\"1.ResourceManager(RM)\"></a>1.ResourceManager(RM)</h4><p>RM是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。</p>\n<p>(1)：调度器</p>\n<p>调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执 行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分 配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、 CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN 提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。</p>\n<p>（2）:应用程序管理器</p>\n<p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。</p>\n<h4 id=\"2-ApplicationMaster-AM\"><a href=\"#2-ApplicationMaster-AM\" class=\"headerlink\" title=\"2.ApplicationMaster(AM)\"></a>2.ApplicationMaster(AM)</h4><p>用户提交的每个应用程序均包含1个AM，主要功能包括：</p>\n<p>与RM调度器协商以获取资源（用Container表示）；</p>\n<p>将得到的任务进一步分配给内部的任务；</p>\n<p>与NM通信以启动/停止任务；</p>\n<p>监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p>\n<h4 id=\"3-NodeManager-NM\"><a href=\"#3-NodeManager-NM\" class=\"headerlink\" title=\"3.NodeManager(NM)\"></a>3.NodeManager(NM)</h4><p>NM是每个节点上的资源和任务管理器，一方面，它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自AM的Container启动/停止等各种请求。</p>\n<h4 id=\"4-Container\"><a href=\"#4-Container\" class=\"headerlink\" title=\"4.Container\"></a>4.Container</h4><p>Container 是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用 Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。<br>目前，YARN仅支持CPU和内存两种资源，且使用了轻量级资源隔离机制Cgroups进行资源隔离。</p>\n<h3 id=\"YARN工作流程\"><a href=\"#YARN工作流程\" class=\"headerlink\" title=\"YARN工作流程\"></a>YARN工作流程</h3><p>当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：<br>第一个阶段是启动ApplicationMaster；<br>第二个阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。</p>\n<p>YARN的工作流程分为以下几个步骤：</p>\n<ul>\n<li>步骤1：　用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。</li>\n<li>步骤2：　ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。</li>\n<li>步骤3：　ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。</li>\n<li>步骤4：　ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。</li>\n<li>步骤5：　一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。</li>\n<li>步骤6：　NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li>\n<li>步骤7：　各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。</li>\n<li>步骤8：　应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。</li>\n</ul>\n<h3 id=\"Hadoop-Writing-YARN-Applications\"><a href=\"#Hadoop-Writing-YARN-Applications\" class=\"headerlink\" title=\"Hadoop: Writing YARN Applications\"></a>Hadoop: Writing YARN Applications</h3><p>see <a href=\"http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html</a></p>\n<h4 id=\"1-文件格式化与启动namenode-amp-DataNode\"><a href=\"#1-文件格式化与启动namenode-amp-DataNode\" class=\"headerlink\" title=\"1. 文件格式化与启动namenode&amp;DataNode\"></a>1. 文件格式化与启动namenode&amp;DataNode</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ bin/hdfs namenode -format</div><div class=\"line\"></div><div class=\"line\">$ sbin/start-dfs.sh</div></pre></td></tr></table></figure>\n<h4 id=\"2-启动RM-amp-NM\"><a href=\"#2-启动RM-amp-NM\" class=\"headerlink\" title=\"2. 启动RM&amp;NM\"></a>2. 启动RM&amp;NM</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sbin/start-yarn.sh</div><div class=\"line\"></div><div class=\"line\">ResourceManager - http://localhost:8088/</div></pre></td></tr></table></figure>\n<h4 id=\"3-例子：\"><a href=\"#3-例子：\" class=\"headerlink\" title=\"3. 例子：\"></a>3. 例子：</h4><p>包含了实现一个application的三个要求:</p>\n<ul>\n<li>客户端和RM （Client.Java）<ul>\n<li>客户端提交application</li>\n</ul>\n</li>\n<li>AM和RM （ApplicationMaster.java）<ul>\n<li>注册AM，申请分配container</li>\n</ul>\n</li>\n<li>AM和NM （ApplicationMaster.java）<ul>\n<li>启动container</li>\n</ul>\n</li>\n</ul>\n<p>执行命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar hadoop-yarn-applications-distributedshell-3.0.0-alpha2.jar -shell_command &apos;/bin/date&apos;</div></pre></td></tr></table></figure></p>\n<p>启动10个container，每个都执行<code>date</code>命令<br>执行代码流程:</p>\n<ol>\n<li>客户端通过org.apache.hadoop.yarn.applications.distributedshell.Client提交application到RM，需提供ApplicationSubmissionContext</li>\n<li>org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster提交containers请求，执行用户提交的命令ContainerLaunchContext.commands</li>\n</ol>\n<p>客户端(Client.java):</p>\n<ol>\n<li>YarnClient.getNewApplication</li>\n<li>填充ApplicationSubmissionContext,ContainerLaunchContext（启动AM的Container）​</li>\n<li>YarnClient.submitApplication​</li>\n<li>每隔一段时间调用YarnClient.getApplicationReport获得Application Status</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 创建AM的上下文信息  </div><div class=\"line\">ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);  </div><div class=\"line\">// 设置本地资源，AppMaster.jar包，log4j.properties  </div><div class=\"line\">amContainer.setLocalResources(localResources);  </div><div class=\"line\">// 环境变量,shell脚本在hdfs的地址, CLASSPATH  </div><div class=\"line\">amContainer.setEnvironment(env);  </div><div class=\"line\">// 设置启动AM的命令和参数  </div><div class=\"line\">Vector&lt;CharSequence&gt; vargs = new Vector&lt;CharSequence&gt;(30);  </div><div class=\"line\">vargs.add(&quot;$&#123;JAVA_HOME&#125;&quot; + &quot;/bin/java&quot;);  </div><div class=\"line\">vargs.add(&quot;-Xmx&quot; + amMemory + &quot;m&quot;);  </div><div class=\"line\">// AM主类  </div><div class=\"line\">vargs.add(&quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster?&quot;);  </div><div class=\"line\">vargs.add(&quot;--container_memory &quot; + String.valueOf(containerMemory));  </div><div class=\"line\">vargs.add(&quot;--num_containers &quot; + String.valueOf(numContainers));  </div><div class=\"line\">vargs.add(&quot;--priority &quot; + String.valueOf(shellCmdPriority));  </div><div class=\"line\">if (!shellCommand.isEmpty()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_command &quot; + shellCommand + &quot;&quot;);  </div><div class=\"line\">&#125;  </div><div class=\"line\">if (!shellArgs.isEmpty()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_args &quot; + shellArgs + &quot;&quot;);  </div><div class=\"line\">&#125;  </div><div class=\"line\">for (Map.Entry&lt;String, String&gt; entry : shellEnv.entrySet()) &#123;  </div><div class=\"line\">vargs.add(&quot;--shell_env &quot; + entry.getKey() + &quot;=&quot; + entry.getValue());  </div><div class=\"line\">&#125;  </div><div class=\"line\">vargs.add(&quot;1&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/AppMaster.stdout&quot;);  </div><div class=\"line\">vargs.add(&quot;2&gt;&quot; + ApplicationConstants.LOG_DIR_EXPANSION_VAR + &quot;/AppMaster.stderr&quot;);  </div><div class=\"line\"></div><div class=\"line\">amContainer.setCommands(commands);  </div><div class=\"line\">// 设置Resource需求，目前只设置memory  </div><div class=\"line\">capability.setMemory(amMemory);  </div><div class=\"line\">amContainer.setResource(capability);  </div><div class=\"line\">appContext.setAMContainerSpec(amContainer);  </div><div class=\"line\">// 提交application到RM  </div><div class=\"line\">super.submitApplication(appContext);</div></pre></td></tr></table></figure>\n<p>ApplicationMaster(ApplicationMaster.java​)</p>\n<ol>\n<li>AMRMClient.registerApplicationMaster​​</li>\n<li>提供ContainerRequest到AMRMClient.addContainerRequest​</li>\n<li>通过AMRMClient.allocate获得container</li>\n<li>container放入新建的LaunchContainerRunnable线程内执行</li>\n<li>创建ContainerLaunchContext​，设置localResource，shellcommand, shellArgs等​​container启动信息</li>\n<li>ContainerManager.startContainer(startReq)​​</li>\n<li>下次RPC call后得到的Response信息，AMResponse.getCompletedContainersStatuses​​</li>\n<li>AMRMClient.unregisterApplicationMaster​​</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div></pre></td><td class=\"code\"><pre><div class=\"line\">// 新建AMRMClient，2.1beta版本实现了异步AMRMClient，这里还是同步的方式  </div><div class=\"line\">resourceManager = new AMRMClientImpl(appAttemptID);  </div><div class=\"line\">resourceManager.init(conf);  </div><div class=\"line\">resourceManager.start();  </div><div class=\"line\">// 向RM注册自己  </div><div class=\"line\">RegisterApplicationMasterResponse response = resourceManager  </div><div class=\"line\">  .registerApplicationMaster(appMasterHostname, appMasterRpcPort,  </div><div class=\"line\">      appMasterTrackingUrl);  </div><div class=\"line\">while (numCompletedContainers.get() &lt; numTotalContainers &amp;&amp; !appDone) &#123;  </div><div class=\"line\">// 封装Container请求，设置Resource需求，这边只设置了memory  </div><div class=\"line\">ContainerRequest containerAsk = setupContainerAskForRM(askCount);  </div><div class=\"line\">resourceManager.addContainerRequest(containerAsk);  </div><div class=\"line\"></div><div class=\"line\">// Send the request to RM  </div><div class=\"line\">LOG.info(&quot;Asking RM for containers&quot; + &quot;, askCount=&quot; + askCount);  </div><div class=\"line\">AMResponse amResp = sendContainerAskToRM();  </div><div class=\"line\"></div><div class=\"line\">// Retrieve list of allocated containers from the response  </div><div class=\"line\">List&lt;Container&gt; allocatedContainers = amResp.getAllocatedContainers();  </div><div class=\"line\">for (Container allocatedContainer : allocatedContainers) &#123;  </div><div class=\"line\">    //新建一个线程来提交container启动请求，这样主线程就不会被block住了  </div><div class=\"line\">    LaunchContainerRunnable runnableLaunchContainer = new LaunchContainerRunnable(  </div><div class=\"line\">      allocatedContainer);  </div><div class=\"line\">    Thread launchThread = new Thread(runnableLaunchContainer);  </div><div class=\"line\">    launchThreads.add(launchThread);  </div><div class=\"line\">    launchThread.start();  </div><div class=\"line\">&#125;  </div><div class=\"line\">List&lt;ContainerStatus&gt; completedContainers = amResp.getCompletedContainersStatuses();  </div><div class=\"line\">&#125;  </div><div class=\"line\">// 向RM注销自己  </div><div class=\"line\">resourceManager.unregisterApplicationMaster(appStatus, appMessage, null);</div></pre></td></tr></table></figure>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cj2in1bff0000i2728ktted8p","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bfz000ci272zwxhktfl"},{"post_id":"cj2in1bg3000ki2724vyik1af","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bg9000qi272ygunlcvs"},{"post_id":"cj2in1bg3000ki2724vyik1af","category_id":"cj2in1bg2000ii272x2bpn3fh","_id":"cj2in1bgb000ti272up5p12q0"},{"post_id":"cj2in1bfz000di272exv0bhy5","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bgc000vi272fobgrto7"},{"post_id":"cj2in1bfz000di272exv0bhy5","category_id":"cj2in1bg2000ii272x2bpn3fh","_id":"cj2in1bgd000yi2726hfvn41v"},{"post_id":"cj2in1bg4000li272zthqr4vt","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bgf0010i272iot9vki1"},{"post_id":"cj2in1bg4000li272zthqr4vt","category_id":"cj2in1bg2000ii272x2bpn3fh","_id":"cj2in1bgh0013i272lpqgbcnx"},{"post_id":"cj2in1bg7000oi2723g1myplz","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bgj0016i272rq4cutrp"},{"post_id":"cj2in1bg7000oi2723g1myplz","category_id":"cj2in1bg2000ii272x2bpn3fh","_id":"cj2in1bgj001ai272f8uowuqq"},{"post_id":"cj2in1bg1000hi272akte0wzm","category_id":"cj2in1bfo0004i272m5481pju","_id":"cj2in1bgk001ci272bm5fp2wl"},{"post_id":"cj2in1bg1000hi272akte0wzm","category_id":"cj2in1bg2000ii272x2bpn3fh","_id":"cj2in1bgk001ei272l1wrhbp5"}],"PostTag":[{"post_id":"cj2in1bff0000i2728ktted8p","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bg1000fi272rco1vuji"},{"post_id":"cj2in1bff0000i2728ktted8p","tag_id":"cj2in1bfy000ai2728qtz7sng","_id":"cj2in1bg3000ji272f1jt8bnl"},{"post_id":"cj2in1bfz000di272exv0bhy5","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bgi0014i272rcnizzum"},{"post_id":"cj2in1bfz000di272exv0bhy5","tag_id":"cj2in1bg1000gi2728u3bmkxr","_id":"cj2in1bgj0017i272bmcu7d2g"},{"post_id":"cj2in1bfz000di272exv0bhy5","tag_id":"cj2in1bg5000mi272afi727oo","_id":"cj2in1bgk001bi272to99uv87"},{"post_id":"cj2in1bfz000di272exv0bhy5","tag_id":"cj2in1bga000ri27269f0t1u3","_id":"cj2in1bgk001di272ixhlx6sc"},{"post_id":"cj2in1bfz000di272exv0bhy5","tag_id":"cj2in1bgd000wi27255tyw3fm","_id":"cj2in1bgl001gi272n3oq433l"},{"post_id":"cj2in1bg1000hi272akte0wzm","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bgn001ji272kmcz9t54"},{"post_id":"cj2in1bg1000hi272akte0wzm","tag_id":"cj2in1bg1000gi2728u3bmkxr","_id":"cj2in1bgn001ki272f5qoh0lv"},{"post_id":"cj2in1bg1000hi272akte0wzm","tag_id":"cj2in1bgj0018i272tx9w9api","_id":"cj2in1bgn001mi2722qdl0ytq"},{"post_id":"cj2in1bg1000hi272akte0wzm","tag_id":"cj2in1bga000ri27269f0t1u3","_id":"cj2in1bgo001ni272220gklg3"},{"post_id":"cj2in1bg1000hi272akte0wzm","tag_id":"cj2in1bgm001hi27297nmtwxw","_id":"cj2in1bgp001pi272o4rms8bk"},{"post_id":"cj2in1bg3000ki2724vyik1af","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bgr001ri272xoinocua"},{"post_id":"cj2in1bg3000ki2724vyik1af","tag_id":"cj2in1bg1000gi2728u3bmkxr","_id":"cj2in1bgr001si272q48n6192"},{"post_id":"cj2in1bg3000ki2724vyik1af","tag_id":"cj2in1bg5000mi272afi727oo","_id":"cj2in1bgs001ui2726h89r7mh"},{"post_id":"cj2in1bg3000ki2724vyik1af","tag_id":"cj2in1bga000ri27269f0t1u3","_id":"cj2in1bgs001vi2725znnd8ef"},{"post_id":"cj2in1bg3000ki2724vyik1af","tag_id":"cj2in1bgd000wi27255tyw3fm","_id":"cj2in1bgt001xi272ho3dw5iy"},{"post_id":"cj2in1bg4000li272zthqr4vt","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bgu001zi272cd2d0ds1"},{"post_id":"cj2in1bg4000li272zthqr4vt","tag_id":"cj2in1bg1000gi2728u3bmkxr","_id":"cj2in1bgu0020i272bp3ybozh"},{"post_id":"cj2in1bg4000li272zthqr4vt","tag_id":"cj2in1bgq001qi2721k8c8cr9","_id":"cj2in1bgu0022i272300p8nhq"},{"post_id":"cj2in1bg4000li272zthqr4vt","tag_id":"cj2in1bga000ri27269f0t1u3","_id":"cj2in1bgu0023i272s035ildx"},{"post_id":"cj2in1bg4000li272zthqr4vt","tag_id":"cj2in1bgs001wi2723unjgrty","_id":"cj2in1bgv0025i2727xfglhv0"},{"post_id":"cj2in1bg7000oi2723g1myplz","tag_id":"cj2in1bfr0005i2722zmqhuf1","_id":"cj2in1bgv0026i272jb7xy3h9"},{"post_id":"cj2in1bg7000oi2723g1myplz","tag_id":"cj2in1bg1000gi2728u3bmkxr","_id":"cj2in1bgv0027i2724w2tfvfw"},{"post_id":"cj2in1bg7000oi2723g1myplz","tag_id":"cj2in1bgj0018i272tx9w9api","_id":"cj2in1bgv0028i272cy2wf8g0"},{"post_id":"cj2in1bg7000oi2723g1myplz","tag_id":"cj2in1bga000ri27269f0t1u3","_id":"cj2in1bgv0029i27214u98b0d"},{"post_id":"cj2in1bg7000oi2723g1myplz","tag_id":"cj2in1bgu0024i27239pio73j","_id":"cj2in1bgw002ai272urlyas42"}],"Tag":[{"name":"分布式","_id":"cj2in1bfr0005i2722zmqhuf1"},{"name":"架构设计","_id":"cj2in1bfy000ai2728qtz7sng"},{"name":"Kubernetes","_id":"cj2in1bg1000gi2728u3bmkxr"},{"name":"container","_id":"cj2in1bg5000mi272afi727oo"},{"name":"容器","_id":"cj2in1bga000ri27269f0t1u3"},{"name":"PaaS","_id":"cj2in1bgd000wi27255tyw3fm"},{"name":"路由","_id":"cj2in1bgj0018i272tx9w9api"},{"name":"Ingress","_id":"cj2in1bgm001hi27297nmtwxw"},{"name":"配置","_id":"cj2in1bgq001qi2721k8c8cr9"},{"name":"Secrets","_id":"cj2in1bgs001wi2723unjgrty"},{"name":"ConfigMap","_id":"cj2in1bgu0024i27239pio73j"}]}}